{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSS 201 / 202 - CSS Bootcamp\n",
    "\n",
    "## Week 04 - Lecture 03\n",
    "\n",
    "### Umberto Mignozzetti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Applications**:\n",
    "\n",
    "**Centered vector:** Use the following formula to compute the mean-centered vector:\n",
    "\n",
    "$$\\mathbf{x} - \\dfrac{1}{n}\\mathbf{i}\\mathbf{i}^T\\mathbf{x}$$\n",
    "\n",
    "Where $\\mathbf{i}$ is a vector of ones.\n",
    "\n",
    "Hint 1: A vector of ones can be done with `np.ones([rows, cols])`.\n",
    "\n",
    "Hint 2: To transpose a numpy vector, do `vec.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Applications**:\n",
    "\n",
    "1. **Centered vector**: Prove that the operation $M_0 = I - \\dfrac{1}{n}\\mathbf{i}\\mathbf{i}^T$ is idempotent.\n",
    "\n",
    "2. **Sum of squares**: Compute the product $\\mathbf{x}^TM_0\\mathbf{x}$. It is equal to the sum of squares.\n",
    "\n",
    "3. **Covariance matrix**: Suppose you have a dataset $X$. The covariance matrix is defined as $\\dfrac{\\mathbf{X}^TM_0\\mathbf{X}}{n-1}$. Compute the covariance matrix of the data below.\n",
    "\n",
    "4. **Covariance matrix**: A power move in here is to plot the matrix. Do that. Did you find it interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answers here\n",
    "dat = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/anes2020.csv').values\n",
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Matrix Inverse** (cont'd): Solve a system with inverse matrices is straightforward:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "A\\mathbf{x} & = \\mathbf{b} \\\\\n",
    "A^{-1}A\\mathbf{x} & = A^{-1}\\mathbf{b} \\\\\n",
    "I\\mathbf{x} & = A^{-1}\\mathbf{b} \\\\\n",
    "\\mathbf{x} & = A^{-1}\\mathbf{b}\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.random.randn(3, 3)\n",
    "np.linalg.inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Matrix Inverse**:\n",
    "\n",
    "- Left Inverse: Inverse such that $LX = I$ is defined by $XL$ is not\n",
    "\n",
    "- Right Inverse: Inverse such that $XR = I$ is defined by $RX$ is not.\n",
    "\n",
    "- One efficient way to find a one-side inverse is to make the matrix square. But how to do that? Consider the following ideas:\n",
    "\n",
    "    1. $X^TX$\n",
    "    2. $XX^T$\n",
    "    \n",
    "- Let us see the derivations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.random.randn(10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Matrix Inverse**:\n",
    "\n",
    "**Check-in**: Derive the matrix for the right inverse.\n",
    "\n",
    "**Proposition:** If the inverse exists, then it is unique. Prove that.\n",
    "\n",
    "**Pseudoinverse**: Makes a singular matrix close (but not equal) to the inverse. Not unique!\n",
    "    \n",
    "- *Moore-Penrose Pseudoinverse*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [2, 4]])\n",
    "print('Pseudoinverse: ')\n",
    "print(np.linalg.pinv(A), end = '\\n\\n')\n",
    "\n",
    "print('Pseudoinverse in action: ')\n",
    "print(A @ np.linalg.pinv(A), end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Matrix Inverse**:\n",
    "\n",
    "Inverting a matrix is a mathematically demanding operation.\n",
    "\n",
    "The computer may get it wrong eventually.\n",
    "\n",
    "This is a major source of discrepancy in solutions to the same problem.\n",
    "\n",
    "Example: *Hilbert matrices*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = scipy.linalg.hilbert(3)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Decompositions**: \n",
    "\n",
    "The main reason to study so much matrices, is to create new representation of data.\n",
    "\n",
    "We achieve this by doing decompositions into matrices that have desirable properties.\n",
    "\n",
    "What is a decomposition? It is when we rewrite an object as a combination of others, provided that these others have desirable properties.\n",
    "\n",
    "Example: For numbers, if you get the number $3.13$, a useful decomposition is\n",
    "\n",
    "$$3.13 = 3 + 0.13$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Orthogonal Matrices**: Very useful types of matrices. We will learn how to decompose a matrix into its orthogonal matrices.\n",
    "\n",
    "**Definition**: A matrix $Q$ is said to be **orthogonal** if:\n",
    "\n",
    "1. All columns are pair-wise orthogonal\n",
    "2. The norm of each column is 1 (unit column vectors)\n",
    "\n",
    "Meaning:\n",
    "$$\n",
    "<\\mathbf{q}_i, \\mathbf{q}_j> = \\begin{cases}\n",
    "    0 & \\text{if } i \\neq j \\\\\n",
    "    1 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Orthogonal Matrices**: Why do we care about those? Simple:\n",
    "\n",
    "$$Q^TQ = I$$\n",
    "\n",
    "Awesome! How to compute an orthogonal matrix from a non-orthogonal one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Gram-Schmidt** procedure:\n",
    "\n",
    "For all column vectors in  starting from the first (leftmost) and moving systematically to the last (rightmost):\n",
    "\n",
    "1. Orthogonalize $\\mathbf{v}_k$ to all previous columns in matrix $Q$ using orthogonal vector decomposition.\n",
    "    - Compute the component of $\\mathbf{v}_k$ that is perpendicular to $\\mathbf{q}_{k-1}$, $\\mathbf{q}_{k-2}$, and so on down to $\\mathbf{q}_{1}$. The orthogonalized vector is called $\\mathbf{v}^*_k$\n",
    "\n",
    "2. Normalize $\\mathbf{v}^*_k$ to unit length. This is now $\\mathbf{v}_k$, the kth column in matrix $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**QR Decomposition**: We decompose a matrix $A$ into its orthogonal component and the rest.\n",
    "\n",
    "$$A = QR$$\n",
    "\n",
    "Let us find $R$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.random.randn(3,3)\n",
    "Q, R = np.linalg.qr(A)\n",
    "print(Q)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**QR Decomposition**:\n",
    "\n",
    "1. Note that the matrix $R$ is always upper-triangular.\n",
    "2. How much is $A^{-1}$ in QRese?\n",
    "3. Inverse with QR has fewer computation errors! Show that for a 30 x 30 random matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**LU Decomposition**: Another useful decomposition.\n",
    "\n",
    "We transform a matrix into a product of a lower-triangular times an upper triangular matrices.\n",
    "\n",
    "How to do that? It is part of the steps we do to find the inverse using Gauss-Jordan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([ [2, 2, 4], [1, 0, 3], [2, 1, 2] ])\n",
    "_, L, U = scipy.linalg.lu(A)\n",
    "print(L, end = '\\n\\n')\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Applications**:\n",
    "\n",
    "**Least Square method**: The least square method is a projection.\n",
    "\n",
    "<div>\n",
    "<img src=\"./imgl3/projreg.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "1. Derive the Least Square method (meaning: derive $\\mathbf{\\beta}$.\n",
    "\n",
    "2. Compute the least square method and apply it on the dataset, where the first row is the $\\mathbf{y}$ and the rest is $X$.\n",
    "\n",
    "3. Compute the least-square method using QR decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answers here\n",
    "y = dat[:,0]\n",
    "X = np.append(np.ones([dat.shape[0], 1]), dat[:,1:], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Eigendecomposition:** Probably the most useful decomposition in data sciences.\n",
    "\n",
    "We want to find vectors $\\mathbf{v}$ (eigenvector) and constants $\\lambda$ (eigenvalues) such that\n",
    "\n",
    "$$A\\mathbf{v} = \\lambda\\mathbf{v}$$\n",
    "\n",
    "*The effect of the matrix on the vector is the same as the effect of the scalar on the same vector!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Eigendecomposition:** Geometry\n",
    "\n",
    "<div>\n",
    "<img src=\"./imgl3/eigen1.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Eigendecomposition:** Principal Components Analysis\n",
    "\n",
    "<div>\n",
    "<img src=\"./imgl3/pcaill.svg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Eigendecomposition:** It is straightforward to compute them in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Eigendecomposition:** But what are we doing?\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "A\\mathbf{v} &=& \\lambda \\mathbf{v} \\\\\n",
    "A\\mathbf{v} - \\lambda \\mathbf{v} &=& \\mathbf{0} \\\\\n",
    "(A - \\lambda I)\\mathbf{v} &=&\\mathbf{0}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "And so, the matrix $A - \\lambda I$ should be singular. To this be true:\n",
    "\n",
    "$$det(A - \\lambda I) = 0$$\n",
    "\n",
    "And to compute determinants in numpy: `np.linalg.det`! This will find the *characteristic polynomial*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Eigendecomposition:** The decomposition leads to:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "A\\mathbf{v}_1 &=& \\lambda_1 \\mathbf{v}_1 \\\\\n",
    "&\\vdots& \\\\\n",
    "A\\mathbf{v}_n &=& \\lambda_n \\mathbf{v}_n\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "And thus, the decomposition is:\n",
    "\n",
    "$$AV = V\\Lambda$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Eigendecomposition:**\n",
    "\n",
    "***Symmetric matrices have orthogonal eigenvectors!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# just some random symmetric matrix\n",
    "A = np.random.randint(-3, 4, (3, 3))\n",
    "A = A.T @ A\n",
    "\n",
    "# its eigendecomposition\n",
    "L, V = np.linalg.eig(A)\n",
    "\n",
    "# all pairwise dot products\n",
    "print( np.dot(V[:,0], V[:,1]) )\n",
    "print( np.dot(V[:,0], V[:,2]) )\n",
    "print( np.dot(V[:,1], V[:,2]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Quadratic Forms**: One application is in quadratic forms. These forms are:\n",
    "\n",
    "$$\\mathbf{w}^TA\\mathbf{w} = \\alpha$$\n",
    "\n",
    "Important:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "A\\mathbf{v} &=& \\lambda\\mathbf{v} \\\\\n",
    "\\mathbf{v}^TA\\mathbf{v} &=& \\lambda\\mathbf{v}^T\\mathbf{v} \\\\\n",
    "\\mathbf{v}^TA\\mathbf{v} &=& \\lambda \\lVert \\mathbf{v} \\rVert^2\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "And this is useful to classify matrices:\n",
    "\n",
    "<div>\n",
    "<img src=\"./imgl3/deff1.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "And this is useful to classify matrices:\n",
    "\n",
    "<div>\n",
    "<img src=\"./imgl3/deff2.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Let us do some calculus now :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Great job!!!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
