{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSS 201 / 202 - CSS Bootcamp\n",
    "\n",
    "## Week 05 - Lecture 04\n",
    "\n",
    "### Umberto Mignozzetti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSS 201 / 202 - CSS Bootcamp\n",
    "\n",
    "## Statistical and Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\newcommand\\pN{N}$\n",
    "$\\newcommand\\pE{\\mathop{{}\\mathbb{E}}}$\n",
    "$\\newcommand\\pV{\\mathop{{}\\mathbb{V}}}$\n",
    "$\\newcommand\\pP{\\mathop{{}\\mathbb{P}}}$\n",
    "$\\newcommand{\\ind}{\\perp\\!\\!\\!\\!\\perp}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "\n",
    "\n",
    "# Statistical and Probability Theory\n",
    "\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\be}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bbs}{\\mathbf{b}}$\n",
    "$\\newcommand{\\bbeta}{\\mathbf{\\beta}}$\n",
    "$\\newcommand{\\hbbeta}{\\widehat{\\mathbf{\\beta}}}$\n",
    "$\\newcommand{\\bhb}{\\widehat{\\mathbf{b}}}$\n",
    "$\\newcommand{\\bvx}{\\mathbf{x}}$ \n",
    "$\\newcommand{\\Ex}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Vax}{\\mathbb{V}}$\n",
    "$\\newcommand{\\real}{\\mathbb{R}}$\n",
    "$\\newcommand{\\realp}{\\mathbb{R}^{+}}$\n",
    "$\\newcommand{\\cov}{\\text{Cov}}$\n",
    "$\\newcommand{\\convp}{\\overset{p}{\\longrightarrow}}$\n",
    "$\\newcommand{\\convd}{\\overset{d}{\\longrightarrow}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "**Definition: Random Variable**: Let $S$ be the sample space for an experiment. A real-valued function that is defined on $S$ is called a random variable.\n",
    "\n",
    "A random variable maps each state of the world to a real number.\n",
    "    \n",
    "1. Consider an experiment where a fair coin is tossed 10 times. We can define a random variable $X$ that counts the number of heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "When a probability measure has been defined on the sample space, we can determine the probabilities associated with each possible value of the random variable.\n",
    "\n",
    "**Definition -- Distribution of a random variable**: Let $X$ be a random variable. The **distribution** of $X$ is the collection of all probabilities of the form $P(X \\in C)$ for all sets $C$ of real numbers such that $\\{X \\in C\\}$ is an event.\n",
    "\n",
    "Example: Let a fair coin be tossed 10 times,\n",
    "and let $X$ be the number of heads that are obtained. In this experiment, the possible values of $X$ are $0, 1, 2, \\cdots, 10$. \n",
    "\n",
    "For each $x$, $P(X = x)$ is the sum of the probabilities\n",
    "of all of the outcomes in the event $\\{X = x\\}$. \n",
    "\n",
    "Because the coin is fair, each outcome has the same probability $\\big(\\frac{1}{2}\\big)^{10}$, and we need only count how many outcomes $s$ have $X(s) = x$.\n",
    "\n",
    "We know that $X(s) = x$ if and only if exactly $x$ of the 10 tosses are $H$.\n",
    "\n",
    "How much is this probabily?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "**Definition - Discrete Random Variable**: A *random variable* $X$ is discrete if takes at most $k$ values or an infinite sequence of countable values.\n",
    "\n",
    "Example: In the 10 times coin toss example, each outcome has chance $\\dfrac{1}{2^{10}}$. But there are ${10 \\choose x}$ number of events with $x$ heads. Therefore:\n",
    "\n",
    "$$P(X = x) = {10 \\choose x}\\dfrac{1}{2^{10}}$$\n",
    "\n",
    "**Definition - Probability Mass Function (PMF) and Support**:\n",
    "\n",
    "- Probability mass function: $f(x) = P(X = x)$\n",
    "\n",
    "- Support of a probability function: The closure of the set $\\{x: f(x) > 0\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "**Theorem**: If $X$ has a discrete distribution, the the probability of each subset $C$ in the real line is:\n",
    "\n",
    "$$P(X \\in C) = \\sum_{x_i \\in C} f(x_i)$$\n",
    "\n",
    "**Theorem**: Let $X$ be a discrete random variable with p.f. (probability function) $f$. \n",
    "\n",
    "1. If $x$ is not one of the possible values of $X$, then $f(x) = 0$.\n",
    "1. If the sequence $x_1, x_2, \\cdots$ includes all the possible values of $X$, then $\\sum_{i=1}^{\\infty}f(x_i) = 1$.\n",
    "\n",
    "**Definition - Bernoulli Distribution**: A random variable $Z$ that takes only two values $0$ and $1$ with $P(Z = 1) = p$ has the ***Bernoulli distribution*** with parameter p. We also say that $Z$ is a Bernoulli random variable with parameter $p$ and denote $Z \\sim Bernoulli(p)$\n",
    "\n",
    "The distribution of the variable is:\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "  \\begin{cases}\n",
    "    1-p & \\text{for $x = 0$} \\\\\n",
    "    p & \\text{for $x = 1$} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "**Definition - Uniform Distribution**: Let $a \\leq b$ integers. Suppose that the value of a random variable $X$ is equally likely to be each of the integers $a, \\cdots, b$. Then we say that $X$ has the uniform distribution on the integers $a, \\cdots, b$.\n",
    "\n",
    "The distribution of the variable is:\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "  \\begin{cases}\n",
    "    \\dfrac{1}{b - a + 1} & \\text{for $x = a, \\cdots, b$} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "Suppose that we want to model the distribution of votes for a given politician. If each person has chance $p$, the number of voters is $n$. Then, the chance that the politician gets $x$ votes is:\n",
    "\n",
    "$$P(X = x) = {n \\choose x} p^x(1-p)^{n-x}$$\n",
    "\n",
    "**Definition - Binomial Distribution**: The discrete distribution represented by the probability function\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "  \\begin{cases}\n",
    "    {n \\choose x} p^x(1-p)^{n-x} & \\text{for $x = 0, 1, \\cdots, n$} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "is called the binomial distribution with parameters $n$ and $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. Suppose that two balanced dice are rolled, and let $X$ denote the absolute value of the difference between the two numbers that appear. Determine and sketch the p.f. of $X$.\n",
    "\n",
    "1. Suppose that a random variable X has a discrete distribution with the following probability function: $$\n",
    "f(x) =\n",
    "  \\begin{cases}\n",
    "    cx & \\text{for $x = 1, \\cdots, 5$} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$ Determine the value of the constant $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "**Check-in**\n",
    "\n",
    "1. Suppose that a random variable X has the uniform distribution on the integers $10, \\cdots, 20$. Find the probability that X is even.\n",
    "\n",
    "1. If 10 percent of the balls in a certain box are red, and if 20 balls are selected from the box at random, with replacement, what is the probability that more than three red balls will be obtained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous Distribution\n",
    "\n",
    "**Definition -- Continuous Distribution:** We say that $X$ is a continuous random variable if there exists a\n",
    "nonnegative function $f$, defined on the real line, such that for every interval of real numbers (bounded or unbounded), the probability that $X$ takes a value in the interval is the integral of $f$ over the interval.\n",
    "\n",
    "**Definition -- Probability Density Function:** If $X$ has a continuous distribution, the function $f$ is called the **probability density function** (abbreviated p.d.f.) of $X$. The closure of the set $\\{x: \\ f(x)>0 \\}$ is called the support of (the distribution of) $X$.\n",
    "\n",
    "Example notation:\n",
    "\n",
    "$$P(a \\leq X \\leq b) = \\int_a^bf(x)dx$$\n",
    "\n",
    "$$P(X \\geq a) = \\int_a^\\infty f(x)dx$$\n",
    "\n",
    "$$P(X \\leq b) = \\int_{-\\infty}^b f(x)dx$$\n",
    "\n",
    "$$P(X = b) = \\int_{b}^b f(x)dx = 0$$\n",
    "\n",
    "![img](./imgl4/im1.png)\n",
    "\n",
    "To think: What is the chance that a person is 6' tall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous Distribution\n",
    "\n",
    "Example:\n",
    "\n",
    "1. Suppose that the p.d.f. of a random variable X is as follows:\n",
    "\n",
    "$$f(x) =\n",
    "  \\begin{cases}\n",
    "    cx^2 & \\text{for $1 \\leq x \\leq 2$}, \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "    - Find the value of the constant $c$ and sketch the p.d.f.\n",
    "    - Find the value of $P(X > 3/2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous Distribution\n",
    "\n",
    "**Definition -- Uniform Distribution:** Let $a$ and $b$ be two given real numbers such that $a < b$. Let $X$ be $a$ random variable such that it is known that $a \\leq X \\leq b$ and, for every subinterval of $[a, b]$, the probability that $X$ will belong to that subinterval is proportional to the length of that subinterval. We then say that the random variable $X$ has the **uniform distribution on the interval $[a, b]$**.\n",
    "\n",
    "p.d.f: \n",
    "\n",
    "$$f(x) =\n",
    "  \\begin{cases}\n",
    "    \\dfrac{1}{b-a} & \\text{for $a \\leq x \\leq B$}, \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "![img](./imgl4/im2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous Distribution\n",
    "\n",
    "**Check-in**\n",
    "\n",
    "1. Suppose that a random variable $X$ has the uniform distribution on the interval $[−2, 8]$. Find the p.d.f. of $X$ and the value of $P(0 < X < 7)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cumulative Distribution Function\n",
    "\n",
    "**Definition -- Cumulative Distribution Function**: The distribution function or cumulative distribution function (abbreviated c.d.f.) $F$ of a random variable $X$ is the function:\n",
    "\n",
    "$$ F(x) = P(X \\leq x), \\quad \\text{ for } -\\infty \\leq x \\leq \\infty$$\n",
    "\n",
    "Note: Any distribution, regardless of the type, has a c.d.f.\n",
    "\n",
    "![img](./imgl4/im3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cumulative Distribution Function\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "1. $F$ is non-decreasing as $x$ increases. That is: if $x_1 < x_2$, then $F(x_1) \\leq F(x_2)$.\n",
    "\n",
    "1. $\\lim_{x \\rightarrow -\\infty} F(x) = 0$ and $\\lim_{x \\rightarrow \\infty} F(x) = 1$\n",
    "\n",
    "1. $P(X > x) = 1 - F(x)$\n",
    "\n",
    "1. $P(X > x) = 1 - F(x)$\n",
    "\n",
    "1. $P(x_1 < X < x_2) = F(x_2) - F(x_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cumulative Distribution Function\n",
    "\n",
    "Example:\n",
    "\n",
    "1. Suppose, that a random variable $X$ has the uniform distribution on the interval [−2, 8]. Find and sketch the c.d.f. of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cumulative Distribution Function\n",
    "\n",
    "**Check-in**\n",
    "\n",
    "1. Suppose that a random variable $X$ can take only the values $−2$, $0$, $1$, and $4$, and that the probabilities of these values are as follows: $P(X = −2) = 0.4$, $P(X = 0) = 0.1$, $P(X = 1) = 0.3$, and $P(X = 4) = 0.2$. Sketch the c.d.f. of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantile Function\n",
    "\n",
    "*Fair Bets.* Suppose that $X$ is the amount of rain that will fall tomorrow, and $X$ has c.d.f. $F$. Suppose that we want to place an even-money bet on $X$ as follows: If $X \\leq x_0$, we win one dollar and if $X>x_0$ we lose one dollar. \n",
    "\n",
    "*In order to make this bet fair, we need $P(X \\leq x_0) = P(X > x_0) = 1/2$.*\n",
    "\n",
    "We could search through all of the real numbers $x$ trying to find one such that $F(x) = 1/2$, and then we would let $x_0$ equal the value we found. \n",
    "\n",
    "If $F$ is a one-to-one function, then $F$ has an inverse $F^−1$ and $x_0 = F^{−1}(1/2)$.\n",
    "\n",
    "**Definition -- Quantiles/Percentiles:** Let $X$ be a random variable with c.d.f. $F$. For each $p$ strictly between $0$ and $1$, define $F^{−1}(p)$ to be the smallest value $x$ such that $F(x) \\geq p$. Then $F^{−1}(p)$ is called the $p$ quantile of $X$ or the $100p$ percentile of $X$. The function $F^{−1}$ defined here on the open interval $(0, 1)$ is called the quantile function of $X$.\n",
    "\n",
    "![img](./imgl4/im4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantile Functions\n",
    "\n",
    "**Fact -- Quantiles of Continuous Distributions:** When the c.d.f. of a random variable $X$ is continuous and one-to-one over the whole set of possible values of $X$, the inverse $F^{−1}$ of $F$ exists and equals the quantile function of $X$.\n",
    "\n",
    "Example:\n",
    "\n",
    "1. Compute the quantile function of the uniform distribution on the interval $[a, b]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantile Functions\n",
    "\n",
    "**Fact -- Quantiles of Discrete Distributions**: It is convenient to be able to calculate quantiles for discrete distributions as well.\n",
    "\n",
    "*Quantiles of a Binomial Distribution:* Let $X$ have the binomial distribution with parameters $n = 5$ and $p = 0.3$. The binomial table in the back of the book has the p.f. $f$ of $X$, which we reproduce here together with the $c.d.f.$ F:\n",
    "\n",
    "![img](./imgl4/im5.png)\n",
    "\n",
    "And the quantile function is:\n",
    "\n",
    "![img](./imgl4/im6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantile Functions\n",
    "\n",
    "**Definition -- Median/Quartiles**: The $1/2$ quantile or the 50-th percentile of a distribution is called its median. The $1/4$ quantile or 25-th percentile is the lower quartile. The $3/4$ quantile or 75-th percentile is called the upper quartile.\n",
    "\n",
    "Example: Suppose that $X$ has the p.d.f.\n",
    "\n",
    "$$f(x) =\n",
    "  \\begin{cases}\n",
    "    2x & \\text{for $0 \\leq x \\leq 1$}, \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "1. Find and sketch the c.d.f. or X.\n",
    "1. Find the quantile function and compute:\n",
    "    - The median\n",
    "    - 90-th percentile\n",
    "    - The quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "**Definition -- Joint/Bivariate Distribution**: Let $X$ and $Y$ be random variables. The joint distribution or bivariate distribution of $X$ and $Y$ is the collection of all probabilities of the form $P[(X,Y) \\in C]$ for all sets $C$ of pairs of real numbers such that $\\{(X, Y) \\in C\\}$ is an event.\n",
    "\n",
    "**Definition -- Discrete Joint Distribution**: Let $X$ and $Y$ be random variables, and consider the ordered pair $(X, Y)$. If there are only finitely or at most countably many different possible values $(x, y)$ for the pair $(X, Y)$, then we say that $X$ and $Y$ have a discrete joint distribution.\n",
    "\n",
    "**Definition -- Joint Probability Function, p.f.**: The joint probability function, or the joint p.f., of $X$ and\n",
    "$Y$ is defined as the function $f$ such that for every point $(x, y)$ in the xy-plane:\n",
    "\n",
    "$$f(x, y) = P(X = x \\text{ and } Y = y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "Example: What is the joint distribution of the clinical trial results below\n",
    "\n",
    "![img](./imgl4/im13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "**Properties**\n",
    "\n",
    "Let X and Y have a discrete joint distribution. \n",
    "\n",
    "1. If $(x, y)$ is ***not*** one of the possible values of the pair $(X, Y)$, then $f(x, y) = 0$.\n",
    "\n",
    "1. $\\sum_{\\text{All } (x,y)} f(x,y) = 1$\n",
    "\n",
    "1. $P[(X,Y) \\in C] = \\sum_{(x,y) \\in C}f(x,y)$\n",
    "\n",
    "Example: In a certain suburban area, each household reported the number of cars and the number of television sets that they owned. Let X stand for the number of cars owned by a randomly selected household in this area. Let Y stand for the number of television sets owned by that same randomly selected household. In this case, X takes only the values 1, 2, and 3; Y takes only the values 1, 2, 3, and 4; and the joint p.f. f of X and Y is:\n",
    "\n",
    "\n",
    "Joint pdf | Plot\n",
    "- | - \n",
    "![alt](./imgl4/im7.png) | ![alt](./imgl4/im8.png)\n",
    "\n",
    "\n",
    "How much is $P(X = 1)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "**Definition -- Continuous Joint Distribution/Joint p.d.f./Support**: Two random variables $X$ and $Y$ have a continuous joint distribution if there exists a nonnegative function $f$ defined over the entire xy-plane such that for every subset C of the plane,\n",
    "\n",
    "$$P[(X, Y) \\in C] = \\int_C\\int f(x, y)dxdy$$\n",
    " \n",
    "If the integral exists. The function $f$ is called the ***joint probability density function*** (abbreviated joint p.d.f.) of $X$ and $Y$. The closure of the set $\\{(x,y): \\ f(x, y) > 0\\}$ is called the ***support*** of (the distribution of) $(X, Y)$.\n",
    "\n",
    "![alt](./imgl4/im9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "Example -- Calculating a Normalizing Constant:\n",
    "\n",
    "Suppose that the joint p.d.f. of $X$ and $Y$ is specified\n",
    "as follows:\n",
    "\n",
    "$$f(x) =\n",
    "  \\begin{cases}\n",
    "    cx^2y & \\text{for $x^2 \\leq y \\leq 1$}, \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "(easy...ish) What is the value of $c$?\n",
    "(hard) What is $P(X \\geq Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "(hard) What is $P(X \\geq Y)$\n",
    "\n",
    "Slice 1 | Slice 2\n",
    "- | - \n",
    "![alt](./imgl4/im10.png) | ![alt](./imgl4/im11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "**Definition -- Mixed Bivariate Distributions:** Let $X$ and $Y$ be random variables such that $X$ is discrete and $Y$ is continuous. Suppose that there is a function $f(x, y)$ defined on the xy-plane such that, for every pair $A$ and $B$ of subsets of the real numbers,\n",
    "\n",
    "$$P(X \\in A \\text{ and } Y \\in B) = \\int_B\\sum_{x \\in A}f(x,y)dy$$\n",
    "\n",
    "If the integral exists. Then the function $f$ is called the joint p.f./p.d.f. of $X$ and $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "**Definition -- Joint (Cumulative) Distribution Function/c.d.f.**: The joint distribution function or joint cumulative distribution function (joint c.d.f.) of two random variables $X$ and $Y$ is defined as the function $F$ such that for all values of $x$ and $y$ ($-\\infty<x<\\infty$ and $-\\infty<y<\\infty$),\n",
    "\n",
    "$$F(x, y) = P(X \\leq x \\text{ and } Y \\leq y)$$\n",
    "\n",
    "Interesting: Assuming $a < b$ and $c < d$, compute\n",
    "\n",
    "$$P(a < X \\leq b \\text{ and } c < Y \\leq d)$$\n",
    "\n",
    "**Fact:** $$f(x,y) = \\dfrac{\\partial^2F(x,y)}{\\partial x \\partial y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "**Check-in**: \n",
    "\n",
    "Suppose that in an electric display sign there are three light bulbs in the first row and four light bulbs in the second row. Let $X$ denote the number of bulbs in the first row that will be burned out at a specified time $t$, and let $Y$ denote the number of bulbs in the second row that will be burned out at the same time $t$. Suppose that the joint p.f. of $X$ and $Y$ is as specified in the following table:\n",
    "\n",
    "![alt](./imgl4/im12.png)\n",
    "\n",
    "Compute:\n",
    "\n",
    "1. $P(X = 2)$\n",
    "1. $P(Y \\geq 2)$\n",
    "1. $P(X \\leq 2 \\text{ and } Y \\leq 2)$\n",
    "1. $P(X = Y)$\n",
    "1. $P(X > Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Marginal Distributions\n",
    "\n",
    "**Definition -- Marginal c.d.f./p.f./p.d.f.**: Suppose that $X$ and $Y$ have a joint distribution. The c.d.f. of\n",
    "$X$ is called the marginal c.d.f. of $X$. Similarly, the p.f. or p.d.f. of $X$ associated with the marginal c.d.f. of $X$ is called the marginal p.f. or marginal p.d.f. of $X$. \n",
    "\n",
    "**Definition -- Discrete Marginal c.d.f./p.f./p.d.f.**: If $X$ and $Y$ have a discrete joint distribution for which the joint p.f. is $f$, then the marginal p.f. $f_1$ of $X$ is\n",
    "\n",
    "$$f_1(x) = \\sum_{\\text{All }y}f(x, y)$$\n",
    "\n",
    "$f_2(y)$ is defined analogously.\n",
    "\n",
    "For the neighborhood example:\n",
    "\n",
    "![alt](./imgl4/im14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Marginal Distributions\n",
    "\n",
    "**Definition -- Continuous Marginal c.d.f./p.f./p.d.f.**: If $X$ and $Y$ have a continuous joint distribution with joint p.d.f. $f$, then the marginal p.d.f. $f_1$ of $X$ is\n",
    "\n",
    "$$f_1(x) = \\int_{-\\infty}^{\\infty}f(x,y)dy$$\n",
    "\n",
    "$f_2(y)$ is defined analogously.\n",
    "\n",
    "Pdf Slice | Marginal\n",
    "- | - \n",
    "![alt](./imgl4/im15.png) | ![alt](./imgl4/im16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independent Random Variables\n",
    "\n",
    "**Definition -- Independent Random Variables**: Two random variables X and Y are independent if, for every two sets $A$ and $B$ of real numbers such that $\\{X \\in A\\}$ and\n",
    "$\\{Y \\in B\\}$ are events,\n",
    "\n",
    "$$P(X \\in A \\text{ and } Y \\in B) = P(X \\in A) P(Y \\in B)$$\n",
    "\n",
    "**Fact**: Two random variables $X$ and $Y$ are independent if and only if the following factorization is satisfied for all real numbers x and y:\n",
    "\n",
    "$$f(x, y) = f_1(x)f_2(y)$$\n",
    "\n",
    "**Interpretation**: \n",
    "\n",
    "- For events: Learning that one of them occurs does not change the probability that the other one occurs.\n",
    "\n",
    "- For r.v.s (random variables): Learning a realization of one does not change the chances of the other.\n",
    "\n",
    "![alt](./imgl4/im17.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independent Random Variables\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose that the joint p.d.f. of $X$ and $Y$ is as follows:\n",
    "\n",
    "$$f(x, y) =\n",
    "  \\begin{cases}\n",
    "    \\dfrac{15}{4}x^2 & \\text{for $0 \\leq y \\leq 1-x^2$}, \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "1. Determine the marginal p.d.f.’s of $X$ and $Y$.\n",
    "1. Are $X$ and $Y$ independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Marginal Distributions\n",
    "\n",
    "**Check-in**: Suppose that $X$ and $Y$ have a continuous joint distribution for which the joint p.d.f. is\n",
    "\n",
    "$$f(x, y) =\n",
    "  \\begin{cases}\n",
    "    k & \\text{for $a \\leq x \\leq b$ and $c \\leq y \\leq d$}, \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "Find the marginal distributions of $X$ and $Y$. Are $X$ and $Y$ independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Distributions\n",
    "\n",
    "**Definition -- Conditional Distribution/p.f.**: Let $X$ and $Y$ have a discrete joint distribution with joint\n",
    "p.f. $f$. Let $f_2$ denote the marginal p.f. of $Y$. For each $y$ such that $f_2(y) > 0$, define:\n",
    "\n",
    "$$g_1(x|y) = \\dfrac{f(x, y)}{f2(y)}$$\n",
    "\n",
    "Then $g_1$ is called the **conditional p.f. of X given Y**. The discrete distribution whose p.f. is $g_1(.|y)$ is called the **conditional distribution of X given that $Y = y$**.\n",
    "\n",
    "Example: Auto insurance\n",
    "\n",
    "Pdf Slice | Marginal\n",
    "- | - \n",
    "![alt](./imgl4/im18.png) | ![alt](./imgl4/im19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Distributions\n",
    "\n",
    "![alt](./imgl4/im20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence\n",
    "\n",
    "**Theorem -- Implications of Independence**: Let $X$ and $Y$ two random variables with joint *p.f.* $f$. The following statements are equivalent:\n",
    "\n",
    "1. $X \\perp Y$\n",
    "2. $\\forall x,y \\in \\mathbb{R}$, $f(x, y) = f_{X}(x)f_{Y}(y)$\n",
    "3. $\\forall x \\in \\mathbb{R}$ and $y \\in \\text{Supp}[Y]$, $f_{X|Y}(x|y) = f_{X}(x)$\n",
    "4. $\\forall D, E \\subseteq \\mathbb{R}$, the events $\\{X \\in D\\}$ and $\\{Y \\in E\\}$ are independent.\n",
    "5. For all functions $g$ of $X$ and $h$ of $Y$ $g(X) \\perp h(Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate Generalizations\n",
    "\n",
    "- (Definition) **Random Vectors**: A random vector is a function $X: S \\rightarrow \\mathbb{R}^{K}$ such that, for all event A, $\\bX(A) \\ = \\ \\bigg( X_{1}(A), \\cdots, X_{K}(A) \\bigg)$. And where $X_{i}$ is a random variable.\n",
    "\n",
    "- (Definition) **Joint Cumulative Density Function**: For a random vector $\\bX$, evaluated at $\\bvx$, is denoted $F(\\bvx) = \\mathbb{P}\\left[ \\bX \\leq \\bvx \\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate Generalizations\n",
    "\n",
    "And from here, the extensions are evident:\n",
    "\n",
    "- For continuous random vectors, we use multiple integrals (*CDF*s) or partial derivatives (*PDF*s)\n",
    "\n",
    "- For discrete random vectors, we use multiple sums (*CDF*s).\n",
    "\n",
    "- For the conditional *PDFs/PMFs*, we marginalize on the variables by summing up across their support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- Statistics would not be very useful if we did not find ways to summarize the objects we work with.\n",
    "\n",
    "- **Definition -- Expected Value**: For a random variable $X$ with bounded variation:\n",
    "  1. If $X$ is discrete, then $\\Ex[X] \\ = \\ \\sum_{x}xf(x)$\n",
    "  + If $X$ is continuous, then $\\Ex[X] \\ = \\ \\int_{-\\infty}^{\\infty}xf(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Theorem) **Expectation of a Function**: For a random variable $X$ with *PMF/PDF* $f$ and a function $g$ (still assuming bounded variation):\n",
    "  1. If $X$ is discrete, then $\\Ex [g(X)] \\ = \\ \\sum_{x}g(x)f(x)$\n",
    "  + If $X$ is continuous, then $\\Ex [g(X)] \\ = \\ \\int_{-\\infty}^{\\infty}g(x)f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Theorem) **Linearity of Expected Values**: For a random variable $X$, and $a, b \\in \\mathbb{R}$, then:\n",
    "\n",
    "$$\\Ex[aX + b] \\ = \\ a\\Ex[X] + b$$\n",
    "\n",
    "- (Definition) **Expectation of a Bivariate Random Vector**: For a random vector $(X,Y)$, the expected value is $\\Ex [(X, Y)] \\ = \\ \\bigg(\\Ex[X], \\Ex[Y] \\bigg)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Definition) **Raw Moment**: For a random variable $X$, the $j$-th *raw moment* is defined as:\n",
    "\n",
    "$$ \\mu_{j}' = \\Ex [X^{j}] $$\n",
    "\n",
    "- (Definition) **Central Moment**: For a random variable $X$, the $j$-th *central moment* is defined as:\n",
    "\n",
    "$$ \\mu_{j} = \\Ex \\left[(X -\\Ex [X])^{j} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Definition) **Variance**: For a random variable $X$, the variance is defined as the *second central moment* of $X$:\n",
    "\n",
    "$$ \\Vax [X] = \\Ex \\left[(X -\\Ex [X])^{2} \\right] $$\n",
    "\n",
    "- (Definition) **Standard Deviation**: For a random variable $X$, the variance is defined as $\\sigma [X] = \\sqrt{\\Vax[X]}$\n",
    "\n",
    "- (Theorem) **Alternative formula for the Variance**: For a random variable $X$, the variance is defined as the *second central moment* of $X$:\n",
    "\n",
    "$$ \\Vax [X] = \\Ex [X^2] - \\left[ \\Ex [X] \\right]^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Theorem) **Algebra of the Variance**: For a random variable $X$ and $a \\in \\mathbb{R}$, $b \\in \\mathbb{R}$:\n",
    "\n",
    "$$ \\Vax [aX + b] \\ = \\ a^{2} \\Vax [X] $$\n",
    "\n",
    "- (Corollary) **Algebra of the Standard Deviations**: Let a random variable $X$, $a \\in \\mathbb{R}$, and $b \\in \\mathbb{R}$. Then, $\\sigma [aX + b] = |a| \\sigma [X]$.\n",
    "\n",
    "- (Theorem) **Chebyshev Inequality**: Let a random variable $X$ and $\\sigma [X] > 0$. Then, $\\forall \\epsilon > 0$:\n",
    "\n",
    "$$ \\mathbb{P} \\bigg[ \\big| X - \\Ex[X] \\big| \\geq \\epsilon \\sigma [X] \\bigg] \\leq \\dfrac{1}{\\epsilon^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Definition) **Normal Distribution**: A continuous random variable $X$ follows a normal distribution (denoted by $X \\sim N(\\mu, \\sigma^2)$) if:\n",
    "\n",
    "$$ f(x) \\ = \\ \\dfrac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\dfrac{(x - \\mu)^{2}}{2\\sigma^2}} $$\n",
    "\n",
    "- (Theorem) **Mean and Standard Deviation of a Normal Distribution**: Let $X \\sim N(\\mu, \\sigma^2)$. Then, $\\Ex[X] \\ = \\ \\mu$ and $\\sigma[X] \\ = \\ \\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Theorems) **Algebra of Normal Distributions**: Let $X \\sim N(\\mu_{X}, \\sigma_{X}^2)$, $Y \\sim N(\\mu_{Y}, \\sigma_{Y}^2)$, and $a, b \\in \\mathbb{R}$, $a \\neq 0$. Then:\n",
    "  1. If $W = aX + b$, then $W \\sim N(a\\mu_{X} + b, a^2\\sigma_{X}^2)$\n",
    "  2. If $X \\ind Y$, and $Z = X + Y$, then $Z \\sim N(\\mu_{X} + \\mu_{Y}, \\sigma_{X}^2 + \\sigma_{Y}^2)$\n",
    "\n",
    "- This proof should be straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- And one of the most important objects: suppose we want to say how well a random variable $X$ approximates a given value $c \\in \\mathbb{R}$.\n",
    "\n",
    "- The most common metric we use for this purpose is the *Mean Squared Error*.\n",
    "\n",
    "- (Definition) **Mean Squared Error**: The *MSE* of a random variable $X$ about $c$ is equal to: $\\Ex[(X - c)^2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Theorem) **Alternative formulation for the Mean Squared Error**: The *MSE* of a random variable about $c$ is equal to:\n",
    "\n",
    "$$ \\Ex[(X - c)^2] \\ = \\ \\Vax[X] + (\\Ex[X] - c)^2 $$\n",
    "\n",
    "- (Theorem) **Mean Squared Error Minimization**: The value $c$ that minimizes the *MSE* of a random variable $X$ about $c$ is $c = \\Ex[X]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions (Joint Distributions)\n",
    "\n",
    "- (Definition) **Covariance**: The covariance of two random variables is defined as \n",
    "$$\\text{Cov}[X, Y] = \\big[ \\big(X - \\Ex[X]\\big)\\big(Y - \\Ex[Y]\\big) \\big] = \\Ex[XY] - \\Ex[X]\\Ex[Y]$$\n",
    "\n",
    "- (Theorem) **Variance Rule**: Let $X$ and $Y$ two r.v. Then,\n",
    "\n",
    "1. $$\\Vax[X + Y] = \\Vax[X] +2\\text{Cov}[X, Y] + \\Vax[Y]$$\n",
    "\n",
    "2. And if $a, b, c \\in \\real$, then $$\\Vax[aX + bY + c] = a^2\\Vax[X] +2ab\\text{Cov}[X, Y] + b^2\\Vax[Y]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions (Joint Distributions)\n",
    "\n",
    "- (Theorem) **Algebra of Covariance**: Let $X$, $Y$, $Z$, $W$ four r.v.s and $a,b,c,d \\in \\real$. Then:\n",
    "\n",
    "1. $\\text{Cov}[X, c]=\\text{Cov}[c, X]=\\text{Cov}[d, c]$\n",
    "2. $\\text{Cov}[X, Y] = \\text{Cov}[Y, X]$\n",
    "3. $\\text{Cov}[X, X] = \\Vax[X]$\n",
    "4. \n",
    "$$\\text{Cov}[X + W, Y + Z]=\\text{Cov}[X, Z]+\\text{Cov}[X, Y]+\\text{Cov}[W, Y]+\\text{Cov}[W,Z]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions (Joint Distributions)\n",
    "\n",
    "(Definition) **Correlation**: The correlation of two random variables $X$ and $Y$, with $\\sigma[X]>0$ and $\\sigma[Y]>0$ is defined as \n",
    "$$\\rho[X, Y] = \\dfrac{\\cov[X,Y]}{\\sigma[X]\\sigma[Y]}$$\n",
    "\n",
    "(Theorem) **Correlation and Linear Dependence**: Let two random variables $X$ and $Y$, with $\\sigma[X]>0$ and $\\sigma[Y]>0$. Then:\n",
    "\n",
    "1. $\\rho[X, Y] \\in [-1, 1]$\n",
    "\n",
    "2. For $a, b \\in \\real$, and $Y = aX + b$:\n",
    "    - $\\rho[X, Y] = 1$ if $a > 0$\n",
    "    - $\\rho[X, Y] = -1$ if $a < 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions (Joint Distributions)\n",
    "\n",
    "(Theorem) **Properties of Correlation**: Let random variables $X$, $Y$, and $Z$, all with variance higher than zero. Let also $a,b,c,d \\in \\real$.\n",
    "1. $\\rho[X, Y] = \\rho[Y, X]$\n",
    "2. $\\rho[X, X] = 1$\n",
    "3. If $ab>0$, then $\\rho[aX + c, bY + d] = \\rho[X,Y]$\n",
    "4. If $ab<0$, then $\\rho[aX + c, bY + d] = -\\rho[X,Y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence\n",
    "\n",
    "(Theorem) **Independence**: Let $X$ and $Y$ two independents r.v.s. Then:\n",
    "1. $\\rho[X, Y] = 0$\n",
    "2. $\\cov[X, Y] = 0$\n",
    "3. $\\Ex[XY] = \\Ex[X]\\Ex[Y]$\n",
    "4. $\\Vax[X + Y] = \\Vax[X] + \\Vax[Y]$\n",
    "\n",
    "- However, these statements are not equivalent, since you may have $\\rho[X, Y] = 0$ without having independence!\n",
    "  + See pages 65 and 66 of the Aronow and Miller book (especially the footnote on pg 66 for a notable exception)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Functions\n",
    "\n",
    "(Definition) **Conditional Expectation**: For two random variables $X$ and $Y$, the conditional expectation of $Y$ given $X=x$ is:\n",
    "\n",
    "1. If $X$ and $Y$ are discrete, and $x \\in \\text{Supp}[X]$, then $$\\Ex[Y|X=x] = \\sum_{y}yf_{Y|X}(y|x)$$\n",
    "\n",
    "2. If $X$ and $Y$ are continuous, and $x \\in \\text{Supp}[X]$, then $$\\Ex[Y|X=x] = \\int_{-\\infty}^{\\infty}yf_{Y|X}(y|x)dy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Functions\n",
    "\n",
    "(Definition) **Conditional Expectation of a function**: For two random variables $X$ and $Y$ and a function of $X$ and $Y$, the conditional expectation of $h(X,Y)$ given $X=x$ is:\n",
    "\n",
    "1. If $X$ and $Y$ are discrete, and $x \\in \\text{Supp}[X]$, then $$\\Ex[h(X,Y)|X=x] = \\sum_{y}h(x,y)f_{Y|X}(y|x)$$\n",
    "\n",
    "2. If $X$ and $Y$ are continuous, and $x \\in \\text{Supp}[X]$, then $$\\Ex[h(X,Y)|X=x] = \\int_{-\\infty}^{\\infty}h(x,y)f_{Y|X}(y|x)dy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Functions\n",
    "\n",
    "(Theorem) **Linearity of Conditional Expectation**: Let $X$ and $Y$ rvs. If $g$ and $h$ are functions (with $x \\in \\text{Supp}[X]$ is: \n",
    "\n",
    "$$\\Ex[g(X)Y + h(X) | X=x] = g(x)\\Ex[Y | X=x] + h(x)$$\n",
    "\n",
    "(Definition) **Conditional Expectation Function**: Let $X$ and $Y$ rvs with joint distribution $f$ ($x \\in \\text{Supp}[X]$) is: \n",
    "\n",
    "$$G_Y(x) = \\Ex[Y | X=x]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Functions\n",
    "\n",
    "(Theorem) **Law of Iterated Expectations**: Let $X$ and $Y$ rvs. \n",
    "\n",
    "$$\\Ex[Y] = \\Ex\\big[\\Ex[Y | X]\\big]$$\n",
    "\n",
    "(Theorem) **Law of Total Variance**: Let $X$ and $Y$ rvs. \n",
    "\n",
    "$$\\Vax[Y] = \\Ex\\big[\\Vax[Y | X]\\big] + \\Vax\\big[\\Ex[Y | X]\\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Functions\n",
    "\n",
    "(Theorem) **Properties of Deviations from the CEF**: Let $X$ and $Y$ rvs and let $\\epsilon = Y - \\Ex[Y|X]$.\n",
    "\n",
    "1. $\\Ex[\\epsilon|X] = 0$\n",
    "2. $\\Ex[\\epsilon] = 0$\n",
    "3. If $g$ is a function of $X$, $\\cov\\big[g(X),\\epsilon \\big] = 0$\n",
    "4. $\\Vax[\\epsilon|X] = \\Vax[Y|X]$\n",
    "5. $\\Vax[\\epsilon] = \\Ex\\big[\\Vax[Y|X]\\big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Functions\n",
    "\n",
    "- (Theorem) **Properties of Deviations from the CEF**: Let $X$ and $Y$ rvs and let $\\epsilon = Y - \\Ex[Y|X]$.\n",
    "\n",
    "1. $\\Ex[\\epsilon|X] = 0$\n",
    "2. $\\Ex[\\epsilon] = 0$\n",
    "3. If $g$ is a function of $X$, $\\cov\\big[g(X),\\epsilon \\big] = 0$\n",
    "4. $\\Vax[\\epsilon|X] = \\Vax[Y|X]$\n",
    "5. $\\Vax[\\epsilon] = \\Ex\\big[\\Vax[Y|X]\\big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Function\n",
    "\n",
    "(Theorem) **CEF and Best Linear Predictor**: Let $X$ and $Y$ rvs, $\\Ex[Y|X]$ is the best predictor of $Y$ given $X$.\n",
    "\n",
    "(Theorem) **Best Linear Predictor**: Let $X$ and $Y$ rvs, if $\\Vax[X] > 0$, then the BLP of $Y$ given $X$ is $g(X) = \\beta_0 + \\beta_1X$ where:\n",
    "\n",
    "1. $\\beta_0 = \\Ex[Y] - \\beta_1\\Ex[X]$\n",
    "\n",
    "2. $\\beta_1 = \\dfrac{\\cov[X,Y]}{\\Vax[X]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Function\n",
    "\n",
    "(Theorem) **Implications of Independence**: Let $X$ and $Y$ independent rvs.:\n",
    "\n",
    "1. $\\Ex[Y|X] = \\Ex[Y]$\n",
    "2. $\\Vax[Y|X] = \\Vax[Y]$\n",
    "3. The BLP of $Y$ given $X$ is $\\Ex[Y]$\n",
    "4. If $g$ is a function of $X$ and $h$ is a function of $Y$:\n",
    "    - $\\Ex[h(Y)|g(X)] = \\Ex[h(Y)]$\n",
    "    - The BLP of $h(Y)$ given $g(X)$ is $\\Ex[h(X)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Great work!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
