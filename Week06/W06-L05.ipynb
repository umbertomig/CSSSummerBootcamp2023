{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b8e7e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSS 201 / 202 - CSS Bootcamp\n",
    "\n",
    "## Week 06 - Lecture 05\n",
    "\n",
    "### Umberto Mignozzetti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b027f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c5cf0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Loading the relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Plotting things:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "# Little bit of image processing\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# Loading scikit learn relevant packages\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, get_scorer_names, mean_squared_error, r2_score, mean_squared_error, roc_auc_score, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, SplineTransformer\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import VotingClassifier, BaggingRegressor, BaggingClassifier, RandomForestRegressor, RandomForestClassifier, AdaBoostClassifier, AdaBoostRegressor, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# CNN processing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, BatchNormalization, MaxPool2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import set_random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9770c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Education Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696a377",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Education Expenditure Dataset\n",
    "educ = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/educexp.csv')\n",
    "educy = educ['education']\n",
    "educX = educ[['income', 'young', 'urban']]\n",
    "\n",
    "## Polynomials\n",
    "for power in range(2, 4):\n",
    "    for var in ['income', 'young', 'urban']:\n",
    "        educX[var + '_' + str(power)] = educX[var] ** power\n",
    "        \n",
    "## Standardizing the X variables\n",
    "scaler = StandardScaler()\n",
    "educX_scaled = scaler.fit_transform(educX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3ab58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Civil War Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6629a71c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "civwars = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/mshk-pa-2017/SambanisImp.csv')\n",
    "\n",
    "## Target\n",
    "target = \"warstds\"\n",
    "\n",
    "## Predictors\n",
    "predictors = [\"ager\", \"agexp\", \"anoc\", \"army85\", \"autch98\", \"auto4\",\"autonomy\", \"avgnabo\", \n",
    "              \"centpol3\", \"coldwar\", \"decade1\", \"decade2\",\"decade3\", \"decade4\", \"dem\", \n",
    "              \"dem4\", \"demch98\", \"dlang\", \"drel\", \"durable\", \"ef\", \"ef2\", \"ehet\", \"elfo\", \n",
    "              \"elfo2\", \"etdo4590\", \"expgdp\", \"exrec\", \"fedpol3\", \"fuelexp\", \"gdpgrowth\", \n",
    "              \"geo1\", \"geo2\", \"geo34\", \"geo57\", \"geo69\", \"geo8\", \"illiteracy\", \"incumb\", \n",
    "              \"infant\", \"inst\", \"inst3\", \"life\", \"lmtnest\", \"ln_gdpen\", \"lpopns\", \"major\", \n",
    "              \"manuexp\", \"milper\", \"mirps0\", \"mirps1\", \"mirps2\", \"mirps3\", \"nat_war\", \n",
    "              \"ncontig\", \"nmgdp\", \"nmdp4_alt\", \"numlang\", \"nwstate\", \"oil\", \"p4mchg\", \n",
    "              \"parcomp\", \"parreg\", \"part\", \"partfree\", \"plural\", \"plurrel\", \"pol4\", \"pol4m\", \n",
    "              \"pol4sq\", \"polch98\", \"polcomp\", \"popdense\", \"presi\", \"pri\", \"proxregc\", \n",
    "              \"ptime\", \"reg\", \"regd4_alt\", \"relfrac\", \"seceduc\", \"second\", \"semipol3\", \"sip2\", \n",
    "              \"sxpnew\", \"sxpsq\", \"tnatwar\", \"trade\", \"warhist\", \"xconst\"]\n",
    "\n",
    "civwars = civwars[[target] + predictors]\n",
    "civwarsy = civwars[target]\n",
    "civwarsX = civwars[predictors]\n",
    "civwarsX_train, civwarsX_test, civwarsy_train, civwarsy_test = train_test_split(civwarsX, civwarsy, test_size = 0.25, stratify = civwarsy, random_state = 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b41c38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Women in Parliament Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2641c92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Countries Dataset\n",
    "wip = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/countrdat.csv')\n",
    "wipy = wip['wdi_wip']\n",
    "wipX = wip[['wdi_expedu', 'pwt_pop', 'mad_gdppc']]\n",
    "wipX = wipX.join(pd.get_dummies(wip.ccodealp, drop_first = True, prefix = 'country'))\n",
    "wipX = wipX.join(pd.get_dummies(wip.year, drop_first = True, prefix = 'year'))\n",
    "wipX_train, wipX_test, wipy_train, wipy_test = train_test_split(wipX, wipy, test_size = 0.25, random_state = 4321)\n",
    "wipX.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0302f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ANES Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddba917",
   "metadata": {},
   "outputs": [],
   "source": [
    "anes = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/anes2016.csv')\n",
    "anesX = anes[['pay_attn_pol_cont', 'int_follow_campg', 'anything_like_dem_cand',\n",
    "              'anything_like_rep_cand', 'approve_congr', 'things_right_track',\n",
    "              'feel_dem_cand_cont', 'feel_rep_cand_cont', 'how_many_live_hh_cont',\n",
    "              'better_1y_ago_cont', 'has_hinsur', 'favor_aca', 'afraid_dem_cand',\n",
    "              'disgust_dem_cand', 'afraid_rep_cand', 'disgust_rep_cand',\n",
    "              'lib_con_scale_cont', 'incgap_morethan_20y_ago', 'economy_improved',\n",
    "              'unempl_improved', 'speaksmind_dem_cand', 'speaksmind_rep_cand',\n",
    "              'soc_spend_favor_cont', 'def_spend_favor_cont', 'private_hi_favor_cont',\n",
    "              'shoud_hard_buy_gun', 'favor_affirmaction', 'govt_benefit_all',\n",
    "              'all_ingovt_corrup', 'election_makegovt_payattn',\n",
    "              'global_warming_happen', 'favor_death_penalty',\n",
    "              'econ_better_since_2008', 'relig_important', 'age_cont', 'married',\n",
    "              'schooling_cont', 'latinx', 'white', 'black', 'both_parents_bornUS',\n",
    "              'any_grandparent_foreign', 'rent_home', 'has_unexp_passap',\n",
    "              'should_roughup_protestors', 'justified_useviolence',\n",
    "              'consider_self_feminist', 'ppl_easily_offended_nowadays',\n",
    "              'soc_media_learn_pres', 'satisfied_life']]\n",
    "anesy = anes.swing_2016_2012"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4046a13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b879690",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supporting Vector Machines\n",
    "\n",
    "- The idea is to try to separate targets to be classified using a hyperplane on the feature space.\n",
    "\n",
    "- SVMs extend the use of a line by cleverly changing the data.\n",
    "\n",
    "- This approach is used for binary feature classification.\n",
    "\n",
    "- But it may be extended to multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60396a32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "Let $n$ training observations, $x_1, \\cdots, x_n$ and a vector of labels $y_1, \\cdots, y_n \\in \\{-1, 1\\}$. Also, let $C$ be a measure of tolerance for wrong classifications. The support vector classifier consists in:\n",
    "\n",
    "\\begin{align} \n",
    "\\max_{\\beta_j, \\varepsilon_i M} M & \\\\\n",
    "\\text{ subject to (1)} & \\quad \\sum_j \\beta_j^2 = 1 \\\\ \n",
    "\\text{(2)} & \\ \\quad y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}) \\geq M (1 - \\varepsilon_i) \\\\\n",
    "\\text{(3)} & \\ \\quad \\varepsilon_i \\geq 0 \\\\\n",
    "\\text{(4)} & \\quad \\sum_i \\varepsilon_i \\leq C\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2257091",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/svm6.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033e7aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "Benefits:\n",
    "\n",
    "1. $C$ indexes how much we tolerate wrong predictions.\n",
    "    + it is effectively the number of observations that we accept classify wrong.\n",
    "\n",
    "2. If, for a given $i$, $\\varepsilon_i = 0$, then the $i$ is correctly classified.\n",
    "\n",
    "3. If, for a given $i$, $\\varepsilon_i > 0$ (in this case, $\\varepsilon_i = 1$), then the $i$ is **incorrectly** classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c58bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "Benefits:\n",
    "\n",
    "4. **Important**: Only observations on the wrong side affect the classifier!\n",
    "\n",
    "- This is important and different from all other classifiers:\n",
    "    + LDA: Depends on the mean of all observations within one class.\n",
    "    + Logistic Regression: Also good, but still weakly (in fact, very weakly) affected by observations far from the decision boundary.\n",
    "\n",
    "- Support Vector Classifier: Only affected by cases around the supporting vector!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716eec95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "Negative:\n",
    "\n",
    "1. Linear assumption means that there will always be datasets with poor classification.\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/svm7.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24850020",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- Now, let us drop the linear assumption.\n",
    "\n",
    "- To do so, we note that the restriction (2) in the problem above in:\n",
    "\n",
    "$$ \\quad y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}) \\geq M (1 - \\varepsilon_i) $$\n",
    "\n",
    "- And this is the hyperplane:\n",
    "\n",
    "$$ \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a2ec1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- But any hyperplane is composed of something called an inner product:\n",
    "\n",
    "$$ \\beta_0 + \\bf{\\beta}'\\bf{x} $$\n",
    "\n",
    "- And if we call $f(x) = \\beta_0 + \\bf{\\beta}'\\bf{x}$, we may redefine the bounds of our classifier.\n",
    "\n",
    "- For instance, if we want quadratic bounds:\n",
    "\n",
    "$$ f(x) = \\beta_0 + \\bf{\\beta}_1'\\bf{x} + \\bf{\\beta}_2\\bf{x}'I\\bf{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76b1d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- And to generalize:\n",
    "\n",
    "$$ f(x) = \\beta_0 + \\sum_i \\alpha_i \\langle \\bf{x},x_i \\rangle $$\n",
    "\n",
    "- Where $\\langle \\bf{x},x_i \\rangle$ is the [inner product](https://en.wikipedia.org/wiki/Inner_product_space) of two observations.\n",
    "\n",
    "- The inner product is *sort like the multiplication operation for vectors*.\n",
    "\n",
    "- Support vector machines toy with these definitions of inner products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2f0c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- The generalization is called the `kernel`.\n",
    "\n",
    "- The **kernel** will be a function that quantifies the similarity between two observations.\n",
    "\n",
    "- *Linear Kernel* (supporting vector classifier):\n",
    "\n",
    "$$ K(x_i, x_i') = \\sum_j x_{ij}x_{i'j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5bf74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- *Polynomial Kernel* (d > 1):\n",
    "\n",
    "$$ K(x_i, x_i') = \\left[1 + \\sum_j x_{ij}x_{i'j} \\right]^d $$\n",
    "\n",
    "- *Radial Kernel*:\n",
    "\n",
    "$$ K(x_i, x_i') = \\exp\\left[-\\gamma\\sum_j (x_{ij} - x_{i'j})^2 \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242428cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Poly Kernel ($d = 3$) and Radial Kernel:\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/svm8.png?raw=true)\n",
    "\n",
    "- Next class: fitting SVMs in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639cf822",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- Let us use SVM to classify the vote for Pinochet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6addc8f4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Chile Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc34ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Loading Chile data\n",
    "chile = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/chilesurvey.csv')\n",
    "chile_clean = chile.dropna()\n",
    "chile_clean = chile_clean[chile_clean['vote'].isin(['Y', 'N'])]\n",
    "chile_clean['vote'] = np.where(chile_clean['vote'] == 'Y', 1, 0)\n",
    "chile_clean['logincome'] = np.log(chile_clean['income'])\n",
    "chile_clean['logpop'] = np.log(chile_clean['population'])\n",
    "dummies = pd.get_dummies(chile_clean['sex'], prefix = 'sex', drop_first = True)\n",
    "chile_clean = pd.concat([chile_clean, dummies], axis=1)\n",
    "dummies = pd.get_dummies(chile_clean['region'], prefix = 'region', drop_first = True)\n",
    "chile_clean = pd.concat([chile_clean, dummies], axis=1)\n",
    "dummies = pd.get_dummies(chile_clean['education'], prefix = 'education', drop_first = True)\n",
    "chile_clean = pd.concat([chile_clean, dummies], axis=1)\n",
    "chile_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86486fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chile Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85cf3f1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Chile data\n",
    "chiley = chile_clean['vote']\n",
    "chileX = chile_clean[['age', 'statusquo']]\n",
    "chileX_train, chileX_test, chiley_train, chiley_test = train_test_split(chileX, chiley, test_size = 0.3, stratify = chiley, random_state = 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05d332",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf08c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Linear SVM (Support Vector Classifier)\n",
    "svc = SVC(kernel = 'linear').fit(chileX_train, chiley_train)\n",
    "chiley_pred = svc.predict(chileX_test)\n",
    "print(confusion_matrix(chiley_test, chiley_pred))\n",
    "print(classification_report(chiley_test, chiley_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebae820",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d28ebf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(svc, chileX_test, chiley_test,\n",
    "        display_labels = ['No', 'Yes'],\n",
    "        cmap = plt.cm.Blues, normalize = 'true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d98919",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eedbd8",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Decision Function with Support Vectors\n",
    "plt.scatter(chileX_train.age, \n",
    "            chileX_train.statusquo, \n",
    "            c = chiley_train, \n",
    "            s = 15,\n",
    "            cmap=plt.cm.Paired, \n",
    "            alpha = 0.5)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(svc,\n",
    "                                       chileX_train, \n",
    "                                       plot_method = \"contour\",\n",
    "                                       colors = \"k\", \n",
    "                                       levels = [-1, 0, 1],\n",
    "                                       alpha = 0.5, \n",
    "                                       linestyles = [\"--\", \"-\", \"--\"],\n",
    "                                       ax = ax)\n",
    "\n",
    "ax.scatter(svc.support_vectors_[:, 0],\n",
    "           svc.support_vectors_[:, 1],\n",
    "           s = 25,\n",
    "           linewidth = 0.9,\n",
    "           facecolors = \"none\",\n",
    "           edgecolors = \"k\",\n",
    "           alpha = 1)\n",
    "\n",
    "plt.ylim(-1.5, 1.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b980278",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "**Check-in**: Add all the variables to your classifier. You will lose the decision boundaries viz but probably have a better prediction. Is that the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb3721",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e848894",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- How do we optimize these results?\n",
    "\n",
    "- There are many ways to improve here. \n",
    "\n",
    "Let us learn how to check the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246562a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Find parameters\n",
    "svc.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c64568",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- We usually focus on $C$.\n",
    "\n",
    "- Here, we will use GridSearch to find the best $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8d973",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Cs\n",
    "Cs = np.logspace(0, 2, 50)\n",
    "print(Cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f39a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960b6b0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Choosing the best C\n",
    "# Model\n",
    "svc = SVC(kernel = 'linear')\n",
    "\n",
    "# Parameters for gridsearch\n",
    "param_grid = {'C': Cs}\n",
    "\n",
    "# Build the GridSearch\n",
    "search = GridSearchCV(svc, param_grid, n_jobs = -1)\n",
    "search.fit(chileX_train, chiley_train)\n",
    "\n",
    "# Results\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "\n",
    "## Using the best parameters\n",
    "svc = search.best_estimator_\n",
    "y_pred = svc.predict(chileX_test)\n",
    "print(confusion_matrix(chiley_test, chiley_pred))\n",
    "print(classification_report(chiley_test, chiley_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcddadca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a0b36",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(svc, \n",
    "                                      chileX_test, \n",
    "                                      chiley_test,\n",
    "                                      display_labels = ['No', 'Yes'],\n",
    "                                      cmap = plt.cm.Blues, \n",
    "                                      normalize = 'true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d21e93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1765c6",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Decision Function with Support Vectors\n",
    "plt.scatter(chileX_train.age, \n",
    "            chileX_train.statusquo, \n",
    "            c = chiley_train, \n",
    "            s = 15, \n",
    "            cmap=plt.cm.Paired, \n",
    "            alpha = 0.5)\n",
    "\n",
    "ax = plt.gca()\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    svc,\n",
    "    chileX_train,\n",
    "    plot_method = \"contour\",\n",
    "    colors = \"k\",\n",
    "    levels = [-1, 0, 1],\n",
    "    alpha = 0.5,\n",
    "    linestyles = [\"--\", \"-\", \"--\"],\n",
    "    ax = ax)\n",
    "\n",
    "ax.scatter(\n",
    "    svc.support_vectors_[:, 0],\n",
    "    svc.support_vectors_[:, 1],\n",
    "    s = 25,\n",
    "    linewidth = 0.9,\n",
    "    facecolors = \"none\",\n",
    "    edgecolors = \"k\",\n",
    "    alpha = 1\n",
    ")\n",
    "\n",
    "plt.ylim(-1.5, 1.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0835afb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- Now, let us change the `kernel`. We will start with a polynomial kernel to warm up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cee013",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Polynomial SVM (Support Vector Classifier) ## btw: default is cubic\n",
    "svc = SVC(kernel = 'poly').fit(chileX_train, chiley_train)\n",
    "chiley_pred = svc.predict(chileX_test)\n",
    "print(confusion_matrix(chiley_test, chiley_pred))\n",
    "print(classification_report(chiley_test, chiley_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310ff21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0f1a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(svc, chileX_test, chiley_test,\n",
    "        display_labels = ['No', 'Yes'],\n",
    "        cmap = plt.cm.Blues, normalize = 'true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c63214",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feada1f7",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Decision Function with Support Vectors\n",
    "plt.scatter(chileX_train.age, chileX_train.statusquo, c = chiley_train, s = 15, cmap=plt.cm.Paired, alpha = 0.5)\n",
    "\n",
    "ax = plt.gca()\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    svc,\n",
    "    chileX_train,\n",
    "    plot_method = \"contour\",\n",
    "    colors = \"k\",\n",
    "    levels = [-1, 0, 1],\n",
    "    alpha = 0.5,\n",
    "    linestyles = [\"--\", \"-\", \"--\"],\n",
    "    ax = ax)\n",
    "\n",
    "ax.scatter(\n",
    "    svc.support_vectors_[:, 0],\n",
    "    svc.support_vectors_[:, 1],\n",
    "    s = 25,\n",
    "    linewidth = 0.9,\n",
    "    facecolors = \"none\",\n",
    "    edgecolors = \"k\",\n",
    "    alpha = 1\n",
    ")\n",
    "\n",
    "plt.ylim(-1.5, 1.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ddfcb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Now, let us use the radial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff16b7",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Choosing the best C and gammas\n",
    "Cs = np.logspace(0, 2, 25)\n",
    "gammas = np.logspace(-1, 1, 25)\n",
    "\n",
    "# Model (Radial Kernel)\n",
    "svc = SVC(kernel = 'rbf')\n",
    "\n",
    "# Parameters for gridsearch\n",
    "param_grid = {\n",
    "    'C': Cs,\n",
    "    'gamma': gammas\n",
    "}\n",
    "\n",
    "# Build the GridSearch\n",
    "search = GridSearchCV(svc, param_grid, n_jobs = -1)\n",
    "search.fit(chileX_train, chiley_train)\n",
    "\n",
    "# Results\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "\n",
    "## Using the best parameters\n",
    "svc = search.best_estimator_\n",
    "y_pred = svc.predict(chileX_test)\n",
    "print(confusion_matrix(chiley_test, chiley_pred))\n",
    "print(classification_report(chiley_test, chiley_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af8370",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c4bc2",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(svc, chileX_test, chiley_test,\n",
    "        display_labels = ['No', 'Yes'],\n",
    "        cmap = plt.cm.Blues, normalize = 'true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae4838",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433f939",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Decision Function with Support Vectors\n",
    "plt.scatter(chileX_train.age, chileX_train.statusquo, c = chiley_train, s = 15, cmap=plt.cm.Paired, alpha = 0.5)\n",
    "\n",
    "ax = plt.gca()\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    svc,\n",
    "    chileX_train,\n",
    "    plot_method = \"contour\",\n",
    "    colors = \"k\",\n",
    "    levels = [-1, 0, 1],\n",
    "    alpha = 0.5,\n",
    "    linestyles = [\"--\", \"-\", \"--\"],\n",
    "    ax = ax)\n",
    "\n",
    "ax.scatter(\n",
    "    svc.support_vectors_[:, 0],\n",
    "    svc.support_vectors_[:, 1],\n",
    "    s = 25,\n",
    "    linewidth = 0.9,\n",
    "    facecolors = \"none\",\n",
    "    edgecolors = \"k\",\n",
    "    alpha = 1\n",
    ")\n",
    "\n",
    "plt.ylim(-1.5, 1.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818fb620",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7683eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "- The next method we are going to cover is deep learning. \n",
    "\n",
    "- Deep learning is now at the frontier of Machine Learning knowledge.\n",
    "\n",
    "- It is one of the most inventive flexible models there is.\n",
    "\n",
    "- It does a fantastic job of predicting hard-to-see patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319af4e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "- We will see two deep learning methods: *Convoluted Neural Networks*, and *Recurrent Neural Networks*.\n",
    "    + **Convoluted Neural Networks**: Does well with image recognition problems and other problems without recurrent data structures.\n",
    "    + **Recurrent Neural Networks**: Does well with recurrent structures, such as time-series data (where present values depend on past values, such as GDP, stocks, and others).\n",
    "    \n",
    "- The best news computationally is that Google created a library called `TensorFlow`, which does all the estimations for us.\n",
    "\n",
    "- We will learn how to use an interface to this library called `keras`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733207e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "- Let us see a very simple *feed-forward* neural network.\n",
    "\n",
    "![nn1](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/nn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211a649",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "- This neural network is, mathematically:\n",
    "\n",
    "$$ f(X) \\ = \\ \\beta_0 + \\sum_k \\beta_k h_k(X) $$\n",
    "\n",
    "- The features we start our NN call the *input layer*.\n",
    "\n",
    "- The output of our NN is called the *output layer*.\n",
    "\n",
    "- And the insides we call the *hidden layers*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebad74d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning \n",
    "\n",
    "- In the case of a single feed-forward structure, we have one hidden layer:\n",
    "\n",
    "![nn1](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/nn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b7035",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning \n",
    "\n",
    "- Each hidden layer is comprised by activation functions:\n",
    "\n",
    "$$ A_k \\ = \\ h_k(X) \\ = \\ g(w_{k0} + \\sum_j g(w_{kj}X_j) $$\n",
    "\n",
    "- The activation function is based on the weights $w_{kj}$, estimated from the data\n",
    "\n",
    "- And the $g(.)$ is a non-linear function specified by us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681b542",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "- The main thing here is the shape of the $g()$ function.\n",
    "\n",
    "- If $g$ is the identity function, we would recover a simple linear regression model.\n",
    "    + The $g$ should be non-linear!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b02f0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "- If you look closely, the name neural networks would then make sense:\n",
    "    + It seems like a bunch of neurons firing up depending on the activation.\n",
    "    \n",
    "- But there is something else going on here:\n",
    "    + If you look at the activation functions carefully, you will realize they have new features!\n",
    "    + So, the strength of Deep Learning is that it comes up with new features based on older features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a89a15e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "Choices for $g$: \n",
    "\n",
    "- *Sigmoid* activation function: \n",
    "\n",
    "$$ g(z) \\ = \\ \\dfrac{1}{1 + e^{-z}}$$\n",
    "\n",
    "- *Rectified linear unit* function:\n",
    "\n",
    "$$ g(z) \\ = \\ (z)_+ $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f2e04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "Example of these $g$s here:\n",
    "\n",
    "![nn2](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/nn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb41dc2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "Great prediction power at the expense of a clear view of what is going on: *low interpretability*\n",
    "\n",
    "Example: hand-writing classification problem discussed in the book.\n",
    "\n",
    "![nn3](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/nn3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abdebf9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Some Keras\n",
    "# To install it in mykernel\n",
    "# source mykernel/bin/activate\n",
    "# pip install tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554fbecf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Building a simple NN\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer with four neurons (four starting features) and six neurons, 'relu' activation\n",
    "model.add(Dense(6, input_shape=(4,), activation=\"relu\"))\n",
    "\n",
    "# One output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarise your model\n",
    "model.summary()\n",
    "\n",
    "# Do you think you can you draw it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c27e59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/nn1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a5aa2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Now write a code that creates this one here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5cd083",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "- Let us get started by fitting a Neural Networking to predict votes in the Chile Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b9650",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Getting the two variables and feeding them into 10 neurons\n",
    "model.add(Dense(10, input_shape=(2,), activation='relu'))\n",
    "\n",
    "# Adding 10 extra neurons\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Spitting out the prediction\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55cb38d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeee4066",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train your model for 20 epochs\n",
    "model.fit(chileX_train, chiley_train, epochs = 50, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ec995",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate your model accuracy on the test set\n",
    "accuracy = model.evaluate(chileX_test, chiley_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce180b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "**Check-in**: Adapt this code to run with all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd3e15",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b73453",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "- Now, let us predict a multiple outcomes. \n",
    "\n",
    "- Remember that the Chile dataset the answers were:\n",
    "    + Undecided\n",
    "    + Abstain\n",
    "    + Yes\n",
    "    + No\n",
    "    \n",
    "- Let us see if we can improve upon the results from the multinomial support vector machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45327d9",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Loading Chile data\n",
    "chile_clean2 = chile.dropna()\n",
    "chile_clean2['logincome'] = np.log(chile_clean2['income'])\n",
    "chile_clean2['logpop'] = np.log(chile_clean2['population'])\n",
    "dummies = pd.get_dummies(chile_clean2['sex'], prefix = 'sex', drop_first = True)\n",
    "chile_clean2 = pd.concat([chile_clean2, dummies], axis=1)\n",
    "dummies = pd.get_dummies(chile_clean2['region'], prefix = 'region', drop_first = True)\n",
    "chile_clean2 = pd.concat([chile_clean2, dummies], axis=1)\n",
    "dummies = pd.get_dummies(chile_clean2['education'], prefix = 'education', drop_first = True)\n",
    "chile_clean2 = pd.concat([chile_clean2, dummies], axis=1)\n",
    "print(chile_clean2.vote.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f147ffe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Getting the 11 variables and feeding them into 22 neurons\n",
    "model.add(Dense(22, input_shape=(11,), activation='sigmoid'))\n",
    "\n",
    "# Adding 22 extra neurons\n",
    "model.add(Dense(22, activation='relu'))\n",
    "\n",
    "# Adding a second layer with 22 extra neurons\n",
    "model.add(Dense(22, activation='sigmoid'))\n",
    "\n",
    "# Adding a third layer with 22 extra neurons\n",
    "model.add(Dense(22, activation='relu'))\n",
    "\n",
    "# Spitting out the predictions (note: softmax, the best when multinomial problem)\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile your model (note: categorical_crossentropy)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ec32c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## A bit of pre-processing in the voting variable\n",
    "y2 = chile_clean2['vote']\n",
    "y2 = pd.Categorical(y2)\n",
    "cats = y2.categories\n",
    "y2 = y2.codes\n",
    "y2 = to_categorical(y2)\n",
    "X2 = chile_clean2[['age', 'logincome', 'statusquo', 'logpop', 'sex_M', \n",
    "                  'region_M', 'region_N', 'region_S', 'region_SA', 'education_PS', 'education_S']]\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = 0.3, random_state = 12345)\n",
    "chile_clean2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ed281",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Fitting 20 epochs\n",
    "model.fit(X2_train, y2_train, epochs = 20, verbose = 0)\n",
    "preds = model.predict(X2_test)\n",
    "y2_test2 = [np.argmax(pred) for pred in y2_test]\n",
    "y2_pred = np.array([np.argmax(pred) for pred in preds])\n",
    "\n",
    "## Confusion Matrix\n",
    "confusion_matrix(y2_test2, y2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acdd91a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Are we overfitting? We can save the history and check for that!\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Getting the 11 variables and feeding them into 22 neurons\n",
    "model.add(Dense(22, input_shape=(11,), activation='sigmoid'))\n",
    "\n",
    "# Adding 22 extra neurons\n",
    "model.add(Dense(22, activation='relu'))\n",
    "\n",
    "# Adding a second layer with 22 extra neurons\n",
    "model.add(Dense(22, activation='sigmoid'))\n",
    "\n",
    "# Adding a third layer with 22 extra neurons\n",
    "model.add(Dense(22, activation='relu'))\n",
    "\n",
    "# Spitting out the predictions (note: softmax, the best when multinomial problem)\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile your model (note: categorical_crossentropy)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1a751",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Are we overfitting? We can save the history and check for that!\n",
    "h_callback = model.fit(X2_train, y2_train, epochs = 100, validation_data=(X2_test, y2_test), verbose = 0)\n",
    "\n",
    "# prep to confusion matrix\n",
    "preds = model.predict(X2_test)\n",
    "y2_test2 = [np.argmax(pred) for pred in y2_test]\n",
    "y2_pred = np.array([np.argmax(pred) for pred in preds])\n",
    "\n",
    "## Confusion Matrix\n",
    "confusion_matrix(y2_test2, y2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train vs test loss during training\n",
    "plt.figure()\n",
    "plt.plot(h_callback.history['loss'])\n",
    "plt.plot(h_callback.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ddc9bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f7b9a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "- It was one of the reasons that NN rebounded in the 2010s. It is excellent for classifying images.\n",
    "\n",
    "- It benefitted from the ever-growing training datasets that were already human-classified.\n",
    "\n",
    "- It mirrors how the human brain does image classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784fc56-27f5-47ab-84b8-7c44a9f7c2ce",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "![cnn1](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/cnn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb75051",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "- Convolutional layers:\n",
    "    + Every image can be decomposed into many layers\n",
    "    + Here is an example:\n",
    "    \n",
    "![cnn2](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/cnn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b74fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "- And this has an interesting structure when looking into the *linear algebra* of the problem.\n",
    "\n",
    "- Consider that the original image is:\n",
    "\n",
    "$$\n",
    "\\text{Image} = \\begin{bmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "h & g & i\\\\\n",
    "j & k & l\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523cf6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "- Now, consider a convolution filter:\n",
    "$$\n",
    "\\text{Convolution Filter}= \\begin{bmatrix}\n",
    "\\alpha & \\beta \\\\\n",
    "\\gamma & \\delta\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941a801",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "- Applying the filter to the image, we get the *convolved image*:\n",
    "\n",
    "$$\n",
    "\\text{Convolved Image}= \\begin{bmatrix}\n",
    "a\\alpha + b\\beta + d\\gamma + e\\delta & b\\alpha + c\\beta + e\\gamma + f\\delta \\\\\n",
    "d\\alpha + e\\beta + g\\gamma + h\\delta & e\\alpha + f\\beta + h\\gamma + i\\delta \\\\\n",
    "g\\alpha + h\\beta + j\\gamma + k\\delta & h\\alpha + i\\beta + k\\gamma + l\\delta\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d83748",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "- Back to the cool example: \n",
    "    + This was generates using 192 x 179 pixels image, and applying two 15 x 15 convolution filters.\n",
    "    \n",
    "![cnn2](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/cnn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6209be2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "- At the end of the process, we must return to the image.\n",
    "\n",
    "- To do so, we pool the layers back together.\n",
    "\n",
    "- The process increases size (convolution filters), then decreases (pooling), then increases again, and so on.\n",
    "\n",
    "- This is repeated until there are very few pixels in each layer. Then we *flatten* the image back to one pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3321a29f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "![cnn3](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/cnn3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa1f8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "- Applied several times to the same image: You are augmenting your data space (generating features!)\n",
    "    \n",
    "![cnn4](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/cnn4.png)\n",
    "\n",
    "- Next class we will talk about how to fit them in `keras`, and also how to fit `Recurrent NN`s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a95e09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "Let us do some protest predicting.\n",
    "\n",
    "In the `pics` folder, there are 50 pictures. \n",
    "\n",
    "- 01 to 25: Black Lives Matter protests\n",
    "- 26 to 50: Blue Lives Matter protests\n",
    "\n",
    "Do you think we can predict which is which?\n",
    "\n",
    "(Btw, this is Lab 10's content)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a9041c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "First, we need to load the pictures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d040641",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "myfiles = filter(lambda x: '.jpg' in x, os.listdir('./pics'))\n",
    "myfiles = list(myfiles)\n",
    "myfiles.sort()\n",
    "mypics = [imread('./pics/' + x) for x in list(myfiles)]\n",
    "plt.imshow(mypics[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d7daf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "Second, we need to resize all pictures. For the sake of this class, we will also make them all grayscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6ea73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Make all gray\n",
    "mypics = [rgb2gray(x) for x in mypics]\n",
    "\n",
    "# Resize all to 200 x 200 pixels\n",
    "mypics = [resize(x, (200,200), anti_aliasing = True) for x in mypics]\n",
    "\n",
    "plt.imshow(mypics[26], cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daefb52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "Finally, we create it as a 3-dimensional array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2c570",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mypics = np.array(mypics)\n",
    "mypics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991c0b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "Finally, we create it as an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dd805",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mypics = np.array(mypics)\n",
    "mypics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3296ef0f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "Let us fit a simple model without convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f566a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    set_random_seed(12345)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(10, activation = 'relu',\n",
    "                    input_shape = (40000, )))\n",
    "    \n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    \n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a8188",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53263fc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "\n",
    "# Target\n",
    "black_lm = np.append(np.ones(25), np.zeros(25))\n",
    "y = black_lm\n",
    "\n",
    "# Features (pictures)\n",
    "X = mypics\n",
    "X = X.reshape((50, 40000))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, stratify = y, random_state = 54321)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df3aaf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f134e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model = my_model()\n",
    "print(model.summary())\n",
    "\n",
    "# Model\n",
    "trace = model.fit(X_train, \n",
    "                  y_train, \n",
    "                  validation_split = 0.2, \n",
    "                  epochs = 5, \n",
    "                  batch_size = 15, \n",
    "                  verbose = 0)\n",
    "\n",
    "# Plot the fit\n",
    "y_pred = model.predict(X_test, verbose = 0)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), \n",
    "                              display_labels = ['BlueLM', 'BlackLM'])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657270e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "Now, to do a convolution, we need add a Conv2D layer and a Flatten layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d93c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    set_random_seed(12345)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(10, \n",
    "                     kernel_size=3, \n",
    "                     activation='relu',\n",
    "                     input_shape = (200, 200, 1)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d917e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5637b53",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "trace = model.fit(X_train, \n",
    "                  y_train, \n",
    "                  validation_split = 0.2, \n",
    "                  epochs = 5, \n",
    "                  batch_size = 15, \n",
    "                  verbose = 0)\n",
    "\n",
    "# Plot the fit\n",
    "y_pred = model.predict(X_test, verbose = 0)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), \n",
    "                              display_labels = ['BlueLM', 'BlackLM'])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea75f1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "To improve, we can change:\n",
    "\n",
    "1. kernel_size\n",
    "2. padding\n",
    "3. strides\n",
    "4. dilation_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba68ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    set_random_seed(12345)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(10, \n",
    "                     kernel_size=3, \n",
    "                     activation='relu',\n",
    "                     input_shape = (200, 200, 1),\n",
    "                     padding = 'same',\n",
    "                     dilation_rate = 1))\n",
    "    \n",
    "    model.add(Conv2D(10, \n",
    "                     kernel_size=2, \n",
    "                     activation='relu',\n",
    "                     strides = 2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a18b5bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61530e41",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "trace = model.fit(X_train, \n",
    "                  y_train, \n",
    "                  validation_split = 0.2, \n",
    "                  epochs = 5, \n",
    "                  batch_size = 15, \n",
    "                  verbose = 0)\n",
    "\n",
    "# Plot the fit\n",
    "y_pred = model.predict(X_test, verbose = 0)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), \n",
    "                              display_labels = ['BlueLM', 'BlackLM'])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35d651",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "We can also reduce the number of parameters with `MaxPool2D`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd734e1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    set_random_seed(12345)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(10, \n",
    "                     kernel_size=3, \n",
    "                     activation='relu',\n",
    "                     input_shape = (200, 200, 1),\n",
    "                     padding = 'same'))\n",
    "    \n",
    "    model.add(MaxPool2D(2))\n",
    "    \n",
    "    model.add(Conv2D(25, \n",
    "                     kernel_size=3, \n",
    "                     activation='relu',\n",
    "                     strides = 2,\n",
    "                     padding = 'same'))\n",
    "    \n",
    "    model.add(MaxPool2D(2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24351fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144490fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "trace = model.fit(X_train, \n",
    "                  y_train, \n",
    "                  validation_split = 0.2, \n",
    "                  epochs = 5, \n",
    "                  batch_size = 15, \n",
    "                  verbose = 0)\n",
    "\n",
    "# Plot the fit\n",
    "y_pred = model.predict(X_test, verbose = 0)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), \n",
    "                              display_labels = ['BlueLM', 'BlackLM'])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049843ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net\n",
    "\n",
    "To improve the flow of the optimizer, we can use  `BatchNormalization`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ec53d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    set_random_seed(12345)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(10, \n",
    "                     kernel_size=3, \n",
    "                     activation='relu',\n",
    "                     input_shape = (200, 200, 1),\n",
    "                     padding = 'same'))\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(25, \n",
    "                     kernel_size=3, \n",
    "                     activation='relu',\n",
    "                     strides = 2,\n",
    "                     padding = 'same'))\n",
    "    \n",
    "    model.add(MaxPool2D(2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de457240",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "### Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e521a88",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "trace = model.fit(X_train, \n",
    "                  y_train, \n",
    "                  validation_split = 0.2, \n",
    "                  epochs = 5, \n",
    "                  batch_size = 15, \n",
    "                  verbose = 0)\n",
    "\n",
    "# Plot the fit\n",
    "y_pred = model.predict(X_test, verbose = 0)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), \n",
    "                              display_labels = ['BlueLM', 'BlackLM'])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd5c1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0922a795",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "- The main difficulty is that we do not have a *target* variable.\n",
    "\n",
    "- The question is: Can we extract any relevant information, even though we do not have a target variable?\n",
    "\n",
    "- This makes our work much more challenging, but we can still do many things:\n",
    "    + Dimension reduction\n",
    "    + Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa83af3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "- Great to find a low-dimension representation of a large dataset.\n",
    "\n",
    "- But how does it work?\n",
    "\n",
    "- The idea is that we will try to create a feature ($Z_1$) that is a linear combination of other features.\n",
    "\n",
    "- For example, suppose that you have features $X_1, \\ X_2, \\ \\cdots, \\ X_p$. Then, we search for $\\phi_{11}, \\ \\phi_{21}, \\ \\cdots, \\ \\phi_{p1}$ such that:\n",
    "\n",
    "$$ Z_1 \\ = \\ \\phi_{11}X_1 + \\cdots + \\phi_{p1}X_p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e8f6f",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "![pca1](https://upload.wikimedia.org/wikipedia/commons/f/f5/GaussianScatterPCA.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407f9da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "- We want $Z_1$ that: \n",
    "    + Maximize the explanatory power (max variance explained)\n",
    "    \n",
    "    $$ \\max_\\bf{\\phi} \\left[\\sum_i \\left(\\phi_{11}x_{i1} + \\cdots + \\phi_{p1}x_{ip}\\right) \\right] \\text{ subject to } \\sum_j \\phi_{j1}^2 = 1 $$\n",
    "    \n",
    "    + Normalization: \n",
    "    \n",
    "    $$\\sum_j \\phi_{j1}^2 = 1$$\n",
    "\n",
    "- The vector $\\bf{\\phi}_1$ is called the loadings of the principal component.\n",
    "\n",
    "- And we can keep doing it: for the second component, we find the maximal with zero correlation with the first.\n",
    "\n",
    "- And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b3fb2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "- Usefulness:\n",
    "    + Plotting data\n",
    "    + Reduce dimension\n",
    "    + Find patterns in the data (which variables are more correlated with components? Factorial Analysis, the cousin of PCA)\n",
    "    + Regression in high dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd7d32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "- **Plotting data:** We had $p$ variables to plot, but now we have only a few components!\n",
    "\n",
    "- **Reduce dimension:** Before, we had $p$ variables, but now we have a few components! Easier to explain.\n",
    "\n",
    "- **Patterns**: Reducing dimension helps us to find patterns.\n",
    "\n",
    "- **Regression**: Regression is not feasible if we have more variables than data points. PCA helps reduce the number of variables to something manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eac793",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "- $p$ variables can generate $p$ components.\n",
    "\n",
    "- The components are unique, but the directions are not:\n",
    "    + The same component generated twice may differ on one being positive and the other being negative.\n",
    "    \n",
    "- Assumption: the data are correlated! Otherwise, it does not make sense..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf43b9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "![pca2](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/pca2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b56f54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "- Suppose we have the following problem: we want to target campaign to people sensitive to the message optimally.\n",
    "\n",
    "- In some ways, that's what Cambridge Analytica did.\n",
    "\n",
    "- We need to sort people out in terms of their preferences.\n",
    "\n",
    "- We can do that with K-means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcba1181",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "- K-Means: find k-clusters that:\n",
    "    + Minimizes the distance between similar observations (minimize within distance)\n",
    "    + And maximizes the distance between different clusters (maximizes between distance)\n",
    "    \n",
    "- A wonderful and simple idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe9125",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "![kmcl1](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/kmcl1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb61dd7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "![kmcl2](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/kmcl2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2c40e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "- Same idea, but without pre-define the number of clusters we want.\n",
    "\n",
    "- You can pick and choose empirically!\n",
    "\n",
    "- What it does is that: \n",
    "    + It starts with each observation\n",
    "    + Then it starts to group observations closer\n",
    "    + Keep going until one reaches only one cluster (the complete data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50880b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "![hcl1](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/hcl1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6560303",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "![hcl2](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/hcl2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f38ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "![hcl3](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/hcl3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390ef2d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "![hcl4](https://raw.githubusercontent.com/umbertomig/POLI175public/main/img/hcl4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2bdaed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "- Much more! If you want to learn, take a text analysis class\n",
    "\n",
    "- LDA and Topic Models are great and valuable unsupervised learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d8cd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## A bit of psychology for a change\n",
    "bigfive = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/bigfive.csv')\n",
    "bigfive = bigfive.dropna()\n",
    "est = ['EST1', 'EST2', 'EST3', 'EST4', 'EST5', \n",
    "       'EST6', 'EST7', 'EST8', 'EST9', 'EST10']\n",
    "ext = ['EXT1', 'EXT2', 'EXT3', 'EXT4', 'EXT5', \n",
    "       'EXT6', 'EXT7', 'EXT8', 'EXT9', 'EXT10']\n",
    "agr = ['AGR1', 'AGR2', 'AGR3', 'AGR4', 'AGR5', \n",
    "       'AGR6', 'AGR7', 'AGR8', 'AGR9', 'AGR10']\n",
    "con = ['CSN1', 'CSN2', 'CSN3', 'CSN4', 'CSN5', \n",
    "       'CSN6', 'CSN7', 'CSN8', 'CSN9', 'CSN10']\n",
    "ope = ['OPN1', 'OPN2', 'OPN3', 'OPN4', 'OPN5',\n",
    "       'OPN6', 'OPN7', 'OPN8', 'OPN9', 'OPN10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93013471",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## PCA\n",
    "X = bigfive[ext]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd2f97c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = PCA()\n",
    "model.fit(X)\n",
    "extvar = model.transform(X)\n",
    "extcomp1 = extvar[:,0]\n",
    "X['extcomp'] = extcomp1 # First component :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba5aed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Nice!\n",
    "\n",
    "corr_matrix = X.corr()\n",
    "sns.heatmap(corr_matrix, annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b28e96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# How well?\n",
    "features = range(model.n_components_)\n",
    "plt.bar(features, model.explained_variance_)\n",
    "plt.xticks(features)\n",
    "plt.ylabel('variance')\n",
    "plt.xlabel('PCA feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848ffd5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edb64c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# See you in the next class!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
