{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b8e7e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSS 201 / 202 - CSS Bootcamp\n",
    "\n",
    "## Week 06 - Lecture 01\n",
    "\n",
    "### Umberto Mignozzetti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e145d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa1253",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "- My computer is good. \n",
    "\n",
    "- It is new, fast, and reliable. It is capable of storing 1.0 Terabytes.\n",
    "\n",
    "- But humans produce around 2.5 quintillion bytes every day. This is is equal to 1,000,000 Terabytes.\n",
    "\n",
    "- Every day, we produce the equivalent of 2.5 million computers like mine of data!\n",
    "\n",
    "    + Mostly lovely cat pics :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d3163",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "- This is popularly known as Big Data.\n",
    "\n",
    "- However, data per se means nothing! Big data is just a passive description of the world we live in now.\n",
    "\n",
    "- To prove that, try to take a large dataset and learn something from it.\n",
    "\n",
    "- If you want inspiration, take all your pictures and try to create coherent slides of moments of your life."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f26ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "- This is hard and time-consuming. You would spend days doing it.\n",
    "\n",
    "- But note that, interestingly, your phone does that to you every day!\n",
    "\n",
    "- Almost every week, I open up my iPhone, and it shows me a slide show with music and pictures of my family and me.\n",
    "\n",
    "- How does it do that? Machine Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed4070",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "- Machine Learning is a branch of Artificial Intelligence that uses data and algorithms to imitate how humans learn (IBM).\n",
    "\n",
    "- Algorithm: short for *recipe*.\n",
    "\n",
    "- Data: can be anything.\n",
    "\n",
    "- **And note the intent:** *Learn* here means both make sense of things, discover patterns, and predict things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d048e05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "- How can we use this as Social Scientists?\n",
    "\n",
    "- Many applications in Political Science, Economics, Public Policy, Cognitive Sciences, etc.\n",
    "\n",
    "- We will have an *applied focus*, meaning that we will talk about theory, but the focus will be on generating results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98add8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "We will use three books:\n",
    "\n",
    "1. [ISL] James et al. (second edition, 2021) *Introduction to Statistical Learning with Applications in R. Springer.* [https://www.statlearning.com]\n",
    "\n",
    "2. [MG] Müller & Guido (2017) *Introduction to Machine Learning with Python.* O'Reilly.\n",
    "\n",
    "3. [PDA] McKinney (2013) *Python for Data Analysis.* O'Reilly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a9773",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "- Suppose you are hired as a consultant to help design campaign expenditures for a firm.\n",
    "\n",
    "- And they ask you: Where should we spend our resources? The options are: `TV`, `radio`, and `newspaper`.\n",
    "\n",
    "- They want to maximize the sales revenue.\n",
    "\n",
    "- Where would you spend the money?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f58c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "- Let me give you a bit more info: here are the previous advertising expenditures and their effects on sales:\n",
    "\n",
    "![image](https://github.com/umbertomig/POLI175public/blob/main/img/sales.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd26a26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "- Did it help?\n",
    "\n",
    "- Some people would say yes, I'd say *not really*.\n",
    "\n",
    "![image](https://github.com/umbertomig/POLI175public/blob/main/img/sales.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86ac02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Let's formalize the ideas:\n",
    "\n",
    "- $X$: Matrix of predictors ($X_1$: TV expenditures, $X_2$: radio, $X_3$: newspaper)\n",
    "\n",
    "- $Y$: Response variable\n",
    "\n",
    "- $f(.)$: Unknown function that connects the predictors with the response variable.\n",
    "\n",
    "- $\\varepsilon$: Random error term\n",
    "\n",
    "$$ Y \\ = \\ f(X) + \\varepsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d546023",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Another example: Do you think your years of study will reflect into a better salary in the future?\n",
    "\n",
    "- $Y$: Future salary\n",
    "\n",
    "- $X$: Years of study\n",
    "\n",
    "![image](https://github.com/umbertomig/POLI175public/blob/main/img/educ.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f89dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why estimate $f$?\n",
    "\n",
    "Our job when doing ML is to estimate $f$. But why do we do that?\n",
    "\n",
    "1. **Prediction**: We want to predict the values of $Y$: $\\hat{Y} = f(\\hat{X})$\n",
    "    \n",
    "$$ E(Y − \\hat{Y})^2 \\ = \\ E[f(X) + \\varepsilon - \\hat{f}(X)]^2 = \\underbrace{[f(X) - \\hat{f}(X)]^2}_{\\text{Reducible}} + \\underbrace{Var(\\varepsilon)}_{\\text{Non-reducible}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb2025",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why estimate $f$?\n",
    "\n",
    "2. **Inference**: We want, as scientists, to understand how $Y$ is related with a set of $X$s.\n",
    "    \n",
    "    1. *Which predictors are associated with the response?*\n",
    "    \n",
    "    2. *What is the relationship between the response and each predictor?*\n",
    "    \n",
    "    3. *Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3483d8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we estimate $f$?\n",
    "\n",
    "- Let a set of $n$ observations, $(Y_1, X_1)$, ..., $(Y_n, X_n)$.\n",
    "\n",
    "- We will call these observations the **training set**, since we will use these to estimate the function $f$.\n",
    "\n",
    "- Broadly speaking we have two methods to estimate the $f$ function:\n",
    "\n",
    "    1. Parametric\n",
    "\n",
    "    2. Non-parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c5ddc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we estimate $f$?\n",
    "\n",
    "**Parametric**:\n",
    "\n",
    "1. We make an assumption about the functional form, e.g., that the f.f. is linear:\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p $$\n",
    "\n",
    "2. After the f.f. is selected, we fit (train) the model using the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515ebe26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we estimate $f$?\n",
    "\n",
    "**Parametric**:\n",
    "\n",
    "$$ \\text{income} \\approx \\beta_0 + \\beta_1 \\times \\text{education} + \\beta_2 \\times \\text{seniority} $$\n",
    "\n",
    "![image](https://github.com/umbertomig/POLI175public/blob/main/img/linreg.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047dbc49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we estimate $f$?\n",
    "\n",
    "**Parametric**:\n",
    "\n",
    "- This parametric approach has advantages. The main one is that it is straightforward to estimate.\n",
    "\n",
    "- However, it is not very flexible, and it does not capture more complex relationships.\n",
    "\n",
    "- We can estimate more flexible relations, but we may *overfit* our estimates.\n",
    "\n",
    "- We can always conjecture the wrong $f$!\n",
    "\n",
    "- In any case, in the parametric models we need to make assumptions regarding the f.f. of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9ff4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we estimate $f$?\n",
    "\n",
    "**Non-parametric**:\n",
    "\n",
    "- Does not assume the f.f. of $f$.\n",
    "\n",
    "- Seek an estimate of $f$ that gets as close to the data points as possible, without being too rough or wiggly.\n",
    "\n",
    "- Requires lots of observations.\n",
    "\n",
    "- *Overfitting* becomes a more salient problem.\n",
    "\n",
    "**Overfitting:** The estimation do well in the training set, but when you apply it to other observations, it does poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235be97c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we estimate $f$?\n",
    "\n",
    "**Non-parametric**: Thin-plate splines\n",
    "\n",
    "![spline](https://github.com/umbertomig/POLI175public/blob/main/img/spline.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33defc5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimation of $f$\n",
    "\n",
    "**Trade-offs:** Flexibility x Interpretability\n",
    "\n",
    "- *Why would we ever choose to use a more restrictive method instead of a very flexible approach?*\n",
    "\n",
    "- If you are a scientist, you may want to interpret the results more than have a flexible but hard-to-understand approach.\n",
    "\n",
    "- Thus, when **inference** is the goal, we may choose a more restrictive model.\n",
    "\n",
    "- When **prediction** is the goal, we may use a more flexible model. It captures more nuanced relationships.\n",
    "\n",
    "- Think self-driving Teslas: you need to predict when to turn, not explain to me.\n",
    "\n",
    "- But the interpretability problem does not go away: think about why some people complain about self-driving Teslas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70995808",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation of $f$\n",
    "\n",
    "**Trade-offs:** Flexibility x Interpretability\n",
    "\n",
    "![flexint](https://github.com/umbertomig/POLI175public/blob/main/img/flexint.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac1348",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation of $f$\n",
    "\n",
    "**Approaches:** Supervised x Unsupervised Machine Learning\n",
    "\n",
    "- The machine learning techniques roughly divide into *Supervised* and *Unsupervised* methods\n",
    "\n",
    "- **Supervised:** For each observation $i$, we have a target $Y_i$.\n",
    "\n",
    "- **Unsupervised:** We have **no** target $Y_i$. Only $X_i$s, and we want to make sense of it.\n",
    "\n",
    "- **Semi-Supervised:** We know a few $Y_i$, but we want to predict the $Y_i$s for the majority of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bba45b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation of $f$\n",
    "\n",
    "**Unsupervised approach:**\n",
    "\n",
    "![unsup](https://github.com/umbertomig/POLI175public/blob/main/img/unsup.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633fd98",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Accuracy\n",
    "\n",
    "- Too many methods... How to choose?\n",
    "\n",
    "- *There is no free lunch in statistics*: **no one method dominates all others over all possible data sets.**.\n",
    "\n",
    "- We will spend some time choosing methods, and then, choosing the best *tunning* parameters for these methods.\n",
    "\n",
    "- One criterion: \n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i − \\hat{f}(x_i))^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc7732b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Accuracy\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i − \\hat{f}(x_i))^2 $$\n",
    "\n",
    "- We can compute the MSE on the *training* data, but what we really want to know is how the MSE performs in *unseen* data.\n",
    "\n",
    "- That's why for most training purposes, we will split our dataset into two parts: *training* and *testing*.\n",
    "\n",
    "- We want to compute the MSE in this *testing* data: it is our best shot at knowing how it is going to behave in real-world applications!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512ae62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Accuracy\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "![bvt](https://github.com/umbertomig/POLI175public/blob/main/img/bvt.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339cf8ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Accuracy\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "![bvt](https://github.com/umbertomig/POLI175public/blob/main/img/bvt2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1613825",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Accuracy\n",
    "\n",
    "- This trade-off is called **Bias-Variance Trade-off**.\n",
    "\n",
    "- When we adopt a more flexible approach, we **decrease** the bias (distance between $f$ and $\\hat{f}$).\n",
    "\n",
    "    - This means that the training MSE decreases.\n",
    "\n",
    "- However, when we adopt a more flexible approach, we **increase** the variance (think overfitting).\n",
    "\n",
    "$$ E(y_0 - \\hat{f}(x_0))^2 \\ = \\ Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\varepsilon) $$\n",
    "\n",
    "- Our job is to fit a model that has **low bias** and **low variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75fbc0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Accuracy\n",
    "\n",
    "**Bias-Variance Trade-off**\n",
    "\n",
    "![bvt](https://github.com/umbertomig/POLI175public/blob/main/img/bvt3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a185f0fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d74a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression\n",
    "\n",
    "- Regression analysis is one of the most studied approaches for Supervised ML.\n",
    "\n",
    "- It has been around for a long time: we know it well.\n",
    "\n",
    "- it is a great starting point for learning more sophisticated methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39960d02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression\n",
    "\n",
    "- Consider that we run a survey to measure the `prestige` of several professions in the U.S. (We are going to study a survey like this in the next class.)\n",
    "\n",
    "- A few questions about `prestige`:\n",
    "    + Is there a relationship between `prestige` and `income`?\n",
    "    + How strong is the relationship between `prestige` and `income`?\n",
    "    + Which variables are associated with `prestige`?\n",
    "    + How can we accurately predict the prestige of professions not studied in this survey?\n",
    "    + Is the relationship linear?\n",
    "    + Is there a synergy among predictors?\n",
    "    \n",
    "- These are relevant questions, and regression analysis can help us here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456be65d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Estimation\n",
    "\n",
    "- It lives for its name! A very simple approach to regression:\n",
    "\n",
    "- Let:\n",
    "    + $y_i$ the variable we want to predict\n",
    "    + $x_i$ is the variable we will be using to make the prediction.\n",
    "    + And if we assume a linear relationship, we want to find a slope $\\beta_1$ and an intercept $\\beta_0$.\n",
    "    + $n$ the number of observations\n",
    "    + $i$ a given observation.\n",
    "    + Thus:\n",
    "\n",
    "$$ y_i \\approx \\beta_0 + \\beta_1 x_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc4ee7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Estimation\n",
    "\n",
    "How do we estimate $\\beta_0$ and $\\beta_1$? \n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175public/blob/main/img/fig1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0f584",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Estimation\n",
    "\n",
    "- We are searching, among all the possible lines, for the one that does `best`.\n",
    "\n",
    "- What does `best` mean in this context?\n",
    "\n",
    "- One concept: minimize the distance between the `predicted` values and the `actual` value.\n",
    "\n",
    "- Predicted value:\n",
    "\n",
    "$$ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c890012",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Estimation\n",
    "\n",
    "- Actual value:\n",
    "\n",
    "$$ y_i = \\hat{y}_i + e_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i + e_i $$\n",
    "\n",
    "- And `best` here will mean that we minimized the **residuals sum of squares**:\n",
    "\n",
    "$$ RSS \\ = \\ e_1^2 + e_2^2 + \\cdots + e_n^2 $$\n",
    "\n",
    "- It is a well-behaved function on $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d60693",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Estimation\n",
    "\n",
    "- With simple optimization, we can find the $\\hat{\\beta}$s that minimize this.\n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175public/blob/main/img/fig2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa65808",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Assessing the accuracy of the estimates\n",
    "\n",
    "We rarely know the true estimates $\\beta_1$ and $\\beta_0$ (we only do if we `cook the data`).\n",
    "\n",
    "How do we know how good these $\\hat{\\beta}_1$ and $\\hat{\\beta}_0$ are as an approximation of the true $\\beta$s?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab538779",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Assessing the accuracy of the estimates\n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175public/blob/main/img/fig3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed328f76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Assessing the accuracy of the estimates\n",
    "\n",
    "- We find how precise our estimates are by computing the `standard error`.\n",
    "\n",
    "- In some sense, the standard error of the $\\beta_k$ in question is the square root of the variance of it.\n",
    "\n",
    "- The variance, for each of the $\\hat{\\beta}$s in here, is:\n",
    "\n",
    "$$ SE(\\hat{\\beta}_0)^2 = \\sigma^2\\left[\\dfrac{1}{n} + \\dfrac{\\overline{x}^2}{\\sum_i(x_i-\\overline{x})^2}\\right]\\ , \\quad SE(\\hat{\\beta}_1)^2 = \\dfrac{\\sigma^2}{\\sum_i(x_i-\\overline{x})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e2d65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Assessing the accuracy of the estimates\n",
    "\n",
    "- And since $\\sigma^2 = Var(\\varepsilon)$, i.e., the real error term, we need also to estimate it:\n",
    "\n",
    "$$ \\sigma \\ = \\ \\sqrt{\\dfrac{RSS}{n-2}} $$\n",
    "\n",
    "- These standard errors give us an idea of how much we can `trust` our estimates. The smaller, the better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ea153",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Assessing the accuracy of the estimates\n",
    "\n",
    "#### Confidence Intervals\n",
    "\n",
    "- We can also put together a `confidence interval` for our estimates:\n",
    "\n",
    "- A 95\\% confidence interval looks like this:\n",
    "\n",
    "$$ \\hat{\\beta}_k \\pm 1.96 \\times SE(\\hat{\\beta}_k) $$\n",
    "\n",
    "- The number 2 would change depending on the confidence levels you choose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642c69d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Assessing the accuracy of the estimates\n",
    "\n",
    "#### Hypothesis testing\n",
    "\n",
    "- We can also test whether the coefficient could be considered `statistically different` than zero.\n",
    "\n",
    "- We test the hypothesis (called null hypothesis):\n",
    "\n",
    "$$ H_0: \\beta_k = 0 $$\n",
    "\n",
    "- Against the hypothesis (called an alternative hypothesis):\n",
    "\n",
    "$$ H_a: \\beta_k \\neq 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df753641",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Assessing the accuracy of the estimates\n",
    "\n",
    "#### Hypothesis testing\n",
    "\n",
    "And to do that, we put together the `t-statistic`:\n",
    "\n",
    "$$ t = \\dfrac{\\hat{\\beta}_k - 0}{SE(\\hat{\\beta}_k)} \\ \\sim \\ \\text{Student's-t}(n-2) $$\n",
    "\n",
    "- And the p-value is the probability of finding a value larger than $t$ in the Student's-t distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b83c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Assessing the accuracy of the whole model\n",
    "\n",
    "#### RSE\n",
    "\n",
    "The residual standard error is one of the best measures of the fit quality.\n",
    "\n",
    "As we said in the second class, it is the criterium we use for most Supervised Machine Learning models.\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$$ \\text{RSE} \\ = \\ \\sqrt{\\dfrac{RSS}{n-2}} \\ = \\ \\sqrt{\\dfrac{\\sum_i(y_i - \\hat{y}_i)^2}{n-2}} $$\n",
    "\n",
    "The lower, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a594bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Assessing the accuracy of the whole model\n",
    "\n",
    "#### $R^2$\n",
    "\n",
    "The $R^2$ is a measure of goodness-of-fit.\n",
    "\n",
    "It is widely used because it is between zero and one.\n",
    "\n",
    "The proportion of the variability of $Y$ that is explained by modeling it using $X$.\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$$ \\text{R}^2 \\ = \\ \\dfrac{TSS - RSS}{TSS} \\  = \\ 1 - \\dfrac{RSS}{TSS} $$\n",
    "\n",
    "And the total sum of squares is defined as $TSS = \\sum_i(y_i-\\overline{y})^2$. \n",
    "\n",
    "The higher the $R^2$, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e86b76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Assessing the accuracy of the whole model\n",
    "\n",
    "#### F-Statistic\n",
    "\n",
    "- Not now! First, multiple Linear Regression :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6629990c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "- We use multiple linear regression when we have multiple predictors for the same outcome variable.\n",
    "\n",
    "- Let:\n",
    "    + $y_i$ the variable we want to predict\n",
    "    + $x_{ik}$ are the variables we will use to make the prediction.\n",
    "    + $p$: number of predictors.\n",
    "    + And if we assume a linear relationship, we want to find a slope $\\beta_1$ and an intercept $\\beta_0$.\n",
    "    + $n$ the number of observations\n",
    "    + $i$ a given observation\n",
    "    + $k$ and $l$: given predictors\n",
    "    + Thus:\n",
    "    \n",
    "$$ y_i \\ = \\ \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\cdots + \\beta_px_{ip} + \\varepsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcdda5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimation\n",
    "\n",
    "- And the residual sum of squares is defined similarly as before, but we optimize over more parameters:\n",
    "\n",
    "$$ \\text{RSS} \\ = \\ \\sum_ie_i^2 \\ = \\ \\sum_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_{i1} - \\cdots - \\hat{\\beta}_px_{ip})^2 $$\n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175public/blob/main/img/fig4.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c985b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assessing accuracy\n",
    "\n",
    "#### F-Statistic\n",
    "\n",
    "- Now, yes! The F-Statistic tests whether at least one predictor are different from zero.\n",
    "\n",
    "- The null hypothesis is:\n",
    "\n",
    "$$ H_0: \\ \\beta_1 = \\beta_2 = \\cdots = \\beta_p $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a2bbbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assessing accuracy\n",
    "\n",
    "#### F-Statistic\n",
    "\n",
    "- And the alternative hypothesis is:\n",
    "\n",
    "$$ H_a: \\ \\exists k \\in \\{1, \\cdots, p\\}, \\ s.t. \\ \\beta_k \\neq 0 $$\n",
    "\n",
    "- And $F$ is equal to:\n",
    "\n",
    "$$ \\text{F} \\ = \\ \\dfrac{\\frac{TSS-RSS}{p}}{\\frac{RSS}{n-p-1}} \\ \\sim \\ F(p, n-p-1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42338272",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assessing accuracy\n",
    "\n",
    "#### F-Statistic\n",
    "\n",
    "- Why is this a good test? Because under the null hypothesis:\n",
    "\n",
    "$$ \\mathbb{E}\\left[\\dfrac{TSS-RSS}{p}\\right] = \\mathbb{E}\\left[\\dfrac{RSS}{n-p-1}\\right] = \\sigma^2 $$\n",
    "\n",
    "- And so, $F \\approx 1$ under $H_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81fba1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### F-Statistic for model selection\n",
    "\n",
    "- Suppose we have $\\{1, \\cdots, l \\}$ predictors, but we could add $\\{l+1, \\cdots, p \\}$ extra predictors in our model.\n",
    "\n",
    "- Does that make sense? We can test the RSS of the restricted model against the RSS of the full model.\n",
    "\n",
    "- The null hypothesis is:\n",
    "\n",
    "$$ H_0: \\ \\beta_{l+1} = \\cdots = \\beta_{p} = 0 $$\n",
    "\n",
    "- And the F-Stat:\n",
    "\n",
    "$$ \\text{F} \\ = \\ \\dfrac{\\frac{RSS_0-RSS}{p-l}}{\\frac{RSS}{n-p-1}} \\ \\sim \\ F(p-l, n-p-1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f164fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Deciding on important variables\n",
    "\n",
    "- Several criteria can be used. We will discuss later their trade-offs.\n",
    "\n",
    "- But we have a couple of automated ways to select them that are easier to implement:\n",
    "\n",
    "1. **Forward selection**:\n",
    "    + Start with the null model and fit $p$ regressions for each predictor. \n",
    "    + Add to the model the variable that results in the lowest RSS.\n",
    "    + Repeat until some stopping rule is satisfied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfe002",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Deciding on important variables\n",
    "\n",
    "2. **Backward selection**:\n",
    "    + Start with the full model, with all $p$ predictors. \n",
    "    + Remove the variable with the lowest p-value.\n",
    "    + Fit the new model with p-1 variables.\n",
    "    + Repeat until some stopping rule is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa19cf6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Deciding on important variables\n",
    "\n",
    "3. **Mixed selection**:\n",
    "    + Start with the null model and fit $p$ regressions for each predictor.\n",
    "    + Add to the model the variable that results in the lowest RSS.\n",
    "    + Look at the p-value and remove it if it drops under a certain threshold.\n",
    "    + Repeat until some stopping rule is satisfied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fda630",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "Some data: `prestige` dataset.\n",
    "\n",
    "| **Variable** | **Meaning**                                                                                                                                                        |\n",
    "|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `type`         | Type of occupation. A factor with the following levels: <br>`prof`, professional and managerial; `wc`, white-collar; `bc`, blue-collar.           |\n",
    "| `income`      | Percentage of occupational incumbents in the 1950 US Census who earned USD 3,500 <br>or more per year (about USD 36,000 in 2017 US dollars).                             |\n",
    "| `education`    | Percentage of occupational incumbents in 1950 who were high school graduates<br>(which, were we cynical, we would say it is roughly equivalent to a Ph.D. in 2017) |\n",
    "| `prestige`     | Percentage of respondents in a social survey who rated the occupation as “good” <br>or better in prestige                                                          |\n",
    "| `profession`   | Name of the profession                                                                                                                                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71cff81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "- Quick reminder of a few relevant questions:\n",
    "\n",
    "    + Is there a relationship between `prestige` and `income`?\n",
    "    + How strong is the relationship between `prestige` and `income`?\n",
    "    + Which variables are associated with `prestige`?\n",
    "    + How can we accurately predict the prestige of professions not studied in this survey?\n",
    "    + Is the relationship linear?\n",
    "    + Is there a synergy among predictors?\n",
    "    \n",
    "- These are relevant questions, and regression analysis can help us here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8ce881",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ca894",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Loading Libraries and Modules\n",
    "\n",
    "# scikit-learn: barebones, but fast and reliable\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# statsmodels: pretty and good to use, great for interpretable ML\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.formula.api import logit\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting things:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1881b74e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "**Check-in**: Explore the `duncan` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d082d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the data\n",
    "duncan = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/Duncan.csv')\n",
    "duncan = duncan.set_index('profession')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645daba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "- There are two packages in Python to run Regression:\n",
    "    + `statsmodels`\n",
    "    + `scikit.learn`\n",
    "\n",
    "- Today we are going to study the `statsmodels`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087a61ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Regression\n",
    "\n",
    "This regression is in the form of \n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1X_1 + \\varepsilon $$\n",
    "\n",
    "We need to load a few packages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320cfce0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running a Bivariate Regression\n",
    "\n",
    "- Intuitively, earning more `income` is probably a good predictor of `prestige`. We can check that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820fa32",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Scatterplot:\n",
    "sns.scatterplot(x = 'income', y = 'prestige', data = duncan)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c52bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Regplot:\n",
    "sns.regplot(x = 'income', y = 'prestige', data = duncan)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b54ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Now running the actual regression:\n",
    "\n",
    "# Create the model.Fit the model\n",
    "model = ols('prestige ~ income', data = duncan).fit()\n",
    "\n",
    "# Print the parameters\n",
    "print(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5b742",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Meaning:\n",
    "\n",
    "$$ \\text{prestige} \\ \\approx \\ 2.46 + 1.08 \\times \\text{income} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0bf1b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "- Relevant Questions:\n",
    "    + *Is there a relationship between `prestige` and `income`?*\n",
    "        + Yes! But not sure yet if `statistically significant` or not...\n",
    "    + *How strong is the relationship between `prestige` and `income`?*\n",
    "        + When we increase the income by one unit (which is a percentage of people earning more than 39k in the profession), we increase the prestige **on average** by 1.08 units.\n",
    "        \n",
    "- How about statistical significance? Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e3955",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3286e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "- Relevant Questions:\n",
    "    + *Is there a relationship between `prestige` and `income`?*\n",
    "        + Yes! It is statistically significant at a level lower than 0.001!\n",
    "    + *How strong is the relationship between `prestige` and `income`?*\n",
    "        + When we increase the income by one unit (which is a percentage of people earning more than 39k in the profession), we increase the prestige **on average** by 1.08 units.\n",
    "        \n",
    "- How accurate is our overall model?\n",
    "    + Let's check the R$^2$ and the Residual Standard Error (RSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cafb800",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# R-squared\n",
    "rsq = model.rsquared\n",
    "print(rsq) \n",
    "\n",
    "# Around 70% of the variation of prestige is explained by income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9080f597",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Mean Squared Error\n",
    "mse = model.mse_resid\n",
    "print('The mean squared error: ' + str(mse))\n",
    "\n",
    "# Residual Standard Error\n",
    "rse = np.sqrt(mse)\n",
    "print('The Residual Standard Error: ' + str(rse))\n",
    "\n",
    "# The \"typical\" distance between the predicted and the observed values is 17.4 prestige points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39779eb7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "- The fit seem to be good.\n",
    "\n",
    "- But if you think about it, we still have many questions about our model.\n",
    "    + Is it linear?\n",
    "    + Can we do better than the bivariate model?\n",
    "    + Are the standard errors well-defined?\n",
    "    + And others...\n",
    "    \n",
    "- Here we are going to see how to diagnose some of these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f024b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "Several plots can help us diagnose the quality of our model.\n",
    "\n",
    "**Warning**: Find and analyzing these violations is **more of an art**.\n",
    "\n",
    "- In any case, be mindful that a careful analysis is frequent enough to ensure you have a `good` model.\n",
    "\n",
    "- We are going to look at some of them, that make sense for the bivariate case. \n",
    "\n",
    "- Later we are going to look at the ones that make sense for the multivariate case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52365b4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-linearity\n",
    "\n",
    "When the relationship is non-linear, you could have done better using a different (more flexible) functional form.\n",
    "\n",
    "The plot to detect this is residual in the y-axis against the fitted values in the x-axis:\n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175public/blob/main/img/fig5.png?raw=true)\n",
    "\n",
    "- Plot: Fitted Values x Raw Residuals\n",
    "\n",
    "- The best: You should find no patterns.\n",
    "\n",
    "- The ugly: A discernible pattern tells you that you could have done better with a more flexible model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e80dbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-linearity\n",
    "\n",
    "- Hint: Look at the smoothing trend line (the `lowess`). You should see no discernible trend.\n",
    "\n",
    "For the `prestige` x `income` relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5460b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Residual x fitted values (linearity + heteroscedasticity)\n",
    "sns.residplot(x = 'income', y = 'prestige', data = duncan, lowess = True)\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90276c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-linearity\n",
    "\n",
    "- Let's `cook` a non-linear relation:\n",
    "\n",
    "- I will cook the following:\n",
    "\n",
    "$$ Y = 2 + 3 X - 2 X^2 + \\varepsilon $$\n",
    "\n",
    "- The relationship is obviously non-linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc76de4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Cooking\n",
    "cooked_data = pd.DataFrame({\n",
    "    'x': np.random.normal(0, 1, 100)\n",
    "})\n",
    "cooked_data['y'] = 2 + 3 * cooked_data['x'] - 2 * (cooked_data['x'] ** 2) + np.random.normal(0, 1, 100)\n",
    "## Fitting\n",
    "model2 = ols('y ~ x', data = cooked_data).fit()\n",
    "\n",
    "## Checking\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5edcd04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Residual x fitted values (linearity + heteroscedasticity)\n",
    "sns.residplot(x = 'x', y = 'y', data = cooked_data, lowess = True)\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adceec0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Heteroscedasticity\n",
    "\n",
    "- It is fancy wording to say that the variance in error is not constant.\n",
    "\n",
    "- It usually means that you are better at fitting some range of the predictors than others.\n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175public/blob/main/img/fig7.png?raw=true)\n",
    "\n",
    "- Plot: Fitted Values x Raw Residuals\n",
    "\n",
    "- The best: You should find no patterns.\n",
    "\n",
    "- The ugly: A funnel-shaped figure tells you that you may have heteroscedasticity. It invalidates your standard errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eae725",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Heteroscedasticity\n",
    "\n",
    "- Hint: Look at the \"shape\" of the data cloud. You should see no discernible \"shape.\"\n",
    "\n",
    "For the `prestige` x `income` relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7779b1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Residual x fitted values (linearity + heteroscedasticity)\n",
    "sns.residplot(x = 'income', y = 'prestige', data = duncan, lowess = True)\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c23b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outliers\n",
    "\n",
    "- Outliers are values very far away from most values predicted by the model.\n",
    "\n",
    "- Sometimes, it is correct, but frequently it may tell you that you made a mistake in collecting the data!\n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175public/blob/main/img/fig8.png?raw=true)\n",
    "\n",
    "- Plot: Fitted x Studentized residuals\n",
    "\n",
    "- The best: You should find no extreme values in the plot.\n",
    "\n",
    "- The ugly: An extreme value can affect your RSE, $R^2$, and messes up with p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf37c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## More technical info about the model\n",
    "summary_info = model.get_influence().summary_frame()\n",
    "summary_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1f343",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the Studentized Residuals\n",
    "summary_info['fittedvalues'] = model.fittedvalues\n",
    "sns.regplot(x = 'fittedvalues', y = 'student_resid', data = summary_info, lowess = True)\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Studentized residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ff291",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the Studentized Residuals\n",
    "print(summary_info[['student_resid', 'fittedvalues']].sort_values('student_resid', ascending = False).head())\n",
    "\n",
    "# Checking the Studentized Residuals\n",
    "print(summary_info[['student_resid', 'fittedvalues']].sort_values('student_resid', ascending = False).tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a1808f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the Absolute Value of the Studentized Residuals\n",
    "summary_info['abs_student_resid'] = np.abs(summary_info['student_resid'])\n",
    "sns.regplot(x = 'fittedvalues', y = 'abs_student_resid', data = summary_info, lowess = True)\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Absolute Value of the Studentized Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63f7ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the Absolute Value of the Studentized Residuals\n",
    "summary_info.sort_values('abs_student_resid', ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e5cdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### High Leverage\n",
    "\n",
    "- Have very unusual $x_i$ values that could potentially tilt the regression line towards them.\n",
    "\n",
    "- **If high leverage and outlier, bad combination!**\n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175public/blob/main/img/fig9.png?raw=true)\n",
    "\n",
    "- Plot: Leverage x Studentizided residuals\n",
    "\n",
    "- The best: You should find no extreme values in the plot.\n",
    "\n",
    "- The ugly: An extreme value can affect your fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e861597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking the Leverage\n",
    "sns.scatterplot(x = 'hat_diag', y = 'student_resid', data = summary_info)\n",
    "plt.xlabel(\"Leverage (called hat_diag there)\")\n",
    "plt.ylabel(\"Studentized residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_info.sort_values('hat_diag', ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking the Leverage and Outliers: Cook's-d x Leverage\n",
    "sns.scatterplot(x = 'hat_diag', y = 'cooks_d', data = summary_info)\n",
    "plt.xlabel(\"Leverage (called hat_diag there)\")\n",
    "plt.ylabel(\"Cook's-d (how much removing the variable\\nshifts the regression)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16e28a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the Cook's-d measure\n",
    "summary_info.sort_values('cooks_d', ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a852b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "- So far:\n",
    "    + Is there a relationship between `prestige` and `income`? **Yes**\n",
    "    + How strong is the relationship between `prestige` and `income`? **Yes**\n",
    "    + Which variables are associated with `prestige`?\n",
    "    + How can we accurately predict the prestige of professions not studied in this survey? **Yes, so far...**\n",
    "    + Is the relationship linear? **Yes, so far...**\n",
    "    + Is there a synergy among predictors?\n",
    "    \n",
    "- Can we do better? **Yes**, we have other predictors that we didn't not explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80cd85c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Let's fit the following model:\n",
    "\n",
    "$$ \\text{prestige} = \\beta_0 + \\beta_1\\text{income} + \\beta_2\\text{education} + \\varepsilon $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a20f07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Running the actual regression:\n",
    "\n",
    "# Create the model.Fit the model\n",
    "model3 = ols('prestige ~ income + education', data = duncan).fit()\n",
    "\n",
    "# Print the parameters\n",
    "print(model3.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76ea74",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Meaning:\n",
    "\n",
    "$$ \\text{prestige} \\ \\approx \\ -6.06 + 0.60\\text{income} + 0.55\\text{education} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173dd03b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## F-Statistic\n",
    "\n",
    "Are we doing better than the linear regression? We can test that!\n",
    "\n",
    "**Null hypothesis:** The model with fewer parameters is better.\n",
    "\n",
    "**Alternative hypothesis:** At least one variable in the new model does well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc591f90",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Anova for model without x model with education\n",
    "anova_lm(model, model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1cb86e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RSE and R$^2$\n",
    "\n",
    "We can also look at the Residual Standard Error and the R$^2$ to determine this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389e460",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Model with only income\n",
    "mse = model.mse_resid\n",
    "print('The mean squared error: ' + str(mse))\n",
    "\n",
    "# Residual Standard Error\n",
    "rse = np.sqrt(mse)\n",
    "print('The Residual Standard Error: ' + str(rse))\n",
    "\n",
    "# R-squared\n",
    "rsq = model.rsquared\n",
    "print(rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9bee34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Model with income and education\n",
    "mse = model3.mse_resid\n",
    "print('The mean squared error: ' + str(mse))\n",
    "\n",
    "# Residual Standard Error\n",
    "rse = np.sqrt(mse)\n",
    "print('The Residual Standard Error: ' + str(rse))\n",
    "\n",
    "# R-squared\n",
    "rsq = model3.rsquared\n",
    "print(rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba4161",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "Besides the diagnostics that we run before, we can check something called *multicollinearity*\n",
    "\n",
    "### Multicollinearity\n",
    "\n",
    "- Multicollinearity is a situation when your predictors are highly correlated.\n",
    "\n",
    "- In extreme cases, it messes up with the computations in your model.\n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175public/blob/main/img/fig10.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75299a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Pairplot to check\n",
    "sns.pairplot(duncan[['prestige', 'income', 'education']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1830c",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "\n",
    "- One measure of multicollinearity is the *Variance Inflation Factor*.\n",
    "    + How much the multicollinearity is messing up with the estimates.\n",
    "    \n",
    "- To compute, it is fairly easy. As a rule-of-thumb, we would like to see values lower than 5.\n",
    "\n",
    "- It is rarely a problem, though... Especially with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63101a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VIF\n",
    "variables = duncan[['income', 'education']]\n",
    "vif = [variance_inflation_factor(variables, i) for i in range(variables.shape[1])]\n",
    "vif "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e07a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deciding on important variables\n",
    "\n",
    "- Several criteria can be used. We will discuss later their trade-offs.\n",
    "\n",
    "- But we have a couple of automated ways to select them that are easier to implement:\n",
    "\n",
    "1. **Forward selection**:\n",
    "    + Start with the null model and fit $p$ regressions for each predictor. \n",
    "    + Add to the model the variable that results in the lowest RSS.\n",
    "    + Repeat until some stopping rule is satisfied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea27d04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Deciding on important variables\n",
    "\n",
    "2. **Backward selection**:\n",
    "    + Start with the full model, with all $p$ predictors. \n",
    "    + Remove the variable with the lowest p-value.\n",
    "    + Fit the new model with p-1 variables.\n",
    "    + Repeat until some stopping rule is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813f716",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Deciding on important variables\n",
    "\n",
    "3. **Mixed selection**:\n",
    "    + Start with the null model and fit $p$ regressions for each predictor.\n",
    "    + Add to the model the variable that results in the lowest RSS.\n",
    "    + Look at the p-value and remove it if it drops under a certain threshold.\n",
    "    + Repeat until some stopping rule is satisfied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f10b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application\n",
    "\n",
    "- So far:\n",
    "    + Is there a relationship between `prestige` and `income`? **Yes**\n",
    "    + How strong is the relationship between `prestige` and `income`? **Yes**\n",
    "    + Which variables are associated with `prestige`? **income, education, others?**\n",
    "    + How can we accurately predict the prestige of professions not studied in this survey? **Yes**\n",
    "    + Is the relationship linear? **It seems so**\n",
    "    + Is there a synergy among predictors? **Good question!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf66a08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application\n",
    "\n",
    "**Check-in**: Chattopadhyay and Duflo run this study here:\n",
    "\n",
    "[Chattopadhyay and Esther Duflo. 2004. \"*Women as Policy Makers: Evidence from a Randomized Policy Experiment in India.*\" **Econometrica**, 72 (5): 1409–43.](https://economics.mit.edu/files/792)\n",
    "\n",
    "Claiming that women implemented different policies than men.\n",
    "\n",
    "| **Variable** | **Description**                                                                                  |\n",
    "|--------------|--------------------------------------------------------------------------------------------------|\n",
    "| village      | village identifier (\"Gram Panchayat number _ village number\")                                    |\n",
    "| female       | whether village was assigned a female politician: 1=yes, 0=no                                    |\n",
    "| water        | number of new (or repaired) drinking water facilities in the village <br>since random assignment |\n",
    "| irrigation   | number of new (or repaired) irrigation facilities in the village<br>since random assignment      |\n",
    "\n",
    "1. Explore the dataset.\n",
    "1. Fit a regression models for `irrigation` and `water`, using `female` as predictor.\n",
    "    + Hint for diagnostics: Do plotly with village as info.\n",
    "1. Do all the diagnostics. Anything odd in there? If yes, rerun removing the *oddity*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "india = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI30Dpublic/main/datasets/india.csv')\n",
    "\n",
    "# Your answers in here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de95f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf49db3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "- Linear regression is great! But it assumes we want to predict a continuous target variable.\n",
    "\n",
    "- But there are situations when our response variables is qualitative.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- Whether a country default its debt obligations?\n",
    "\n",
    "- Whether a person voted Republican, Democrat, Independent, voted for a different party, or did not turnout to vote?\n",
    "\n",
    "- What determines the number of FOI requests that a given public office receives every day?\n",
    "\n",
    "- Is a country expected to meet, exceed, or not meet the Paris Treaty Nationally Determined Contributions?\n",
    "\n",
    "All these questions are qualitative in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb985516",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "- In 1988, the Chilean Dictator Augusto Pinochet conducted a referendum to whether he should step out.\n",
    "\n",
    "- The FLACSO in Chile conducted a surver on 2700 respondents.\n",
    "\n",
    "- We are going to build a model to predict their voting intentions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7e48c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "| **Variable** | **Meaning** |\n",
    "|:---:|---|\n",
    "| region | A factor with levels:<br>- `C`, Central; <br>- `M`, Metropolitan Santiago area; <br>- `N`, North; <br>- `S`, South; <br>- `SA`, city of Santiago. |\n",
    "| population | The population size of respondent's community. |\n",
    "| sex | A factor with levels: <br>- `F`, female; <br>- `M`, male. |\n",
    "| age | The respondent's age in years. |\n",
    "| education | A factor with levels: <br>- `P`, Primary; <br>- `S`, Secondary; <br>- `PS`, Post-secondary. |\n",
    "| income | The respondent's monthly income, in Pesos. |\n",
    "| statusquo | A scale of support for the status-quo. |\n",
    "| vote | A factor with levels: <br>- `A`, will abstain; <br>- `N`, will vote no (against Pinochet);<br>- `U`, is undecided; <br>- `Y`, will vote yes (for Pinochet). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4704653f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Loading the data\n",
    "chile = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/chilesurvey.csv')\n",
    "chile.head()\n",
    "chile_clean = chile.dropna()\n",
    "chile_clean = chile_clean[chile_clean['vote'].isin(['Y', 'N'])]\n",
    "chile_clean['vote'] = np.where(chile_clean['vote'] == 'Y', 1, 0)\n",
    "chile_clean['logincome'] = np.log(chile_clean['income'])\n",
    "chile_clean['logpop'] = np.log(chile_clean['population'])\n",
    "chile_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f20c1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why not run a Linear Regression?\n",
    "\n",
    "You could ask this very valid question. And my answer here differs a bit from the book.\n",
    "\n",
    "**My suggestion:**\n",
    "\n",
    "- If you want to **measure a treatment effect**, or any other fitting where **explanation trumps prediction**, go with the linear regression.\n",
    "    + Easy to explain to a lay audience.\n",
    "    + Good polynomial expansion around the ATE.\n",
    "    + Needs a careful design (in Causal Inference, the design is more important than the statistical method!).\n",
    "    + Interaction terms are just partial derivatives of the fitted equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c7202c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why not run a Linear Regression?\n",
    "\n",
    "You could ask this very valid question. And my answer here differs a bit from the book.\n",
    "\n",
    "**My suggestion:**\n",
    "\n",
    "- If you want to **predict outcomes**, go with a classification model appropriate for your target variable unit.\n",
    "    + You are not going to do `weird` prediction.\n",
    "    + You have a marginal efficiency gain (in terms of Standard Errors).\n",
    "    + If you have an ordered target variable, your model does look like more meaningful.\n",
    "    + Need to be careful about interaction terms (has to do with taking derivatives of link function in Generalized Linear Models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a176fbc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why not run a Linear Regression?\n",
    "\n",
    "You could ask this very valid question. And my answer here differs a bit from the book.\n",
    "\n",
    "**My suggestion:**\n",
    "\n",
    "- Be **careful when you have discrete nominal variation in your target variable**:\n",
    "    + Binary outcome: Linear Regression and Linear Discriminant Analysis are the same.\n",
    "    + Three or more categories, like the `vote` in the Chilean dataset messes up badly with things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef73891",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Book's Example\n",
    "\n",
    "Chance of Default on Credit Card Debt by Account Balance:\n",
    "\n",
    "![linear x logistic regression IRLR book](https://github.com/umbertomig/POLI175public/blob/main/img/linvslogit.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ebd7dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression belongs to a class of models called [Generalized Linear Models](https://en.wikipedia.org/wiki/Generalized_linear_model) (or GLM for short).\n",
    "\n",
    "- A GLM, in a nutshell (and in a proudly lazy definition) is an expansion of Linear Model that assumes:\n",
    "    + A Linear Relationship in part of the model\n",
    "    + But then applies a non-linear transformation to the response variable.\n",
    "\n",
    "- The non-linear transformation is called `link function`. Many link functions around (check [here](https://en.wikipedia.org/wiki/Generalized_linear_model) for various link functions).\n",
    "\n",
    "- The link function is going to determine which types of models we run.\n",
    "\n",
    "- When the outcome variable is binary, we may use the `Logistic` or `Probit` links."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d97c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In a regression, we are investigating something along the lines of:\n",
    "\n",
    "$$ \\mathbb{E}[Y | X] \\ = \\ \\beta_0 + \\beta_1 X $$\n",
    "\n",
    "But when the outcome is binary we would like to get:\n",
    "\n",
    "$$ \\mathbb{E}[Y | X] \\ = \\ \\mathbb{P}(Y = 1 | X) $$\n",
    "\n",
    "And the Logistic link is nothing but:\n",
    "\n",
    "$$ \\mathbb{P}(Y = 1 | X) \\ = \\ \\dfrac{e^{(\\beta_0 + \\beta_1X)}}{1 + e^{(\\beta_0 + \\beta_1X)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6cf25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "With a bit of manipulation, we get to something called odds ratio:\n",
    "\n",
    "$$ \\dfrac{\\mathbb{P}(Y = 1 | X)}{\\mathbb{P}(Y = 0 | X)} \\ = \\ \\dfrac{\\mathbb{P}(Y = 1 | X)}{1 - \\mathbb{P}(Y = 1 | X)} \\ = \\ e^{(\\beta_0 + \\beta_1X)} $$\n",
    "\n",
    "And logging the thing gets rid of the Euler constant:\n",
    "\n",
    "$$ \\log \\left( \\dfrac{\\mathbb{P}(Y = 1 | X)}{1 - \\mathbb{P}(Y = 1 | X)}\\right) \\ = \\ \\beta_0 + \\beta_1X $$\n",
    "\n",
    "And this is the Logit Link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1324852",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Little detour to talk about odd ratios:\n",
    "\n",
    "- Note the odd ratio: $\\dfrac{\\mathbb{P}(Y = 1 | X)}{1 - \\mathbb{P}(Y = 1 | X)}$\n",
    "\n",
    "- It is a ratio between the chance of $Y = 1$ divided by the chance of $Y = 0$.\n",
    "\n",
    "- Since probabilities are between zero and one, the ratio is always between $(0, \\infty)$.\n",
    "\n",
    "Example:\n",
    "\n",
    "- If based on characteristics, two in every ten people vote for Pinochet, $\\mathbb{P}(Y = 1 | X = \\text{some characs.}) = 0.2$ and the odds ratio is $1/4$.\n",
    "\n",
    "- If based on other set of characteristics, nine out of ten people vote for Pinochet, $\\mathbb{P}(Y = 1 | X = \\text{some other characs.}) = 0.9$ and the odds ratio is $9$.\n",
    "\n",
    "- One is like the number that does not change the ratios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe946ad8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Little other detour to talk about the coefficients:\n",
    "\n",
    "- In linear regression, changes in one unit of $x_i$ changes your target variable in $\\beta_i$ units, on average.\n",
    "\n",
    "- In logistic regression, changes in one unit of $x_i$ changes **the log odds** your target variable in $\\beta_i$ units, on average.\n",
    "\n",
    "- Multiplies the odds by $e^{\\beta_i}$! This is **not** a straight line!\n",
    "\n",
    "- Easy proxy (does not work for interaction terms): \n",
    "    + When $\\beta_1$ is **positive**, it **increases** the $\\mathbb{P}(Y = 1 | X)$\n",
    "    + When $\\beta_1$ is **negative**, it **decreases** the $\\mathbb{P}(Y = 1 | X)$\n",
    "    \n",
    "- Try to compute the partial derivatives on $X$ and you will see the complications!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9494c01e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Technical:\n",
    "\n",
    "1. The estimation is through [maximizing the likelihood function](https://en.wikipedia.org/wiki/Likelihood_function).\n",
    "    + This is outside the scope of the course, but an interesting topic to learn in an advanced course.\n",
    "\n",
    "\n",
    "2. The hypothesis test for the coefficient's significance in here is a Z-test (based on the Normal distribution).\n",
    "    + Null Hypothesis: $H_0: \\ \\beta_i = 0$ or alternatively $H_0: \\ e^{\\beta_i} = 1$.\n",
    "\n",
    "\n",
    "3. Making predictions:\n",
    "    + Just insert the predicted $\\hat{\\beta}$s on the equation.\n",
    "    \n",
    "$$ \\hat{p}(X) \\ = \\ \\dfrac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ecbe7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "First, let's fit a Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0b79e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.regplot(x = 'logincome', y = 'vote', x_jitter = 0.1, y_jitter = 0.1, data = chile_clean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8870963",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab91d3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Model\n",
    "modlin = ols('vote ~ logincome', data = chile_clean).fit()\n",
    "modlin.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf9982d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Now, let us fit a Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c8d2e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Seaborn plot\n",
    "sns.regplot(x = 'logincome', y = 'vote', \n",
    "            x_jitter = 0.1, y_jitter = 0.1, \n",
    "            data = chile_clean, logistic = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937bf81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1fd6d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "modlogit = logit('vote ~ logincome', data = chile_clean).fit()\n",
    "modlogit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9619f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffa64a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "modlogit2 = logit('vote ~ logincome + logpop + region + age + education', data = chile_clean).fit()\n",
    "modlogit2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb9a1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- Let's look at the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482614c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Parameters\n",
    "np.exp(modlogit2.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e8f89d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- Let's look at the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6fdb3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Parameters\n",
    "np.exp(modlogit2.params)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b72f2f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- Now with Scikit Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c912014",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y = chile_clean['vote']\n",
    "\n",
    "# Predictors\n",
    "X = chile_clean[['logincome', 'logpop', 'age']]\n",
    "\n",
    "# Loading the model\n",
    "logreg =  LogisticRegression() \n",
    "\n",
    "# Fitting the model\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# Getting parameters\n",
    "print(logreg.intercept_, logreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cc5b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Where are the categorical variables?\n",
    "\n",
    "In Scikit Learn, you need to create dummy variables for the categorical vars. \n",
    "\n",
    "Thus, you should do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd2f40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Detour: Creating Dummies for Male\n",
    "dummies = pd.get_dummies(chile_clean['sex'], prefix = 'sex', drop_first = True)\n",
    "chile_clean_wdumvars = pd.concat([chile_clean, dummies], axis=1)\n",
    "chile_clean_wdumvars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f9278",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "**Your turn:** Create dummies for `region` and `education`. Which category was dropped in each of the processes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2d6dd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a7d29c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4509f49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Dummies\n",
    "\n",
    "# Sex\n",
    "dummies = pd.get_dummies(chile_clean['sex'], prefix = 'sex', drop_first = True)\n",
    "chile_clean_wdumvars = pd.concat([chile_clean, dummies], axis=1)\n",
    "\n",
    "# Education\n",
    "dummies = pd.get_dummies(chile_clean['region'], prefix = 'region', drop_first = True)\n",
    "chile_clean_wdumvars = pd.concat([chile_clean_wdumvars, dummies], axis=1)\n",
    "\n",
    "# Region\n",
    "dummies = pd.get_dummies(chile_clean['education'], prefix = 'education', drop_first = True)\n",
    "chile_clean_wdumvars = pd.concat([chile_clean_wdumvars, dummies], axis=1)\n",
    "\n",
    "## Head\n",
    "chile_clean_wdumvars.head()\n",
    "\n",
    "# You can even drop the original variables, if you want to: \n",
    "# DataFrame.drop(labels = ['v1, 'v2',..., 'vn'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036c003",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- Now with Scikit Learn, and using all the categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacec81",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y = chile_clean_wdumvars['vote']\n",
    "\n",
    "# Predictors\n",
    "X = chile_clean_wdumvars[['logincome', 'logpop', 'age', \n",
    "                          'sex_M', \n",
    "                          'region_M', 'region_N', 'region_S', 'region_SA', \n",
    "                          'education_PS', 'education_S']]\n",
    "\n",
    "# Loading the model\n",
    "logreg =  LogisticRegression(solver = 'newton-cg') \n",
    "\n",
    "# Fitting the model\n",
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6569741f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d151b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Getting parameters\n",
    "print('Original coefficients: ')\n",
    "print(logreg.intercept_, logreg.coef_)\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "# Exps:\n",
    "print('Exponentiated coefficients: ')\n",
    "print(np.exp(logreg.intercept_), np.exp(logreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab07143",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative Models of Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367fa1bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "Logistic regression involves modeling the probability of a response given a set of parameters\n",
    "    + Uses the logistic link for the *conditional distribution*\n",
    "    \n",
    "$$ \\mathbb{E}(Y = 1 | X = x) \\ = \\ \\mathbb{P}(Y = 1 | X = x) \\ = \\ \\text{Logit}(\\beta_0 + \\cdots + \\beta_pX_p) $$\n",
    "\n",
    "Another approach is to model the distribution for each values of $Y$.\n",
    "\n",
    "And then, use the Bayes' Theorem to get the conditional distributions.\n",
    "\n",
    "But why?\n",
    "\n",
    "1. Separation\n",
    "\n",
    "2. Small sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f158971",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "Let $\\pi_k$ the prior probability of $Y = k$.\n",
    "\n",
    "And let $f_k(x) = \\mathbb{P}(X = x | Y = k)$ the density function for an observation that comes from the $k$-th class.\n",
    "\n",
    "The Bayes theorem says that:\n",
    "\n",
    "$$ \\mathbb{P}(Y = k | X = x) \\ = \\ \\dfrac{\\pi_kf_k(x)}{\\sum_l \\pi_l f_l(x)} $$\n",
    "\n",
    "Now, estimating $\\pi_k$ is easy: we just compute the fraction that belongs to the $k$-th class.\n",
    "\n",
    "How about $f$?\n",
    "\n",
    "+ Different estimators are going to give us different classifiers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04bce0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "- Suppose we have only one variable $x$ and $f_k$ is Gaussian:\n",
    "\n",
    "$$ x \\sim N(\\mu_k, \\sigma_k^2) $$\n",
    "\n",
    "- And assuming further that the draws have the same variance: $\\sigma^2 = \\sigma_k^2 \\forall k$\n",
    "\n",
    "- Computing the log of the posterior gives us:\n",
    "\n",
    "$$ \\delta_k(x) \\ = \\ x \\dfrac{\\mu_k}{\\sigma^2} - \\dfrac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf7e99b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "And the decision for which class the $x$ belongs is simple: **Whichever has the highest probability is the \"winner\"**.\n",
    "\n",
    "1. Let $x$\n",
    "\n",
    "2. Compute $\\delta_0(x)$\n",
    "\n",
    "3. Compute $\\delta_1(x)$\n",
    "\n",
    "4. The highest is the winner :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b29d77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "But how the decision boundary looks like? We need to find the *indifference point*:\n",
    "\n",
    "$$ \\delta_1(x) = \\delta_0(x) $$\n",
    "\n",
    "Do the algebra, and you are going to find:\n",
    "\n",
    "$$ x \\ = \\ \\dfrac{\\mu_0 + \\mu_1}{2} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddfddba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "![img lda](https://github.com/umbertomig/POLI175public/blob/main/img/ldabounds.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc8d06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "And the LDA approximate the quantities of interest by doing the following:\n",
    "\n",
    "1. $$ \\widehat{\\mu}_k  \\ = \\ \\dfrac{1}{n_k} \\sum_{i:y_i = k}x_i $$\n",
    "\n",
    "\n",
    "2. $$ \\widehat{\\sigma}^2 \\ = \\ \\dfrac{1}{n - K} \\sum_{k=1}^K\\sum_{i:y_i = k}(x_i - \\widehat{\\mu}_k)^2 $$\n",
    "\n",
    "\n",
    "3. $$ \\widehat{\\pi}_k \\ = \\ \\dfrac{n_k}{n} $$\n",
    "\n",
    "Note that you can classify more than two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d52ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "The chance that $x$ belongs to $y=k$ is going to be:\n",
    "\n",
    "$$ \\widehat{\\delta}_k(x) \\ = \\ x \\dfrac{\\widehat{\\mu}_k}{\\widehat{\\sigma}^2} - \\dfrac{\\widehat{\\mu}_k^2}{2\\widehat{\\sigma}^2} + \\log(\\widehat{\\pi}_k) $$\n",
    "\n",
    "Note that this is a linear function, so the name `Linear Discriminant Analysis`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10350ec3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "Now let's fit it using `scikit learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebd965",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Start a LDA (do not mix this up with Latent Dirichlet Allocation!)\n",
    "X, y = chile_clean[['logincome', 'age']], chile_clean['vote']\n",
    "\n",
    "# Create the model\n",
    "ldan = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Fitting model\n",
    "ldan.fit(X, y)\n",
    "\n",
    "# Plotting the tree boundaries\n",
    "fig = DecisionBoundaryDisplay.from_estimator(ldan, X, response_method=\"predict\",\n",
    "                                             alpha=0.5, cmap=plt.cm.coolwarm)\n",
    "\n",
    "# Plotting the data points    \n",
    "fig.ax_.scatter(x = chile_clean['logincome'], y = chile_clean['age'], \n",
    "                c = y, alpha = 0.5,\n",
    "                cmap = plt.cm.coolwarm)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03c4a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "The most fundamental question:\n",
    "\n",
    "- How much error in classification we are doing?\n",
    "\n",
    "- To learn that, we need to study the `confusion matrix`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd584c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring Performance\n",
    "\n",
    "**Confusion Matrix**:\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "1. **Accuracy:** $$\\dfrac{\\text{correct predictions}}{\\text{total observations}} \\ = \\ \\dfrac{tp + tn}{tp + tn + fp + fn}$$\n",
    "\n",
    "- High accuracy: lots of correct predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c7f89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring Performance\n",
    "\n",
    "**Confusion Matrix**:\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "2. **Precision:** $$\\dfrac{\\text{true positives}}{\\text{total predicted positive}} \\ = \\ \\dfrac{tp}{tp + fp}$$\n",
    "\n",
    "- High precision: low false-positive rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe6d80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring Performance\n",
    "\n",
    "**Confusion Matrix**:\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "3. **Recall:** $$\\dfrac{\\text{true positives}}{\\text{total actual positive}} \\ = \\ \\dfrac{tp}{tp + fn}$$\n",
    "\n",
    "- High recall: low false-negative rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65376f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring Performance\n",
    "\n",
    "4. **F1-Score**:\n",
    "\n",
    "$$ \\text{F1} \\ = \\ 2 \\times \\dfrac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} $$\n",
    "\n",
    "- Lets look at the two models: logistic x lda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a07d858",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y = chile_clean_wdumvars['vote']\n",
    "\n",
    "# Predictors\n",
    "X = chile_clean_wdumvars[['logincome', 'logpop', 'age', \n",
    "                          'sex_M', \n",
    "                          'region_M', 'region_N', 'region_S', 'region_SA', \n",
    "                          'education_PS', 'education_S']]\n",
    "\n",
    "# Loading the model\n",
    "logreg =  LogisticRegression(solver = 'newton-cg')\n",
    "ldan = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Fitting the models\n",
    "logreg.fit(X, y)\n",
    "ldan.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a303ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring Performance\n",
    "\n",
    "- Lets look at the two models: logistic x lda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d090711",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_logreg = logreg.predict(X)\n",
    "y_pred_ldan = ldan.predict(X)\n",
    "\n",
    "# Logistic Regression\n",
    "print(confusion_matrix(y, y_pred_logreg))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "print(confusion_matrix(y, y_pred_ldan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe1a3a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Classification Report\n",
    "print(classification_report(y, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b072c5",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# LDA Classification Report\n",
    "print(classification_report(y, y_pred_ldan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de46ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 2. Quadratic Discriminant Analysis\n",
    "\n",
    "The main difference is that it assumes that every observation has its own covariance matrix:\n",
    "\n",
    "- Drop the `same-sigma-assumption`.\n",
    "\n",
    "### 3. Naïve Bayes\n",
    "\n",
    "Instead of assuming that $f$ belongs to a class of distributions (e.g., Normal), it assumes that the $f$s are independent:\n",
    "\n",
    "- Drop the `Multivariate-Normal-assumption`.\n",
    "\n",
    "- For $p$ predictors, you make only assumptions about each $x_{ik}$:\n",
    "\n",
    "$$ f_k(x) \\ = \\ f_{k1}(x_1)\\times \\cdots \\times f_{kp}(x_p) $$\n",
    "\n",
    "- And you assume a normal distribution (Gaussian shape) for each variable..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3bb54e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/ldaxqdaxnb.png?raw=true)\n",
    "\n",
    "- Purple: Naïve Bayes; Black: LDA; Green: QDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c0b02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## QDA\n",
    "qdan = QuadraticDiscriminantAnalysis()\n",
    "qdan.fit(X, y)\n",
    "\n",
    "## Gaussian Naive Bayes\n",
    "nbays = GaussianNB()\n",
    "nbays.fit(X, y)\n",
    "\n",
    "\n",
    "# Predictions\n",
    "y_pred_logreg = logreg.predict(X)\n",
    "y_pred_ldan = ldan.predict(X)\n",
    "y_pred_qdab = qdan.predict(X)\n",
    "y_pred_nbays = nbays.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c654277b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "print(classification_report(y, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96403e23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "print(classification_report(y, y_pred_ldan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644d3c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Quadratic Discriminant Analysis\n",
    "print(classification_report(y, y_pred_qdab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b3dd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "print(classification_report(y, y_pred_nbays))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549003d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic x Generative Models for Classification\n",
    "\n",
    "**Check-in**: Does social pressure affects turnout?\n",
    "\n",
    "Gerber, Green, and Larimer. 2008 studied this question on their [\"*Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment.*\" **American Political Science Review**, 102 (1): 33-48.](http://www.donaldgreen.com/wp-content/uploads/2015/09/Gerber_Green_Larimer-APSR-2008.pdf).\n",
    "\n",
    "They selected households in Michigan receive a letter containing the following information:\n",
    "\n",
    "> Dear Registered Voter: \\concept{WHAT IF YOUR NEIGHBORS KNEW WHETHER YOU VOTED?} ... We’re sending this mailing to you and your neighbors to publicize who does and does not vote. The chart shows the names of some of your neighbors, showing which have voted in the past. After the August 8 election, we intend to mail an updated chart. You and your neighbors will all know who voted and who did not. \\concept{DO YOUR CIVIC DUTY--VOTE!}\n",
    "\n",
    "| MAPLE DR                 | Aug 2004    | Nov 2004 | Aug 2006 |\n",
    "|--------------------------|-------------|----------|----------|\n",
    "| 9995 JOSEPH JAMES SMITH  | Voted       | Voted    | ???      |\n",
    "| 995 JENNIFER KAY SMITH   | Didn't vote | Voted    | ???      |\n",
    "| 9997 RICHARD B JACKSON   | Didn't vote | Voted    | ???      |\n",
    "| 9999 KATHY MARIE JACKSON | Didn't vote | Voted    | ???      |\n",
    "\n",
    "The treatment assignment is called `pressure`. If no pressure, then the voter received no letter. We want to study whether `pressure` affected `voted`. \n",
    "\n",
    "Fit all models we learned so far on this dataset (Note: The linear is the most adequate, since the data comes from a randomized experiment, but please fit all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9520bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI30Dpublic/main/datasets/voting.csv')\n",
    "\n",
    "# Your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280d845",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edb64c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# See you in the next class!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
