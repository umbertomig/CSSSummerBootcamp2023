{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b8e7e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSS 201 / 202 - CSS Bootcamp\n",
    "\n",
    "## Week 06 - Lecture 04\n",
    "\n",
    "### Umberto Mignozzetti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b027f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c5cf0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Loading the relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting things:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "# Look at our friend here to help with GAM\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "\n",
    "# Loading scikit learn relevant packages (note our new friends!)\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, get_scorer_names, mean_squared_error, r2_score, mean_squared_error, roc_auc_score, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, SplineTransformer\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import VotingClassifier, BaggingRegressor, BaggingClassifier, RandomForestRegressor, RandomForestClassifier, AdaBoostClassifier, AdaBoostRegressor, GradientBoostingRegressor, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6addc8f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chile Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc34ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Loading Chile data\n",
    "chile = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/chilesurvey.csv')\n",
    "chile_clean = chile.dropna()\n",
    "chile_clean = chile_clean[chile_clean['vote'].isin(['Y', 'N'])]\n",
    "chile_clean['vote'] = np.where(chile_clean['vote'] == 'Y', 1, 0)\n",
    "chile_clean['logincome'] = np.log(chile_clean['income'])\n",
    "chile_clean['logpop'] = np.log(chile_clean['population'])\n",
    "dummies = pd.get_dummies(chile_clean['sex'], prefix = 'sex', drop_first = True)\n",
    "chile_clean = pd.concat([chile_clean, dummies], axis=1)\n",
    "dummies = pd.get_dummies(chile_clean['region'], prefix = 'region', drop_first = True)\n",
    "chile_clean = pd.concat([chile_clean, dummies], axis=1)\n",
    "dummies = pd.get_dummies(chile_clean['education'], prefix = 'education', drop_first = True)\n",
    "chile_clean = pd.concat([chile_clean, dummies], axis=1)\n",
    "chile_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9770c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Education Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696a377",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Education Expenditure Dataset\n",
    "educ = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/educexp.csv')\n",
    "y = educ['education']\n",
    "X = educ[['income', 'young', 'urban']]\n",
    "\n",
    "## Polynomials\n",
    "for power in range(2, 4):\n",
    "    for var in ['income', 'young', 'urban']:\n",
    "        X[var + '_' + str(power)] = X[var] ** power\n",
    "        \n",
    "## Standardizing the X variables\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88da1fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569a8a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso Regression\n",
    "\n",
    "- Ridge regression has one disadvantage: it most of the time includes $p$ predictors.\n",
    "\n",
    "- The shrinkage never sets coefficients to be exactly zero (that is, be removed from the prediction).\n",
    "\n",
    "- This could potentially make subset selection better than ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925dd8fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso Regression\n",
    "\n",
    "- But one alternative, using the same principles as the ridge regression is the **lasso** regression.\n",
    "\n",
    "- In the lasso regression, the objective function becomes:\n",
    "\n",
    "$$ RSS + \\alpha \\sum_{j=1}^p|\\beta_j| $$\n",
    "\n",
    "- Does the same as ridge: the larger the $\\alpha$, the more the *shrinkage*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0be89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso Regression\n",
    "\n",
    "- Unlike ridge, for some values of $\\alpha$, **lasso** actually force coefficients to be exactly equal to zero.\n",
    "\n",
    "- Thus, **lasso** performs variable selection, much like the subset selection models we have seen.\n",
    "\n",
    "- **Side-effect**: Makes models easier to interpret!\n",
    "    + Yields *sparse* models: models that only involve a subset of the variables.\n",
    "    \n",
    "- Like ridge, selecting a good $\\alpha$ is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f8154",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7a99e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Lasso Regression (badly done)\n",
    "lasso = Lasso(alpha = 1).fit(X, y)\n",
    "print('The R-squared for this regression is: ' + str(lasso.score(X, y)))\n",
    "plt.bar(X.columns, lasso.coef_)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2f0abc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84848a07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Lasso Regression (greatly done)\n",
    "lasso = Lasso(alpha = 1).fit(X_scaled, y)\n",
    "print('The R-squared for this regression is: ' + str(lasso.score(X_scaled, y)))\n",
    "plt.bar(X.columns, lasso.coef_)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10333971",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso (the book calls the regularization parameter $\\lambda$) \n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/lasso1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdccda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf5a069",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Example by Kornel Kielczewski in the sklearn Ridge documentation, adapted by me.\n",
    "lasso = Lasso(max_iter = 20000000)\n",
    "coefs = list()\n",
    "errors = list()\n",
    "alphas = np.logspace(-6, 6, 50)\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha = a).fit(X_scaled, y)\n",
    "    coefs.append(lasso.coef_)\n",
    "    errors.append(np.mean((lasso.predict(X_scaled) - y) ** 2))\n",
    "\n",
    "coefs = pd.DataFrame(coefs, columns = X.columns, index = alphas)\n",
    "print(errors[0:5])\n",
    "coefs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd8fb6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d903124",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lasso coefficients as a function of the regularization paramenter alpha\n",
    "g = sns.lineplot(data = coefs)\n",
    "g.set(xscale='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f57567",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0084cdf8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lasso Mean Squared Error as a function of the regularization paramenter alpha\n",
    "g = sns.lineplot(x = alphas, y = errors)\n",
    "g.set(xscale='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48efbc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso x Ridge Regression (the book calls the regularization parameter $\\lambda$) \n",
    "\n",
    "- Selection property of lasso: \n",
    "    + Lasso and ridge are equivalent to a constraint on the shape of the acceptable parameter space.\n",
    "    + But the \"diamond shape\" of lasso makes it shrinks some coefficients towards zero.\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/lassovsridge3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000174f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso x Ridge Regression (the book calls the regularization parameter $\\lambda$) \n",
    "\n",
    "- Lasso performs similarly to ridge in most cases. In these cases, I'd say that lasso is better:\n",
    "    + Reduces the complexity in the model.\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/lassovsridge1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3276c1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso x Ridge Regression (the book calls the regularization parameter $\\lambda$) \n",
    "\n",
    "- But when all coefficients are different from zero, then ridge is better.\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/lassovsridge2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6bd627",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso x Ridge Regression\n",
    "\n",
    "- To summarize, none is better in all situations.\n",
    "\n",
    "- You may need to search which model is better.\n",
    "\n",
    "- Moreover, finding $\\alpha$ is also a big deal. Cross-validation can help us with that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f968db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "- To select the tuning parameters you can use cross-validation.\n",
    "\n",
    "- The idea is to search through a grid of tuning parameter candidates, selecting the one that does best in the cross-validation.\n",
    "\n",
    "- It is indeed a very straight-forward idea, if you think about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de838681",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d71300",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lasso with Cross-Validation\n",
    "lasso = Lasso(max_iter = 20000000)\n",
    "coefs = list()\n",
    "errors = list()\n",
    "CVerrors = list()\n",
    "alphas = np.logspace(-6, 6, 50)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state = 12345)\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha = a).fit(X_train, y_train)\n",
    "    CVerrors.append(np.mean((lasso.predict(X_test) - y_test) ** 2))\n",
    "    errors.append(np.mean((lasso.predict(X_train) - y_train) ** 2))\n",
    "\n",
    "print('The alpha that minimizes the Lasso Cross-Validation MSE is: ' + str(alphas[CVerrors.index(min(CVerrors))]))\n",
    "    \n",
    "mses = pd.DataFrame({\n",
    "    'trainMSE': errors,\n",
    "    'testMSE': CVerrors}, index = alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8200d35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991743e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Cross Validation Lasso MSE as a function of the regularization parameter alpha\n",
    "g = sns.lineplot(data = mses)\n",
    "g.set(xscale='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a1fba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5a223",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Linearity\n",
    "\n",
    "- So far, we have focused on linear models.\n",
    "\n",
    "- Most of the time, linear approximations are excellent:\n",
    "    + Easy to interpret\n",
    "    + Easy to run\n",
    "\n",
    "- And all the GLM flavors afford lots of flexibility regarding how we deal with data.\n",
    "\n",
    "- We will now relax the linearity assumption but keep it as simple as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797cddd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "- Expands the default model ($y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i$) to consider a polynomial $d$:\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 +\\cdots + \\beta_dx_i^d + \\varepsilon_i $$\n",
    "                     \n",
    "- Lots of flexibility here, but we seldom use $d>4$ because then it gets *excessively* flexible.\n",
    "\n",
    "- Prediction very straightforward:\n",
    "\n",
    "$$ \\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1x_0 + \\hat{\\beta}_2x_0^2 +\\cdots + \\hat{\\beta}_dx_0^d $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a35d11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/polyreg1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea01a5a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410a180",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Create the polynomials (with interaction terms!)\n",
    "poly = PolynomialFeatures(degree = 4)\n",
    "X = educ.reset_index()['income'].to_frame()\n",
    "X = poly.fit_transform(X)\n",
    "print(list(poly.get_feature_names_out(['income'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80f0e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538376e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Fit a linear regression\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print('The MSE for a quartic polynomial is: ' + str(np.mean((reg.predict(X) - y) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a73e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce0748",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# For the fully-specified model (lots of interactions...)\n",
    "poly = PolynomialFeatures(degree = 4)\n",
    "X = educ.reset_index()[['income', 'urban', 'young']]\n",
    "X = poly.fit_transform(X)\n",
    "print(list(poly.get_feature_names_out(['income', 'urban', 'young'])))\n",
    "## Fit a linear regression\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print('\\n\\nThe MSE for a quartic polynomial on income, urban, and \\nyoung + interactions, is: ' + str(np.mean((reg.predict(X) - y) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d02bd8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8859ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## With cross-validation now\n",
    "\n",
    "# Only income\n",
    "X = educ.reset_index()['income'].to_frame()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 12345)\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_test = poly.fit_transform(X_test)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "print('The CV-MSE for a quartic polynomial on income is: ' + str(np.mean((reg.predict(X_test) - y_test) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba06b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69fc0a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## With cross-validation now\n",
    "\n",
    "# For the fully-specified model (lots of interactions...)\n",
    "X = educ.reset_index()[['income', 'urban', 'young']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 12345)\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_test = poly.fit_transform(X_test)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "print('The CV-MSE for a quartic polynomial on income, urban, and \\nyoung + interactions, is: ' + str(np.mean((reg.predict(X_test) - y_test) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c29e2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Piecewise-constant Regression\n",
    "\n",
    "- Here, we would be breaks in the predictor, making it an ordered categorical variable instead of a continuous variable.\n",
    "\n",
    "- Let a variable $X$, the indicator function $I(.)$, and a set of cutpoints $\\{c_1, c_2, \\cdots, c_k\\}$. The stepped variable $X$, $C_j(X)$, becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "C_{0}(X) &= I(X < c_1) \\\\\n",
    "C_{1}(X) &= I(c_1 \\leq X < c_2) \\\\\n",
    "C_{2}(X) &= I(c_2 \\leq X < c_3) \\\\\n",
    "&\\vdots\\\\\n",
    "C_{K-1}(X) &= I(c_{K-1} \\leq X < c_K) \\\\\n",
    "C_{K}(X) &= I(X \\geq c_K)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847beec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Piecewise-constant Regression\n",
    "\n",
    "- Example use: when age is defined in 5-years bins.\n",
    "\n",
    "- Detail: $C_0(x_i) + C_1(x_i) + \\cdots + C_K(x_i) = 1$. This would make adding all pieces impossible:\n",
    "    + Perfect Collinearity.\n",
    "\n",
    "- The regression model becomes:\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1C_1(x_i) + \\beta_2C_2(x_i) +\\cdots + \\beta_KC_K(x_i) + \\varepsilon_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd797fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Piecewise-constant Regression\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/pcreg1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b0714",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Piecewise-constant Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5590cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Piecewise-constant Regression\n",
    "X = educ.reset_index()['income'].to_frame()\n",
    "X = pd.cut(X.income, bins = [0, 2500, 3000, 3200, 3500, 4000, 5000])\n",
    "X = pd.get_dummies(X, drop_first = True)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print('The MSE for this piecewise constant regression is: ' + str(np.mean((reg.predict(X) - y) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c877009e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e30f8c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## With cross-validation now\n",
    "X = educ.reset_index()['income'].to_frame()\n",
    "X = pd.cut(X.income, bins = [0, 2500, 3000, 3200, 3500, 4000, 5000])\n",
    "X = pd.get_dummies(X, drop_first = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 12345)\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_test = poly.fit_transform(X_test)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "print('The CV-MSE for this piecewise constant regression is: ' + str(np.mean((reg.predict(X_test) - y_test) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4046a13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tree-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b879690",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tree-based methods\n",
    "\n",
    "- Tree-based methods consist of segmenting the predictors' space into many regions\n",
    "\n",
    "- Then, use these regions to predict the target variable.\n",
    "    + We use heuristics here, such as the variable's mean in the region.\n",
    "    \n",
    "- This approach is called the `decision tree method`\n",
    "\n",
    "- It, by itself, is terrible. But then many methods improve the efficiency considerably\n",
    "    + At the cost of interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1bd55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- Can both be applied to regression and classification.\n",
    "\n",
    "- They look like this:\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/tree1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639cf822",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- This is how they segment the space:\n",
    "\n",
    "![tree2](https://github.com/umbertomig/POLI175public/blob/main/img/tree2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d14c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- Formally:\n",
    "    1. We divide the predictors' space into distinct and non-overlapping regions $R_1$, $R_2$, $\\cdots$, $R_J$.\n",
    "    2. For all observations within $R_j$, we make the same prediction.\n",
    "    \n",
    "- How to construct the $R_1, R_2, \\cdots, R_J$ space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731da624",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- We minimize the RSS of the following equation:\n",
    "\n",
    "$$ \\sum_{j=1}^J\\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2 $$\n",
    "\n",
    "- But it is easy to see that solving this is computationally infeasible.\n",
    "    + Multiple variables would amount to multiple regions.\n",
    "    + Many combinations and regions would be possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591f613",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- The way we do this is called the `greedy` approach:\n",
    "\n",
    "1. We consider one variable at a time.\n",
    "\n",
    "2. We split it recursively.\n",
    "\n",
    "3. We consider the regions that divide the space into two half-spaces:\n",
    "\n",
    "$$ R_1(j, s) = \\{X | X_j < s\\} \\quad and \\quad R_2(j, s) = \\{X | X_j \\geq s\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39388f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- Minimizing:\n",
    "\n",
    "$$ \\sum_{i: \\ x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i: \\ x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2$$\n",
    "\n",
    "- With $\\hat{y}_{R_j}$ the mean of $y$ in the $j$-th half-space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf8fcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- This could be the optimal, but never the greedy approach:\n",
    "\n",
    "![tree3](https://github.com/umbertomig/POLI175public/blob/main/img/tree3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed46502",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- This represents the greedy approach:\n",
    "\n",
    "![tree3](https://github.com/umbertomig/POLI175public/blob/main/img/tree4.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60396a32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- And this is the levels / the regression tree:\n",
    "\n",
    "![tree5](https://github.com/umbertomig/POLI175public/blob/main/img/tree5.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033e7aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- This approach will likely overfit the data. We need them to `prune` our tree a bit.\n",
    "\n",
    "- This involves removing parts of our tree that are not being helpful in the testing set predictions.\n",
    "\n",
    "- One heuristic is to use the size of the tree to penalize it. This is called `cost complexity pruning`\n",
    "\n",
    "For all $T \\subset T_0$:\n",
    "\n",
    "$$ \\sum_{m=1}^{|T|} \\sum_{i: \\ x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha |T| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c58bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "**Regression Tree Algorithm:**\n",
    "\n",
    "1. Use recursive binary split (greedy approach) to grow a large tree on the training data. Stop when the terminal node reaches fewer than some threshold minimum number of observations.\n",
    "\n",
    "2. Apply the cost-complexity pruning to the large tree to obtain a sequence of trees $T_1 \\subset T_2 \\subset \\cdots \\subset T_0$, as a function of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716eec95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "**Regression Tree Algorithm:**\n",
    "\n",
    "3. Use K-fold cross-validation to choose $\\alpha$, i.e., for each $k \\in \\{1, 2, \\cdots, K\\}$:\n",
    "    - Repeat steps 1 and 2 on all but $k$th fold of training data\n",
    "    - Compute the MSE in the left-out fold\n",
    "    - Pick $\\alpha$ that minimizes the MSE.\n",
    "\n",
    "4. Fit the corresponding tree for the optimal $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24850020",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "![tree6](https://github.com/umbertomig/POLI175public/blob/main/img/tree6.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a2ec1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "- Very similar, but you change the prediction and the statistic we look at.\n",
    "\n",
    "1. We predict that all classes belong to the `most commonly occurring class` in the data.\n",
    "\n",
    "2. We look at the `classification error rates` to grow our trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76b1d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "- Let $\\hat{p}_{mk}$ the proportion of cases in the $m$-th region that belong to the $k$-th class.\n",
    "\n",
    "- The error rate is:\n",
    "\n",
    "$$ E = 1 - \\max_k (\\hat{p}_{mk}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2f0c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "- But, the classification error is sensitive to the size of the tree. Therefore, it is preferable to also look at the following:\n",
    "\n",
    "1. The `Gini index`:\n",
    "\n",
    "$$ G = \\sum_k \\hat{p}_{mk}(1 - \\hat{p}_{mk}) $$\n",
    "\n",
    "2. Or the `Entropy` level:\n",
    "    \n",
    "$$ D = - \\sum_k \\max_k \\hat{p}_{mk}\\log(\\hat{p}_{mk}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5bf74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "- [Gini index](https://en.wikipedia.org/wiki/Gini_coefficient): measure of *node purity*: \n",
    "\n",
    "    - Small values indicate that all $\\hat{p}_{mk}$ are either close to zero or one.\n",
    "\n",
    "- [Entropy](https://en.wikipedia.org/wiki/Entropy):\n",
    "\n",
    "    - Easy to see that: $0 \\leq -\\hat{p}_{mk}\\log(\\hat{p}_{mk})$\n",
    "    - It will take values close to zero if $\\hat{p}_{mk}$ is close to either zero or one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242428cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "- Feel free to use the measurement that you prefer.\n",
    "\n",
    "- But, minimizing error rates could be preferable when classification accuracy is the target.\n",
    "\n",
    "- But please cross-validate your analysis to pick the best parameters/tree size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a4ebe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "Classification in the book's heart disease data (before pruning):\n",
    "\n",
    "![tree7](https://github.com/umbertomig/POLI175public/blob/main/img/tree7.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56912cd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "Classification in the book's heart disease data (after pruning):\n",
    "\n",
    "![tree8](https://github.com/umbertomig/POLI175public/blob/main/img/tree8.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655eaa21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees x Linear Models\n",
    "\n",
    "- Linear regression:\n",
    "\n",
    "$$ f(X) \\ = \\ \\beta_0 + \\sum_{j=1}^p X_j\\beta_j $$\n",
    "\n",
    "- Decision Trees:\n",
    "\n",
    "$$ f(X) \\ = \\ \\sum_{m=1}^M c_m \\cdot \\mathbb{1}(X \\in R_m) $$\n",
    "\n",
    "- Which model is better? No easy answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d203cdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees x Linear Models\n",
    "\n",
    "Regression is better:\n",
    "\n",
    "![tree9](https://github.com/umbertomig/POLI175public/blob/main/img/tree9.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883eb16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees x Linear Models\n",
    "\n",
    "Decision tree is better:\n",
    "\n",
    "![tree10](https://github.com/umbertomig/POLI175public/blob/main/img/tree10.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d850f88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- **Pros**:\n",
    "\n",
    "    1. Easy to explain.\n",
    "\n",
    "    2. Some argue that it mirrors human decision-making.\n",
    "\n",
    "    3. Allow for graphical display (kind of pretty, if you ask me...).\n",
    "\n",
    "    4. Easily handle qualitative predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a1f25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- **Cons**:\n",
    "\n",
    "    1. Do not have the same level of accuracy as some predictive regression models.\n",
    "    \n",
    "    2. Can be *non-robust*: small changes in data make up to significant changes in final estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a97f957",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- Decision trees for classification and regression tend to do poorly.\n",
    "\n",
    "- This is because they are sensitive to the variation in the data.\n",
    "\n",
    "- And this is something that you cannot change by rescaling the data:\n",
    "    - Trees are **not** sensitive to the data scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db27ca3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- But here is the thing: We can combine many poor models and create a great one!\n",
    "\n",
    "- We can combine many great models and create an even better one, but sometimes it is hard to improve on something already great.\n",
    "\n",
    "- This technique is called **ensemble**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2e9e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Civil War Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6629a71c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "dat = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/mshk-pa-2017/SambanisImp.csv')\n",
    "\n",
    "## Target\n",
    "target = \"warstds\"\n",
    "\n",
    "## Predictors\n",
    "predictors = [\"ager\", \"agexp\", \"anoc\", \"army85\", \"autch98\", \"auto4\",\"autonomy\", \"avgnabo\", \n",
    "              \"centpol3\", \"coldwar\", \"decade1\", \"decade2\",\"decade3\", \"decade4\", \"dem\", \n",
    "              \"dem4\", \"demch98\", \"dlang\", \"drel\", \"durable\", \"ef\", \"ef2\", \"ehet\", \"elfo\", \n",
    "              \"elfo2\", \"etdo4590\", \"expgdp\", \"exrec\", \"fedpol3\", \"fuelexp\", \"gdpgrowth\", \n",
    "              \"geo1\", \"geo2\", \"geo34\", \"geo57\", \"geo69\", \"geo8\", \"illiteracy\", \"incumb\", \n",
    "              \"infant\", \"inst\", \"inst3\", \"life\", \"lmtnest\", \"ln_gdpen\", \"lpopns\", \"major\", \n",
    "              \"manuexp\", \"milper\", \"mirps0\", \"mirps1\", \"mirps2\", \"mirps3\", \"nat_war\", \n",
    "              \"ncontig\", \"nmgdp\", \"nmdp4_alt\", \"numlang\", \"nwstate\", \"oil\", \"p4mchg\", \n",
    "              \"parcomp\", \"parreg\", \"part\", \"partfree\", \"plural\", \"plurrel\", \"pol4\", \"pol4m\", \n",
    "              \"pol4sq\", \"polch98\", \"polcomp\", \"popdense\", \"presi\", \"pri\", \"proxregc\", \n",
    "              \"ptime\", \"reg\", \"regd4_alt\", \"relfrac\", \"seceduc\", \"second\", \"semipol3\", \"sip2\", \n",
    "              \"sxpnew\", \"sxpsq\", \"tnatwar\", \"trade\", \"warhist\", \"xconst\"]\n",
    "\n",
    "dat = dat[[target] + predictors]\n",
    "y = dat[target]\n",
    "X = dat[predictors]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87be73a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf08c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Decision Tree\n",
    "dt = DecisionTreeClassifier(max_depth = 5)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# See Decision Tree (does not look great...)\n",
    "fig = plt.figure(figsize = (25,20))\n",
    "plot_tree(dt, fontsize = 15, feature_names = list(X.columns), \n",
    "          max_depth = 5, class_names = ['Peace', 'War'],\n",
    "         label = 'root', filled = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e632d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b514b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Seeing it in text (not great either)\n",
    "print(export_text(dt, feature_names = list(X.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9597922",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45327d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute test set accuracy\n",
    "print(confusion_matrix(y_pred, y_test))\n",
    "print(classification_report(y_pred, y_test))\n",
    "\n",
    "# Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test,\n",
    "        cmap = plt.cm.Blues, normalize = 'true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8951cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a63cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression(max_iter = 100000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Compute test set accuracy  \n",
    "accuracy_score(y_pred, y_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_estimator(logreg, X_test, y_test,\n",
    "        cmap = plt.cm.Blues, normalize = 'true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc77c21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble\n",
    "\n",
    "- Check an example of an ensemble using three classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc25a2b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Classifiers\n",
    "logreg = LogisticRegression(max_iter = 10000)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "dt = DecisionTreeClassifier(max_depth = 10)\n",
    "classifiers = [('Logistic Regression', logreg), ('LDA', lda), ('Classification Tree', dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d83c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Each classifier\n",
    "for clf_name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('{} accuracy = {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85990e2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Their Ensemble\n",
    "vc = VotingClassifier(estimators = classifiers)     \n",
    "vc.fit(X_train, y_train)   \n",
    "y_pred = vc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc2d7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_estimator(vc, X_test, y_test,\n",
    "        cmap = plt.cm.Blues, normalize = 'true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604510e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detour: Bootstrap\n",
    "\n",
    "- To understand some ensemble techniques, we must first learn what a bootstrap is.\n",
    "\n",
    "- [**Bootstrap**](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)): Technique to fit models empirically, without deriving theoretically the parameters of interest.\n",
    "    + We use it a lot to find standard errors and run things like [*exact tests*](https://en.wikipedia.org/wiki/Exact_test) and [randomization inference](https://dimewiki.worldbank.org/Randomization_Inference).\n",
    "\n",
    "- Very empirical!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ca006",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detour: Bootstrap\n",
    "\n",
    "**Algorithm:** Start with the number of repetitions, N. For each $1, 2, \\cdots, N$ step:\n",
    "\n",
    "1. Draw a sample of the dataset [**with replacement**](https://en.wikipedia.org/wiki/Resampling_(statistics)) that has the same size of the dataset.\n",
    "\n",
    "2. Fit the model (e.g., a regression) in the randomly drawn dataset.\n",
    "\n",
    "3. Save the coefficient of interest.\n",
    "\n",
    "In the end, take the mean of the coefficient as the `bootstrapped` coefficient and the standard deviation as the standard error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d845002",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "- Bagging stands for Bootstrap Aggregation.\n",
    "\n",
    "- The idea is to fit each tree on a bootstrapped dataset, then take the average of all trees.\n",
    "\n",
    "- Each tree performs poorly. However, the average performance of all of them is better!\n",
    "\n",
    "$$ \\hat{f}_{bag}(x) \\ = \\ \\dfrac{1}{B}\\sum_{b = 1}^B \\hat{f}^b(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958433c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "- We let trees grow wildly: no pruning!\n",
    "    + Averaging them out is what reduces the variance!\n",
    "    \n",
    "- For continuous variables, we take averages as the predicted value.\n",
    "\n",
    "- How about classification problems?\n",
    "    + Majority vote for all trees!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797c0fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "![bag1](https://github.com/umbertomig/POLI175public/blob/main/img/bag1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799a53e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "- This straightforward technique decreases the variance of a tree significantly.\n",
    "\n",
    "- But how to interpret what the average of the trees means?\n",
    "    - Well, we lose in terms of interpretation...\n",
    "\n",
    "- One positive thing is that we can still find the **importance of each variable for the *bagging***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603802af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "![bag2](https://github.com/umbertomig/POLI175public/blob/main/img/bag2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f87956",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "- Cross-validation here can be improved by using something called **out-of-bag** errors:\n",
    "    + The bootstrap process usually leaves out 1/3 of the sample.\n",
    "    + We can take advantage of this left-out (or *out-of-bag* sample), to estimate our models.\n",
    "\n",
    "- And we fit RMSE for continuous or accuracy for discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e9802",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07fce6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Bagging (discrete target)\n",
    "dt = DecisionTreeClassifier()\n",
    "bc = BaggingClassifier(estimator = dt, n_estimators = 50)\n",
    "bc.fit(X_train, y_train)\n",
    "y_pred = bc.predict(X_test)\n",
    "print('Test set accuracy of bc: {:.2f}'.format(accuracy_score(y_test, y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75079b63",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ca9d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Bagging with OOB\n",
    "bc = BaggingClassifier(estimator = dt, \n",
    "            n_estimators = 50,\n",
    "            oob_score = True)\n",
    "bc.fit(X_train, y_train)\n",
    "y_pred = bc.predict(X_test)\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "acc_oob = bc.oob_score_\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29bd061",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Women in Parliament Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2641c92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Countries Dataset\n",
    "countr = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/countrdat.csv')\n",
    "y2 = countr['wdi_wip']\n",
    "X2 = countr[['wdi_expedu', 'pwt_pop', 'mad_gdppc']]\n",
    "X2 = X2.join(pd.get_dummies(countr.ccodealp, drop_first = True, prefix = 'country'))\n",
    "X2 = X2.join(pd.get_dummies(countr.year, drop_first = True, prefix = 'year'))\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = 0.3, random_state = 12345)\n",
    "X2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280d845",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Women in Parliament Dataset\n",
    "\n",
    "**Check-in:** Explore the WIP dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d1c03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6ddbd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Bagging (continuous target)\n",
    "dt = DecisionTreeRegressor()\n",
    "bc = BaggingRegressor(estimator = dt, n_estimators = 50)\n",
    "bc.fit(X2_train, y2_train)\n",
    "y2_pred = bc.predict(X2_test)\n",
    "\n",
    "# Lin reg (comparison)\n",
    "lin = LinearRegression()\n",
    "lin.fit(X2_train, y2_train)\n",
    "y2_pred_lin = lin.predict(X2_test)\n",
    "linreg_rmse = mean_squared_error(y2_test, y2_pred_lin) ** 0.5\n",
    "bag_rmse = mean_squared_error(y2_test, y2_pred) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce8375",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Comparing\n",
    "print('Test set RMSE of Linear Regression is: {:.3f}'.format(linreg_rmse))\n",
    "print('Test set RMSE of Bagging is: {:.3f}'.format(bag_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab35e38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "- It is **not** a place where Computational Social Scientists go camping (this joke is getting old...)\n",
    "\n",
    "- Random forests intend to improve the effectiveness of our bagging estimates.\n",
    "\n",
    "- Each bagging tree in the ensemble can be highly correlated with each other.\n",
    "    - This messes up the prediction because it reduces the contribution of each tree.\n",
    "\n",
    "- To fix that, we tweak the bagging to *decorrelate* the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a9887",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "- A simple way to do that is only to consider a subset of the predictors at each tree.\n",
    "    - Why would we even want to do that?\n",
    "    \n",
    "- Let a strong predictor with a bunch of other weak ones. Then:\n",
    "    1. All bagging trees will rely on the stronger predictor more than the others.\n",
    "    2. Subsetting the number of variables, considering subsets where the strong predictor is not there, improves the usage of the weak predictors.\n",
    "        - This *decorrelates* the trees!\n",
    "\n",
    "- Rule of thumb: Use $m = \\sqrt{p}$ predictors at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2296b4fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "- Choose a small(er) $m$ if the predictors are all highly correlated.\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/rf1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971af4a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5bfd56",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Random Forests (discrete target)\n",
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 12345)  \n",
    "rf.fit(X_train, y_train) \n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Print accuracy\n",
    "print('Test set Accuracy of Random Forest: {:.2f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f837ea9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9994a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test,\n",
    "        cmap = plt.cm.Blues, normalize = 'true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb3c89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd3f419",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data = rf.feature_importances_,\n",
    "                        index = X_train.columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "fig = plt.figure(figsize = (25,20))\n",
    "importances_sorted.plot(kind='barh', color='lightgreen')\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8596d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3356bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests (continuous variable)\n",
    "rf = RandomForestRegressor(n_estimators = 25, random_state = 12345)  \n",
    "rf.fit(X2_train, y2_train)\n",
    "y2_pred = rf.predict(X2_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rf_rmse = mean_squared_error(y2_test, y2_pred) ** 0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of Linear Regression is: {:.5f}'.format(linreg_rmse))\n",
    "print('Test set RMSE of Bagging is: {:.3f}'.format(bag_rmse))\n",
    "print('Test set RMSE of Random Forest is: {:.5f}'.format(rf_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7e5521",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efbeafb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data = rf.feature_importances_,\n",
    "                        index = X2_train.columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "fig = plt.figure(figsize = (25,20))\n",
    "importances_sorted.tail(10).plot(kind='barh', color='lightgreen')\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46567473",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "- Ensemble method that combines weak learners to form a stronger one.\n",
    "    + Example: Regression tree that is only allowed to have one leaf!\n",
    "\n",
    "- It builds on accumulation: Every predictor tries to improve the predecessor's job.\n",
    "    - Work with the errors of the previous models, and update the fit slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eed8cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "**Algorithm:** Start with a null model ($\\hat{f}(x) = 0$), the residual equals to $r_i = y_i$, and a number $B$ of steps.\n",
    "\n",
    "For each $b \\in \\{1, 2, \\cdots, B\\}$:\n",
    "\n",
    "1. Fit a tree $\\hat{f}^b(x)$ with $d$ splits (or d+1 terminal nodes).\n",
    "\n",
    "2. Set:\n",
    "\n",
    "$$ \\hat{f}_{new}(x) = \\hat{f}_{old}(x) + \\lambda \\hat{f}^b(x) $$\n",
    "\n",
    "3. Set \n",
    "\n",
    "$$ r_{i_{new}} = r_{i_{old}} - \\lambda \\hat{f}^b(x) $$\n",
    "\n",
    "At the end, you should define $\\hat{f}(x)$ as:\n",
    "\n",
    "$$ \\hat{f}(x) = \\sum_{b=1}^B \\lambda \\hat{f}^b(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda49e2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "- You can overfit using boosting. But only if $B$ is too large.\n",
    "\n",
    "- $\\lambda$: Controls the rate that your boosting algorithm is learning.\n",
    "    + Small $\\lambda$s require large $B$s\n",
    "\n",
    "- $d$: Controls the complexity of each step. $d=1$ tends to work well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2be36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "![imgb](https://upload.wikimedia.org/wikipedia/commons/b/b5/Ensemble_Boosting.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9540f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ac2c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# AdaBoosting (discrete target)\n",
    "dt = DecisionTreeClassifier(max_depth = 2)\n",
    "ada = AdaBoostClassifier(estimator = dt, n_estimators = 250, random_state = 12345)\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred = ada.predict(X_test)\n",
    "print('Test set Accuracy of AdaBoosting: {:3f}'.format(accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc66d1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f2628",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "ConfusionMatrixDisplay.from_estimator(ada, X_test, y_test,\n",
    "        cmap = plt.cm.Blues, normalize = 'true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be59c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d0de8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# AdaBoosting (continuous variable)\n",
    "regtree = DecisionTreeRegressor(max_depth = 2)\n",
    "ada = AdaBoostRegressor(regtree, n_estimators = 25, random_state = 12345)  \n",
    "ada.fit(X2_train, y2_train)\n",
    "y2_pred = ada.predict(X2_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "ada_rmse = mean_squared_error(y2_test, y2_pred) ** 0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of Linear Regression is: {:.5f}'.format(linreg_rmse))\n",
    "print('Test set RMSE of Bagging is: {:.3f}'.format(bag_rmse))\n",
    "print('Test set RMSE of Random Forest is: {:.5f}'.format(rf_rmse))\n",
    "print('Test set RMSE of AdaBoosting is: {:.5f}'.format(ada_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c695256",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cccdaf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# GradientBoost (discrete target)\n",
    "gbr = GradientBoostingClassifier(max_depth = 5, n_estimators = 250, random_state = 12345)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "print('Test set Accuracy of Gradient Boosting: {:3f}'.format(accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc9256",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2fcab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting (continuous variable)\n",
    "gbr = GradientBoostingRegressor(n_estimators = 25, random_state = 12345)  \n",
    "gbr.fit(X2_train, y2_train)\n",
    "y2_pred = gbr.predict(X2_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "gbr_rmse = mean_squared_error(y2_test, y2_pred) ** 0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of Linear Regression is: {:.5f}'.format(linreg_rmse))\n",
    "print('Test set RMSE of Bagging is: {:.3f}'.format(bag_rmse))\n",
    "print('Test set RMSE of Random Forest is: {:.5f}'.format(rf_rmse))\n",
    "print('Test set RMSE of AdaBoosting is: {:.5f}'.format(ada_rmse))\n",
    "print('Test set RMSE of Gradient Boosting is: {:.5f}'.format(gbr_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac03b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "**Check-in:** Use ensemble methods to try to predict swing voters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc19235",
   "metadata": {},
   "outputs": [],
   "source": [
    "anes = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/anes2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673c5b1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edb64c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# See you in the next class!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
