{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b8e7e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSS 201 / 202 - CSS Bootcamp\n",
    "\n",
    "## Week 06 - Lecture 02\n",
    "\n",
    "### Umberto Mignozzetti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e145d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b2057c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression\n",
    "\n",
    "Recap Regression:\n",
    "\n",
    "Theory:\n",
    "- Minimizes mean squared error (or the residual sum of squares)\n",
    "- Can have as many variables as we want (not really...)\n",
    "- Good to answer about relationship (existence and strength)\n",
    "- Synergy\n",
    "- Not very flexible\n",
    "- You need to check the consistency of your model (diagnostics: `model.get_influence()`)\n",
    "\n",
    "Estimation:\n",
    "- `statsmodels` do a good job. Tutorial [here](https://www.statsmodels.org/dev/examples/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ca894",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Loading Libraries and Modules\n",
    "\n",
    "# scikit-learn: barebones, but fast and reliable\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, get_scorer_names\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, cross_val_score, KFold\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# statsmodels: pretty and good to use, great for interpretable ML\n",
    "from statsmodels.formula.api import ols, logit\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.graphics.regressionplots import plot_partregress_grid, influence_plot\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting things:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1881b74e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "Datasets:\n",
    "\n",
    "- `duncan` dataset.\n",
    "- `education` expenditure by US state dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d082d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the data\n",
    "duncan = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/Duncan.csv')\n",
    "duncan = duncan.set_index('profession')\n",
    "educexp = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/educexp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a852b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "- So far:\n",
    "    + Is there a relationship between `prestige` and `income`? **Yes**\n",
    "    + How strong is the relationship between `prestige` and `income`? **Yes**\n",
    "    + Which variables are associated with `prestige`?\n",
    "    + How can we accurately predict the prestige of professions not studied in this survey? **Yes, so far...**\n",
    "    + Is the relationship linear? **Yes, so far...**\n",
    "    + Is there a synergy among predictors?\n",
    "    \n",
    "- Can we do better? **Yes**, we have other predictors that we didn't not explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80cd85c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Let's fit the following model:\n",
    "\n",
    "$$ \\text{prestige} = \\beta_0 + \\beta_1\\text{income} + \\beta_2\\text{education} + \\varepsilon $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a20f07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Running the actual regression:\n",
    "\n",
    "# Create the model.Fit the model\n",
    "model = ols('prestige ~ income', data = duncan).fit()\n",
    "model3 = ols('prestige ~ income + education', data = duncan).fit()\n",
    "\n",
    "# Print the parameters\n",
    "print(model3.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76ea74",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Meaning:\n",
    "\n",
    "$$ \\text{prestige} \\ \\approx \\ -6.06 + 0.60\\text{income} + 0.55\\text{education} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4055b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Partial regression plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf5b35",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_partregress_grid(model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39290a79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Influence plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55093b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = influence_plot(model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173dd03b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## F-Statistic\n",
    "\n",
    "Are we doing better than the linear regression? We can test that!\n",
    "\n",
    "**Null hypothesis:** The model with fewer parameters is better.\n",
    "\n",
    "**Alternative hypothesis:** At least one variable in the new model does well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc591f90",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Anova for model without x model with education\n",
    "anova_lm(model, model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1cb86e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RSE and R$^2$\n",
    "\n",
    "We can also look at the Residual Standard Error and the R$^2$ to determine this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389e460",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Model with only income\n",
    "mse = model.mse_resid\n",
    "print('The mean squared error: ' + str(mse))\n",
    "\n",
    "# Residual Standard Error\n",
    "rse = np.sqrt(mse)\n",
    "print('The Residual Standard Error: ' + str(rse))\n",
    "\n",
    "# R-squared\n",
    "rsq = model.rsquared\n",
    "print(rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f5c98",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RSE and R$^2$\n",
    "\n",
    "We can also look at the Residual Standard Error and the R$^2$ to determine this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9bee34",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Model with income and education\n",
    "mse = model3.mse_resid\n",
    "print('The mean squared error: ' + str(mse))\n",
    "\n",
    "# Residual Standard Error\n",
    "rse = np.sqrt(mse)\n",
    "print('The Residual Standard Error: ' + str(rse))\n",
    "\n",
    "# R-squared\n",
    "rsq = model3.rsquared\n",
    "print(rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba4161",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "Besides the diagnostics that we run before, we can check something called *multicollinearity*\n",
    "\n",
    "### Multicollinearity\n",
    "\n",
    "- Multicollinearity is a situation when your predictors are highly correlated.\n",
    "\n",
    "- In extreme cases, it messes up with the computations in your model.\n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175public/blob/main/img/fig10.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75299a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Pairplot to check\n",
    "sns.pairplot(duncan[['prestige', 'income', 'education']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1830c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multicollinearity\n",
    "\n",
    "- One measure of multicollinearity is the *Variance Inflation Factor*.\n",
    "    + How much the multicollinearity is messing up with the estimates.\n",
    "    \n",
    "- To compute, it is fairly easy. As a rule-of-thumb, we would like to see values lower than 5.\n",
    "\n",
    "- It is rarely a problem, though... Especially with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63101a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VIF\n",
    "variables = duncan[['income', 'education']]\n",
    "vif = [variance_inflation_factor(variables, i) for i in range(variables.shape[1])]\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba6f85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Regression Models\n",
    "\n",
    "**Check-in:** Run a multiple regression model for the education expenditure dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "educexp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c8fd66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adding dummy variables to the mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duncan.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b7ac96",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We should add `type` to the model. But how to do that, since it is a `character` variable?\n",
    "\n",
    "We need to **create dummies**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c2a06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adding dummy variables to the mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(duncan.type).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129efaa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And we can add to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(duncan.type, prefix = 'type', drop_first = True)\n",
    "duncan = pd.concat([duncan, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "duncan.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36163fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression with dummies\n",
    "\n",
    "**Check-in:** Add dummies to the mix and estimate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a791c90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "**Check-in**: Do the diagnostics of the regression you just run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f10b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application\n",
    "\n",
    "- So far:\n",
    "    + Is there a relationship between `prestige` and `income`? **Yes**\n",
    "    + How strong is the relationship between `prestige` and `income`? **Yes**\n",
    "    + Which variables are associated with `prestige`? **income, education, others?**\n",
    "    + How can we accurately predict the prestige of professions not studied in this survey? **Yes**\n",
    "    + Is the relationship linear? **It seems so**\n",
    "    + Is there a synergy among predictors? **Good question!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f717454",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression with interactions\n",
    "\n",
    "Check for interactions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1457357",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = ols('prestige ~ income * education', data = duncan).fit()\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de95f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf49db3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "- Linear regression is great! But it assumes we want to predict a continuous target variable.\n",
    "\n",
    "- But there are situations when our response variables is qualitative.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- Whether a country default its debt obligations?\n",
    "\n",
    "- Whether a person voted Republican, Democrat, Independent, voted for a different party, or did not turnout to vote?\n",
    "\n",
    "- What determines the number of FOI requests that a given public office receives every day?\n",
    "\n",
    "- Is a country expected to meet, exceed, or not meet the Paris Treaty Nationally Determined Contributions?\n",
    "\n",
    "All these questions are qualitative in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb985516",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "- In 1988, the Chilean Dictator Augusto Pinochet conducted a referendum to whether he should step out.\n",
    "\n",
    "- The FLACSO in Chile conducted a surver on 2700 respondents.\n",
    "\n",
    "- We are going to build a model to predict their voting intentions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7e48c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "| **Variable** | **Meaning** |\n",
    "|:---:|---|\n",
    "| region | A factor with levels:<br>- `C`, Central; <br>- `M`, Metropolitan Santiago area; <br>- `N`, North; <br>- `S`, South; <br>- `SA`, city of Santiago. |\n",
    "| population | The population size of respondent's community. |\n",
    "| sex | A factor with levels: <br>- `F`, female; <br>- `M`, male. |\n",
    "| age | The respondent's age in years. |\n",
    "| education | A factor with levels: <br>- `P`, Primary; <br>- `S`, Secondary; <br>- `PS`, Post-secondary. |\n",
    "| income | The respondent's monthly income, in Pesos. |\n",
    "| statusquo | A scale of support for the status-quo. |\n",
    "| vote | A factor with levels: <br>- `A`, will abstain; <br>- `N`, will vote no (against Pinochet);<br>- `U`, is undecided; <br>- `Y`, will vote yes (for Pinochet). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4704653f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Loading the data\n",
    "chile = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/chilesurvey.csv')\n",
    "chile.head()\n",
    "chile_clean = chile.dropna()\n",
    "chile_clean = chile_clean[chile_clean['vote'].isin(['Y', 'N'])]\n",
    "chile_clean['vote'] = np.where(chile_clean['vote'] == 'Y', 1, 0)\n",
    "chile_clean['logincome'] = np.log(chile_clean['income'])\n",
    "chile_clean['logpop'] = np.log(chile_clean['population'])\n",
    "chile_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f20c1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why not run a Linear Regression?\n",
    "\n",
    "You could ask this very valid question. And my answer here differs a bit from the book.\n",
    "\n",
    "**My suggestion:**\n",
    "\n",
    "- If you want to **measure a treatment effect**, or any other fitting where **explanation trumps prediction**, go with the linear regression.\n",
    "    + Easy to explain to a lay audience.\n",
    "    + Good polynomial expansion around the ATE.\n",
    "    + Needs a careful design (in Causal Inference, the design is more important than the statistical method!).\n",
    "    + Interaction terms are just partial derivatives of the fitted equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c7202c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why not run a Linear Regression?\n",
    "\n",
    "You could ask this very valid question. And my answer here differs a bit from the book.\n",
    "\n",
    "**My suggestion:**\n",
    "\n",
    "- If you want to **predict outcomes**, go with a classification model appropriate for your target variable unit.\n",
    "    + You are not going to do `weird` prediction.\n",
    "    + You have a marginal efficiency gain (in terms of Standard Errors).\n",
    "    + If you have an ordered target variable, your model does look like more meaningful.\n",
    "    + Need to be careful about interaction terms (has to do with taking derivatives of link function in Generalized Linear Models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a176fbc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why not run a Linear Regression?\n",
    "\n",
    "You could ask this very valid question. And my answer here differs a bit from the book.\n",
    "\n",
    "**My suggestion:**\n",
    "\n",
    "- Be **careful when you have discrete nominal variation in your target variable**:\n",
    "    + Binary outcome: Linear Regression and Linear Discriminant Analysis are the same.\n",
    "    + Three or more categories, like the `vote` in the Chilean dataset messes up badly with things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef73891",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Book's Example\n",
    "\n",
    "Chance of Default on Credit Card Debt by Account Balance:\n",
    "\n",
    "![linear x logistic regression IRLR book](https://github.com/umbertomig/POLI175public/blob/main/img/linvslogit.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ebd7dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression belongs to a class of models called [Generalized Linear Models](https://en.wikipedia.org/wiki/Generalized_linear_model) (or GLM for short).\n",
    "\n",
    "- A GLM, in a nutshell (and in a proudly lazy definition) is an expansion of Linear Model that assumes:\n",
    "    + A Linear Relationship in part of the model\n",
    "    + But then applies a non-linear transformation to the response variable.\n",
    "\n",
    "- The non-linear transformation is called `link function`. Many link functions around (check [here](https://en.wikipedia.org/wiki/Generalized_linear_model) for various link functions).\n",
    "\n",
    "- The link function is going to determine which types of models we run.\n",
    "\n",
    "- When the outcome variable is binary, we may use the `Logistic` or `Probit` links."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d97c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In a regression, we are investigating something along the lines of:\n",
    "\n",
    "$$ \\mathbb{E}[Y | X] \\ = \\ \\beta_0 + \\beta_1 X $$\n",
    "\n",
    "But when the outcome is binary we would like to get:\n",
    "\n",
    "$$ \\mathbb{E}[Y | X] \\ = \\ \\mathbb{P}(Y = 1 | X) $$\n",
    "\n",
    "And the Logistic link is nothing but:\n",
    "\n",
    "$$ \\mathbb{P}(Y = 1 | X) \\ = \\ \\dfrac{e^{(\\beta_0 + \\beta_1X)}}{1 + e^{(\\beta_0 + \\beta_1X)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6cf25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "With a bit of manipulation, we get to something called odds ratio:\n",
    "\n",
    "$$ \\dfrac{\\mathbb{P}(Y = 1 | X)}{\\mathbb{P}(Y = 0 | X)} \\ = \\ \\dfrac{\\mathbb{P}(Y = 1 | X)}{1 - \\mathbb{P}(Y = 1 | X)} \\ = \\ e^{(\\beta_0 + \\beta_1X)} $$\n",
    "\n",
    "And logging the thing gets rid of the Euler constant:\n",
    "\n",
    "$$ \\log \\left( \\dfrac{\\mathbb{P}(Y = 1 | X)}{1 - \\mathbb{P}(Y = 1 | X)}\\right) \\ = \\ \\beta_0 + \\beta_1X $$\n",
    "\n",
    "And this is the Logit Link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1324852",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Little detour to talk about odd ratios:\n",
    "\n",
    "- Note the odd ratio: $\\dfrac{\\mathbb{P}(Y = 1 | X)}{1 - \\mathbb{P}(Y = 1 | X)}$\n",
    "\n",
    "- It is a ratio between the chance of $Y = 1$ divided by the chance of $Y = 0$.\n",
    "\n",
    "- Since probabilities are between zero and one, the ratio is always between $(0, \\infty)$.\n",
    "\n",
    "Example:\n",
    "\n",
    "- If based on characteristics, two in every ten people vote for Pinochet, $\\mathbb{P}(Y = 1 | X = \\text{some characs.}) = 0.2$ and the odds ratio is $1/4$.\n",
    "\n",
    "- If based on other set of characteristics, nine out of ten people vote for Pinochet, $\\mathbb{P}(Y = 1 | X = \\text{some other characs.}) = 0.9$ and the odds ratio is $9$.\n",
    "\n",
    "- One is like the number that does not change the ratios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe946ad8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Little other detour to talk about the coefficients:\n",
    "\n",
    "- In linear regression, changes in one unit of $x_i$ changes your target variable in $\\beta_i$ units, on average.\n",
    "\n",
    "- In logistic regression, changes in one unit of $x_i$ changes **the log odds** your target variable in $\\beta_i$ units, on average.\n",
    "\n",
    "- Multiplies the odds by $e^{\\beta_i}$! This is **not** a straight line!\n",
    "\n",
    "- Easy proxy (does not work for interaction terms): \n",
    "    + When $\\beta_1$ is **positive**, it **increases** the $\\mathbb{P}(Y = 1 | X)$\n",
    "    + When $\\beta_1$ is **negative**, it **decreases** the $\\mathbb{P}(Y = 1 | X)$\n",
    "    \n",
    "- Try to compute the partial derivatives on $X$ and you will see the complications!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9494c01e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Technical:\n",
    "\n",
    "1. The estimation is through [maximizing the likelihood function](https://en.wikipedia.org/wiki/Likelihood_function).\n",
    "    + This is outside the scope of the course, but an interesting topic to learn in an advanced course.\n",
    "\n",
    "\n",
    "2. The hypothesis test for the coefficient's significance in here is a Z-test (based on the Normal distribution).\n",
    "    + Null Hypothesis: $H_0: \\ \\beta_i = 0$ or alternatively $H_0: \\ e^{\\beta_i} = 1$.\n",
    "\n",
    "\n",
    "3. Making predictions:\n",
    "    + Just insert the predicted $\\hat{\\beta}$s on the equation.\n",
    "    \n",
    "$$ \\hat{p}(X) \\ = \\ \\dfrac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ecbe7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "First, let's fit a Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0b79e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.regplot(x = 'logincome', y = 'vote', x_jitter = 0.1, y_jitter = 0.1, data = chile_clean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8870963",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab91d3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Model\n",
    "modlin = ols('vote ~ logincome', data = chile_clean).fit()\n",
    "modlin.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf9982d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Now, let us fit a Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c8d2e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Seaborn plot\n",
    "sns.regplot(x = 'logincome', y = 'vote', \n",
    "            x_jitter = 0.1, y_jitter = 0.1, \n",
    "            data = chile_clean, logistic = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937bf81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1fd6d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "modlogit = logit('vote ~ logincome', data = chile_clean).fit()\n",
    "modlogit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9619f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffa64a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "modlogit2 = logit('vote ~ logincome + logpop + region + age + education', data = chile_clean).fit()\n",
    "modlogit2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb9a1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- Let's look at the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482614c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Parameters\n",
    "np.exp(modlogit2.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e8f89d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- Let's look at the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6fdb3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Parameters\n",
    "np.exp(modlogit2.params)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b72f2f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- Now with Scikit Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c912014",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y = chile_clean['vote']\n",
    "\n",
    "# Predictors\n",
    "X = chile_clean[['logincome', 'logpop', 'age']]\n",
    "\n",
    "# Loading the model\n",
    "logreg =  LogisticRegression() \n",
    "\n",
    "# Fitting the model\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# Getting parameters\n",
    "print(logreg.intercept_, logreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cc5b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Where are the categorical variables?\n",
    "\n",
    "In Scikit Learn, you need to create dummy variables for the categorical vars. \n",
    "\n",
    "Thus, you should do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd2f40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Detour: Creating Dummies for Male\n",
    "dummies = pd.get_dummies(chile_clean['sex'], prefix = 'sex', drop_first = True)\n",
    "chile_clean_wdumvars = pd.concat([chile_clean, dummies], axis=1)\n",
    "chile_clean_wdumvars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f9278",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "**Your turn:** Create dummies for `region` and `education`. Which category was dropped in each of the processes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2d6dd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a7d29c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4509f49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Dummies\n",
    "\n",
    "# Sex\n",
    "dummies = pd.get_dummies(chile_clean['sex'], prefix = 'sex', drop_first = True)\n",
    "chile_clean_wdumvars = pd.concat([chile_clean, dummies], axis=1)\n",
    "\n",
    "# Education\n",
    "dummies = pd.get_dummies(chile_clean['region'], prefix = 'region', drop_first = True)\n",
    "chile_clean_wdumvars = pd.concat([chile_clean_wdumvars, dummies], axis=1)\n",
    "\n",
    "# Region\n",
    "dummies = pd.get_dummies(chile_clean['education'], prefix = 'education', drop_first = True)\n",
    "chile_clean_wdumvars = pd.concat([chile_clean_wdumvars, dummies], axis=1)\n",
    "\n",
    "## Head\n",
    "chile_clean_wdumvars.head()\n",
    "\n",
    "# You can even drop the original variables, if you want to: \n",
    "# DataFrame.drop(labels = ['v1, 'v2',..., 'vn'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036c003",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- Now with Scikit Learn, and using all the categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacec81",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y = chile_clean_wdumvars['vote']\n",
    "\n",
    "# Predictors\n",
    "X = chile_clean_wdumvars[['logincome', 'logpop', 'age', \n",
    "                          'sex_M', \n",
    "                          'region_M', 'region_N', 'region_S', 'region_SA', \n",
    "                          'education_PS', 'education_S']]\n",
    "\n",
    "# Loading the model\n",
    "logreg =  LogisticRegression(solver = 'newton-cg') \n",
    "\n",
    "# Fitting the model\n",
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6569741f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d151b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Getting parameters\n",
    "print('Original coefficients: ')\n",
    "print(logreg.intercept_, logreg.coef_)\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "# Exps:\n",
    "print('Exponentiated coefficients: ')\n",
    "print(np.exp(logreg.intercept_), np.exp(logreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab07143",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative Models of Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367fa1bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "Logistic regression involves modeling the probability of a response given a set of parameters\n",
    "    + Uses the logistic link for the *conditional distribution*\n",
    "    \n",
    "$$ \\mathbb{E}(Y = 1 | X = x) \\ = \\ \\mathbb{P}(Y = 1 | X = x) \\ = \\ \\text{Logit}(\\beta_0 + \\cdots + \\beta_pX_p) $$\n",
    "\n",
    "Another approach is to model the distribution for each values of $Y$.\n",
    "\n",
    "And then, use the Bayes' Theorem to get the conditional distributions.\n",
    "\n",
    "But why?\n",
    "\n",
    "1. Separation\n",
    "\n",
    "2. Small sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f158971",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "Let $\\pi_k$ the prior probability of $Y = k$.\n",
    "\n",
    "And let $f_k(x) = \\mathbb{P}(X = x | Y = k)$ the density function for an observation that comes from the $k$-th class.\n",
    "\n",
    "The Bayes theorem says that:\n",
    "\n",
    "$$ \\mathbb{P}(Y = k | X = x) \\ = \\ \\dfrac{\\pi_kf_k(x)}{\\sum_l \\pi_l f_l(x)} $$\n",
    "\n",
    "Now, estimating $\\pi_k$ is easy: we just compute the fraction that belongs to the $k$-th class.\n",
    "\n",
    "How about $f$?\n",
    "\n",
    "+ Different estimators are going to give us different classifiers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04bce0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "- Suppose we have only one variable $x$ and $f_k$ is Gaussian:\n",
    "\n",
    "$$ x \\sim N(\\mu_k, \\sigma_k^2) $$\n",
    "\n",
    "- And assuming further that the draws have the same variance: $\\sigma^2 = \\sigma_k^2 \\forall k$\n",
    "\n",
    "- Computing the log of the posterior gives us:\n",
    "\n",
    "$$ \\delta_k(x) \\ = \\ x \\dfrac{\\mu_k}{\\sigma^2} - \\dfrac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf7e99b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "And the decision for which class the $x$ belongs is simple: **Whichever has the highest probability is the \"winner\"**.\n",
    "\n",
    "1. Let $x$\n",
    "\n",
    "2. Compute $\\delta_0(x)$\n",
    "\n",
    "3. Compute $\\delta_1(x)$\n",
    "\n",
    "4. The highest is the winner :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b29d77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "But how the decision boundary looks like? We need to find the *indifference point*:\n",
    "\n",
    "$$ \\delta_1(x) = \\delta_0(x) $$\n",
    "\n",
    "Do the algebra, and you are going to find:\n",
    "\n",
    "$$ x \\ = \\ \\dfrac{\\mu_0 + \\mu_1}{2} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddfddba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "![img lda](https://github.com/umbertomig/POLI175public/blob/main/img/ldabounds.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc8d06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "And the LDA approximate the quantities of interest by doing the following:\n",
    "\n",
    "1. $$ \\widehat{\\mu}_k  \\ = \\ \\dfrac{1}{n_k} \\sum_{i:y_i = k}x_i $$\n",
    "\n",
    "\n",
    "2. $$ \\widehat{\\sigma}^2 \\ = \\ \\dfrac{1}{n - K} \\sum_{k=1}^K\\sum_{i:y_i = k}(x_i - \\widehat{\\mu}_k)^2 $$\n",
    "\n",
    "\n",
    "3. $$ \\widehat{\\pi}_k \\ = \\ \\dfrac{n_k}{n} $$\n",
    "\n",
    "Note that you can classify more than two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d52ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "The chance that $x$ belongs to $y=k$ is going to be:\n",
    "\n",
    "$$ \\widehat{\\delta}_k(x) \\ = \\ x \\dfrac{\\widehat{\\mu}_k}{\\widehat{\\sigma}^2} - \\dfrac{\\widehat{\\mu}_k^2}{2\\widehat{\\sigma}^2} + \\log(\\widehat{\\pi}_k) $$\n",
    "\n",
    "Note that this is a linear function, so the name `Linear Discriminant Analysis`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10350ec3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "Now let's fit it using `scikit learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebd965",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Start a LDA (do not mix this up with Latent Dirichlet Allocation!)\n",
    "X, y = chile_clean[['logincome', 'age']], chile_clean['vote']\n",
    "\n",
    "# Create the model\n",
    "ldan = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Fitting model\n",
    "ldan.fit(X, y)\n",
    "\n",
    "# Plotting the tree boundaries\n",
    "fig = DecisionBoundaryDisplay.from_estimator(ldan, X, response_method=\"predict\",\n",
    "                                             alpha=0.5, cmap=plt.cm.coolwarm)\n",
    "\n",
    "# Plotting the data points    \n",
    "fig.ax_.scatter(x = chile_clean['logincome'], y = chile_clean['age'], \n",
    "                c = y, alpha = 0.5,\n",
    "                cmap = plt.cm.coolwarm)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03c4a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 1. Linear Discriminant Analysis\n",
    "\n",
    "The most fundamental question:\n",
    "\n",
    "- How much error in classification we are doing?\n",
    "\n",
    "- To learn that, we need to study the `confusion matrix`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd584c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring Performance\n",
    "\n",
    "**Confusion Matrix**:\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "1. **Accuracy:** $$\\dfrac{\\text{correct predictions}}{\\text{total observations}} \\ = \\ \\dfrac{tp + tn}{tp + tn + fp + fn}$$\n",
    "\n",
    "- High accuracy: lots of correct predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c7f89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring Performance\n",
    "\n",
    "**Confusion Matrix**:\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "2. **Precision:** $$\\dfrac{\\text{true positives}}{\\text{total predicted positive}} \\ = \\ \\dfrac{tp}{tp + fp}$$\n",
    "\n",
    "- High precision: low false-positive rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe6d80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring Performance\n",
    "\n",
    "**Confusion Matrix**:\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "3. **Recall:** $$\\dfrac{\\text{true positives}}{\\text{total actual positive}} \\ = \\ \\dfrac{tp}{tp + fn}$$\n",
    "\n",
    "- High recall: low false-negative rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65376f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring Performance\n",
    "\n",
    "4. **F1-Score**:\n",
    "\n",
    "$$ \\text{F1} \\ = \\ 2 \\times \\dfrac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} $$\n",
    "\n",
    "- Lets look at the two models: logistic x lda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a07d858",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y = chile_clean_wdumvars['vote']\n",
    "\n",
    "# Predictors\n",
    "X = chile_clean_wdumvars[['logincome', 'logpop', 'age', \n",
    "                          'sex_M', \n",
    "                          'region_M', 'region_N', 'region_S', 'region_SA', \n",
    "                          'education_PS', 'education_S']]\n",
    "\n",
    "# Loading the model\n",
    "logreg =  LogisticRegression(solver = 'newton-cg')\n",
    "ldan = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Fitting the models\n",
    "logreg.fit(X, y)\n",
    "ldan.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a303ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring Performance\n",
    "\n",
    "- Lets look at the two models: logistic x lda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d090711",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_logreg = logreg.predict(X)\n",
    "y_pred_ldan = ldan.predict(X)\n",
    "\n",
    "# Logistic Regression\n",
    "print(confusion_matrix(y, y_pred_logreg))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "print(confusion_matrix(y, y_pred_ldan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe1a3a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Classification Report\n",
    "print(classification_report(y, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b072c5",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# LDA Classification Report\n",
    "print(classification_report(y, y_pred_ldan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de46ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "### 2. Quadratic Discriminant Analysis\n",
    "\n",
    "The main difference is that it assumes that every observation has its own covariance matrix:\n",
    "\n",
    "- Drop the `same-sigma-assumption`.\n",
    "\n",
    "### 3. Naïve Bayes\n",
    "\n",
    "Instead of assuming that $f$ belongs to a class of distributions (e.g., Normal), it assumes that the $f$s are independent:\n",
    "\n",
    "- Drop the `Multivariate-Normal-assumption`.\n",
    "\n",
    "- For $p$ predictors, you make only assumptions about each $x_{ik}$:\n",
    "\n",
    "$$ f_k(x) \\ = \\ f_{k1}(x_1)\\times \\cdots \\times f_{kp}(x_p) $$\n",
    "\n",
    "- And you assume a normal distribution (Gaussian shape) for each variable..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3bb54e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification\n",
    "\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/ldaxqdaxnb.png?raw=true)\n",
    "\n",
    "- Purple: Naïve Bayes; Black: LDA; Green: QDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c0b02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## QDA\n",
    "qdan = QuadraticDiscriminantAnalysis()\n",
    "qdan.fit(X, y)\n",
    "\n",
    "## Gaussian Naive Bayes\n",
    "nbays = GaussianNB()\n",
    "nbays.fit(X, y)\n",
    "\n",
    "\n",
    "# Predictions\n",
    "y_pred_logreg = logreg.predict(X)\n",
    "y_pred_ldan = ldan.predict(X)\n",
    "y_pred_qdab = qdan.predict(X)\n",
    "y_pred_nbays = nbays.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df734c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c654277b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "print(classification_report(y, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea83d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96403e23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "print(classification_report(y, y_pred_ldan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b6326",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644d3c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Quadratic Discriminant Analysis\n",
    "print(classification_report(y, y_pred_qdab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6923f269",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models of Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b3dd3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "print(classification_report(y, y_pred_nbays))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549003d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic x Generative Models for Classification\n",
    "\n",
    "**Check-in**: Does social pressure affects turnout?\n",
    "\n",
    "Gerber, Green, and Larimer. 2008 studied this question on their [\"*Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment.*\" **American Political Science Review**, 102 (1): 33-48.](http://www.donaldgreen.com/wp-content/uploads/2015/09/Gerber_Green_Larimer-APSR-2008.pdf).\n",
    "\n",
    "They selected households in Michigan receive a letter containing the following information:\n",
    "\n",
    "> Dear Registered Voter: \\concept{WHAT IF YOUR NEIGHBORS KNEW WHETHER YOU VOTED?} ... We’re sending this mailing to you and your neighbors to publicize who does and does not vote. The chart shows the names of some of your neighbors, showing which have voted in the past. After the August 8 election, we intend to mail an updated chart. You and your neighbors will all know who voted and who did not. \\concept{DO YOUR CIVIC DUTY--VOTE!}\n",
    "\n",
    "| MAPLE DR                 | Aug 2004    | Nov 2004 | Aug 2006 |\n",
    "|--------------------------|-------------|----------|----------|\n",
    "| 9995 JOSEPH JAMES SMITH  | Voted       | Voted    | ???      |\n",
    "| 995 JENNIFER KAY SMITH   | Didn't vote | Voted    | ???      |\n",
    "| 9997 RICHARD B JACKSON   | Didn't vote | Voted    | ???      |\n",
    "| 9999 KATHY MARIE JACKSON | Didn't vote | Voted    | ???      |\n",
    "\n",
    "The treatment assignment is called `pressure`. If no pressure, then the voter received no letter. We want to study whether `pressure` affected `voted`. \n",
    "\n",
    "Fit all models we learned so far on this dataset (Note: The linear is the most adequate, since the data comes from a randomized experiment, but please fit all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9520bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI30Dpublic/main/datasets/voting.csv')\n",
    "\n",
    "# Your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b027f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3773c9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resampling\n",
    "\n",
    "- Involve repeatedly drawing `samples` for a `training dataset` to obtain fitting information.\n",
    "\n",
    "- `Samples`: A randomly selected fraction of the original data.\n",
    "    + Do not mistake it for a different sample from a population.\n",
    "    \n",
    "- `Training`: Training the model means to fit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b2555",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resampling\n",
    "\n",
    "- This sounds weird: why not fit the model into the actual data?\n",
    "    + We would not have a measure of how well our model is doing.\n",
    "    + In the end, this matters! And matters especially for the data that we did not train the model!\n",
    "\n",
    "- In this sense, resampling is a clever trick to see how the model would do in the `real world`, without going to the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82158a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resampling\n",
    "\n",
    "- It helps us to:\n",
    "    + Evaluate the performance of the model (`Model assessment`).\n",
    "    + Select the proper flexibility for our model (`Model selection`).\n",
    "\n",
    "- Drawback: they are computationally intensive.\n",
    "    + Usually involves refitting the model again and again.\n",
    "    \n",
    "- We are going to discuss the following:\n",
    "    + `Cross-validation`: Measure the performance and select appropriate flexibility.\n",
    "    + (not this now) `Bootstrap`: Measure the accuracy of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4312f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Loading Chile data\n",
    "chile = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/chilesurvey.csv')\n",
    "chile_clean = chile.dropna()\n",
    "chile_clean = chile_clean[chile_clean['vote'].isin(['Y', 'N'])]\n",
    "chile_clean['vote'] = np.where(chile_clean['vote'] == 'Y', 1, 0)\n",
    "chile_clean['logincome'] = np.log(chile_clean['income'])\n",
    "chile_clean['logpop'] = np.log(chile_clean['population'])\n",
    "dummies = pd.get_dummies(chile_clean['sex'], prefix = 'sex', drop_first = True)\n",
    "chile_clean = pd.concat([chile_clean, dummies], axis=1)\n",
    "dummies = pd.get_dummies(chile_clean['region'], prefix = 'region', drop_first = True)\n",
    "chile_clean = pd.concat([chile_clean, dummies], axis=1)\n",
    "dummies = pd.get_dummies(chile_clean['education'], prefix = 'education', drop_first = True)\n",
    "chile_clean = pd.concat([chile_clean, dummies], axis=1)\n",
    "chile_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696a377",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Education Expenditure Dataset\n",
    "educ = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/educexp.csv')\n",
    "educ = educ.set_index('states')\n",
    "for i in educ.columns:\n",
    "    educ[i + '_log'] = np.log(educ[i])\n",
    "educ.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0af4b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "- We talked about it yesterday.\n",
    "\n",
    "- In that context, we looked at the idea of a\n",
    "    + `training error rate` (the boring one): The error when fitting the model to data that was used to train the parameters, and\n",
    "    + `test error rate` (the cool one): The error associated with fitting the model to ***unseen*** data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e9a46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### Validation Set Approach\n",
    "\n",
    "- Randomly divide the data into two sets:\n",
    "    + `Training set`: The data used to fit the model\n",
    "    + `Testing set`: The data used to test the performance of the fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33edcd3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### Validation Set Approach\n",
    "\n",
    "- Split the sample in half training - half testing and running the estimation:\n",
    "\n",
    "![img vsa](https://github.com/umbertomig/POLI175public/blob/main/img/cv1.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ecee73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## With 50% split (no urban_log)\n",
    "y = educ['education_log']\n",
    "X = educ[['income_log', 'young_log']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 1234)\n",
    "\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "np.sum((y_pred - y_test) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6847d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## With 50% split (with urban_log)\n",
    "y = educ['education_log']\n",
    "X = educ[['income_log', 'young_log', 'urban_log']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 1234)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "np.sum((y_pred - y_test) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ac6ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Your turn: Check the MSE when removing income_log. Is it\n",
    "##  better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb05831",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Your turn: Check the MSE when removing 'urban_pop' \n",
    "##   with only 20% of observations in the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43173a8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### Leave-One-Out Cross-Validation\n",
    "\n",
    "- It does what it says: leaves one observation out and fits the model with $n-1$ cases.\n",
    "\n",
    "- Then, it predicts the results in the case left out.\n",
    "\n",
    "- **Great** for small datasets and when prediction is critical.\n",
    "\n",
    "- **Bad** in terms of computational time.\n",
    "\n",
    "$$ CV_n \\ = \\ \\dfrac{1}{n}\\sum_i MSE_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae8ab3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### Leave-One-Out Cross-Validation\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/cv2.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f039a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## LOOCV\n",
    "## Variables: model without urban population\n",
    "y = educ['education_log']\n",
    "X = educ[['income_log','young_log']]\n",
    "\n",
    "## Leave-One-Out-CV\n",
    "cv = LeaveOneOut()\n",
    "reg = LinearRegression()\n",
    "\n",
    "## Run the CV\n",
    "scores = cross_val_score(reg, X, y,\n",
    "                         scoring = 'neg_mean_squared_error',\n",
    "                         cv = cv)\n",
    "\n",
    "## RMSE\n",
    "print(np.sqrt(np.mean(np.absolute(scores))))\n",
    "\n",
    "## MSE\n",
    "np.mean(np.absolute(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d0d15e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## LOOCV\n",
    "## Variables: model **with** urban population\n",
    "y = educ['education_log']\n",
    "X = educ[['income_log', 'young_log', 'urban_log']]\n",
    "\n",
    "## Leave-One-Out-CV\n",
    "cv = LeaveOneOut()\n",
    "reg = LinearRegression()\n",
    "\n",
    "\n",
    "## Run the CV\n",
    "scores = cross_val_score(reg, X, y, \n",
    "                         scoring = 'neg_mean_squared_error',\n",
    "                         cv = cv)\n",
    "\n",
    "## MSE\n",
    "print(np.mean(np.absolute(scores)))\n",
    "\n",
    "## RMSE\n",
    "np.sqrt(np.mean(np.absolute(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb9e5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Your turn: compare the model with x without logs\n",
    "## Note: the target has to be the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd4cbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### Metrics\n",
    "\n",
    "- To do the comparison, you need a metric.\n",
    "\n",
    "- `scikit learn` has many matrics available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b932b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Lots of stats to compute the error:\n",
    "print(get_scorer_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf55dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Your turn: find and use R-squared as the parameter for a\n",
    "## LOOCV. What is the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87217f23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### K-Fold Cross-Validation\n",
    "\n",
    "- Leaves $k$ groups out and fits the model with the observations outside each group.\n",
    "\n",
    "- Then, it predicts the results in the cases left out.\n",
    "\n",
    "- **Great** in most cases.\n",
    "\n",
    "- **Bad** *sometimes* computationally expensive.\n",
    "\n",
    "$$ CV_k \\ = \\ \\dfrac{1}{k}\\sum_i MSE_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b281b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### K-Fold Cross-Validation\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/cv3.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253bf01f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## K-Fold CV (k = 5)\n",
    "y = educ['education_log']\n",
    "X = educ[['income_log', 'young_log']]\n",
    "\n",
    "## k-Fold CV (n_splits = k, shuffle: reshuffle data before split)\n",
    "cv = KFold(n_splits = 5, random_state = 1234, shuffle = True) \n",
    "reg = LinearRegression()\n",
    "\n",
    "\n",
    "## Run the CV\n",
    "scores = cross_val_score(reg, X, y,\n",
    "                         scoring = 'neg_mean_squared_error',\n",
    "                         cv = cv)\n",
    "\n",
    "## MSE\n",
    "print(np.mean(np.absolute(scores)))\n",
    "\n",
    "## RMSE\n",
    "np.sqrt(np.mean(np.absolute(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29899042",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## K-Fold CV (k = 5)\n",
    "y = educ['education_log']\n",
    "X = educ[['income_log', 'young_log', 'urban_log']]\n",
    "\n",
    "## k-Fold CV (n_splits = k, shuffle: reshuffle data before split)\n",
    "cv = KFold(n_splits = 5, random_state = 1234, shuffle = True) \n",
    "reg = LinearRegression()\n",
    "\n",
    "\n",
    "## Run the CV\n",
    "scores = cross_val_score(reg, X, y,\n",
    "                         scoring = 'neg_mean_squared_error',\n",
    "                         cv = cv)\n",
    "\n",
    "## MSE\n",
    "print(np.mean(np.absolute(scores)))\n",
    "\n",
    "## RMSE\n",
    "np.sqrt(np.mean(np.absolute(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3be41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Your turn: Run a 10-fold CV? Any differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05794a41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### Bias-Variance Trade-off\n",
    "\n",
    "- k-Fold CV is more computationally efficient than LOOCV. But how about Bias-Variance Trade-offs?\n",
    "\n",
    "- Larger fractions in a two-split leads to high bias: over-estimates the error rates.\n",
    "\n",
    "- LOOCV: leaves just one, so it gives an unbiased estimate of the testing error rates: \n",
    "    + Very good for bias reduction!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f697d9d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### Bias-Variance Trade-off\n",
    "\n",
    "- LOOCV has high variance: almost the same observations at each run!\n",
    "    + Very bad for variance.\n",
    "    \n",
    "- k-Fold CV:\n",
    "    + Each subset is a *bit more different* than the other.\n",
    "    + Leads to less correlation between each fold.\n",
    "    + Good balance usually with $k=5$ or $k=10$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec711f60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### Bias-Variance Trade-off\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/cv4.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd57b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### CV on Classification Problems\n",
    "\n",
    "- When we have a classification, we must change how we evaluate the error.\n",
    "\n",
    "- With classification, the LOOCV would look like this:\n",
    "\n",
    "$$ CV_n \\ = \\ \\dfrac{1}{n} \\sum_i I(y_i \\neq \\widehat{y}_i) $$\n",
    "\n",
    "- And the `accuracy` measure will be $I(y_i = \\widehat{y}_i)$, so we need to subtract 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f488f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### CV on Classification Problems\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/cv5.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1884093",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## LOOCV on a Logistic Regression\n",
    "# Checking best polynomial for Age\n",
    "poly = list(range(1, 6))\n",
    "errmea = []\n",
    "y = chile_clean['vote']\n",
    "for p in poly:\n",
    "    if p == 1:\n",
    "        X = pd.DataFrame({\n",
    "            'age_1': chile_clean['age']\n",
    "        })\n",
    "    else:\n",
    "        X['age_' + str(p)] = X['age_1'] ** p\n",
    "    cv = LeaveOneOut()\n",
    "    logreg = LogisticRegression()\n",
    "    scores = cross_val_score(logreg, X, y, \n",
    "                             scoring = 'accuracy',\n",
    "                             cv = cv, n_jobs = -1)\n",
    "    print('For polynomial order {a}, the Logistic Regression Error Rate is {b}.\\n'.format(a = str(p), b = str(1-scores.mean())))\n",
    "    errmea.append(1-scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e97dd5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "### K-Nearest Neighbors Classifier\n",
    "\n",
    "- Little detour back to talk about a good algorithm for classification (also very intuitive).\n",
    "\n",
    "- Given an integer $K$, and a test observation, it says that:\n",
    "\n",
    "$$ \\mathbb{P}(Y = j| X = x_0) \\ = \\ \\dfrac{1}{K}\\sum_{i \\in N_0} I(y_i = j) $$\n",
    "\n",
    "- Meaning: classify the observation based on the class of the closest $K$ obs:\n",
    "    + The one more frequent is the winner.\n",
    "    \n",
    "- Closest: the idea of a metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc8989",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "### K-Nearest Neighbors Classifier\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/knn1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f917d93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "### K-Nearest Neighbors Classifier\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175public/blob/main/img/knn2.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc528ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "X = chile_clean[['age', 'statusquo']]\n",
    "y = chile_clean['vote']\n",
    "\n",
    "# Create the model\n",
    "knn = KNeighborsClassifier(n_neighbors = 10).fit(X, y)\n",
    "\n",
    "# Plotting the tree boundaries\n",
    "fig = DecisionBoundaryDisplay.from_estimator(knn, X, response_method=\"predict\",\n",
    "                                             alpha=0.5, cmap=plt.cm.coolwarm)\n",
    "\n",
    "# Plotting the data points    \n",
    "fig.ax_.scatter(x = chile_clean['age'], y = chile_clean['statusquo'], \n",
    "                c = y, alpha = 0.5,\n",
    "                cmap = plt.cm.coolwarm)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4871d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Now choose K! (We will learn a better method for doing this: GridSearchCV!)\n",
    "bigK = list(range(1, 100))\n",
    "errmea = []\n",
    "y = chile_clean['vote']\n",
    "X = chile_clean[['statusquo', 'logincome', 'logpop', 'age']]\n",
    "for smallk in bigK:\n",
    "    cv = KFold(n_splits = 10, random_state = 1234, shuffle = True)\n",
    "    knn = KNeighborsClassifier(n_neighbors = smallk)\n",
    "    scores = cross_val_score(knn, X, y, \n",
    "                             scoring = 'accuracy',\n",
    "                             cv = cv, n_jobs = -1)\n",
    "    errmea.append(1-scores.mean())\n",
    "print('Best K is {a}.'.format(a = str(bigK[errmea.index(min(errmea))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ef717",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x = bigK, y = errmea)\n",
    "plt.title('KNN algorithm')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.scatter(bigK[errmea.index(min(errmea))], min(errmea), marker='X', color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dfb978",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "**Check-in:** Study the best method for predicting default in credit card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "default = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/default.csv')\n",
    "default.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280d845",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edb64c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# See you in the next class!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
